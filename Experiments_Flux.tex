\section{Flux.1-dev Pruning}
Flux.1-dev is a 11.9 billion parameter rectified flow model capable of generating detailed, high-quality, and high-aesthetic images. Due to its high parameter count, it is one primary target for model reduction techniques in the literature and should be used to demonstrate the effectiveness of the best settings found for PixArt-$\Sigma$ by comparing it to various competitors. In addition, as training of Flux.1-dev is extremely computatinally intensive, the training-free GRASP method (see sec. \ref{sec:GRASP}) originally developped for \gls{LLM} compression is transfered to Flux1.dev.

\subsection{Training Details}
\label{sec:flux_training_details}
In this section, all experiments are built upon the training and inference code for Flux.1-dev published in the following git repo \cite{kohya_ss_sd_scripts_2024}. The hyperparameters used for finetuning Flux.1-dev are specified in Tab. \ref{tab:flux_training_config}. For pruning Flux.1-dev the best settings found for PixArt-$\Sigma$ are leveraged and specified here again for completeness:

\begin{itemize}
	\item \textbf{Block Importance Analysis} \\
	To determine the importance of individual blocks in Flux.1-dev, every block is individually compressed by 60\% and a small reference dataset (100 images) is generated which is leveraged to compute the \gls{CMMD} score. Afterwards, the \gls{CMMD} score is weighted by the number of removed parameters, e.g. a single-stream block contains less parameters than a double-stream block. In contrast to the experiments with PixArt-$\Sigma$, the scoring is computed in a single initial pass rather than leveraging a dynamic greedy algorithm that would iteratively reevaluate the remaining blocks after each sequential compression due to the high computational demands of Flux.1-dev.
	
	\item \textbf{Pruning Method} \\
	Progressive structural model pruning is performed.
	
	\item \textbf{Knowledge Distillation Loss} \\
	The original flow matching loss as well as the normalized intermediate feature and output losses are utilized whereby the normalization is computed for features from double-stream and single-stream block seperately
	\begin{equation}
		\mathcal{L} = \mathcal{L_\text{Task}} + \lambda_\text{OutKD} \mathcal{L_\text{OutKD}} + \lambda_\text{SingleFeatKD} \mathcal{L_\text{SingleFeatKD}} + \lambda_\text{DoubleFeatKD} \mathcal{L_\text{DoubleFeatKD}} \quad.
	\end{equation}
	with $\lambda_\text{OutKD}  = 10$, $\lambda_\text{SingleFeatKD}=0.01$ and $\lambda_\text{DoubleFeatKD}=0.1$.
	
	\item \textbf{Finetuning Protocol} \\
	Only the LAION-Flux dataset is utilized, and a complete finetuning is performed directly. 
	
	\item \textbf{Structural Compression Strategy} \\
	The \gls{SVD} compression is leveraged for compressing Flux.1-dev blocks. The individual components of double-stream and single-stream blocks and their corresponding parameters can be found in Tab. \ref{tab:Flux_parameter_count}.
	
	\item \textbf{\gls{SVD} Compression Scheduling} \\
	
\end{itemize} 

A specific 
\begin{table}[H]
	\centering
	\caption[Flux Finetuning Hyperparameters]{Configuration details for the compressed model finetuning run, including optimizer settings and flow fatching specific parameters. This parameters represent the config setup for the training using the git repository \cite{kohya_ss_sd_scripts_2024}.}
	\label{tab:flux_training_config}
	\begin{tabular}{l l}
		\toprule
		\textbf{Hyperparameter} & \textbf{Value} \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Optimization \& Scheduler}}} \\
		Optimizer Type & Adafactor \\
		Optimizer Arguments & \small{relative\_step=False, scale\_parameter=False, warmup\_init=False} \\
		Learning Rate & $1.8 \times 10^{-6}$ \\
		LR Scheduler & constant\_with\_warmup \\
		LR Warmup Steps & 4240 \\
		Gradient Accumulation & 4 \\
		Batch Size & 1 \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Model Precision \& Hardware}}} \\
		Mixed Precision & bf16 \\
		Weight Precision (Base) & fp8 \\
		Full bfloat16 Training & True \\
		Gradient Checkpointing & True \\
		SDPA (Scaled Dot Product Attn) & True \\
		GPU & 1 $\times$ H200 \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Flow Matching \& Loss Settings}}} \\
		Discrete Flow Shift & 3 \\
		Timestep Sampling & sigmoid \\
		Max Timestep & 1000 \\
		Loss Type & L2 \\
		Huber Parameter ($c$) & 0.1 \\
		Huber Schedule & snr \\
		Model Prediction Type & raw \\
		Guidance Scale & 1.0 \\
		Sample Sampler & euler\_a \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Architecture \& Data Settings}}} \\
		T5 Max Token Length & 512 \\
		Clip Skip & 1 \\
		Apply T5 Attn Mask & True \\
		Data Loader Workers & 6 \\
		Seed & 42 \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Training-Free Pruning}
In this section, it is investigated whether \gls{GRASP} (see Sec. \ref{sec:GRASP}) is transferable from \gls{LLM}s to flow-based image generation models like Flux.1-dev. The goal is to identify singular values based on their importance for the image generation task rather than solely based on their magnitude as is standard practise in ordinary \gls{SVD} compression. The performance of \gls{GRASP} is compared to Eco-Diff \cite{EcoDiff}  another recently proposed training-free method to prune Flux.1-dev and to the simple application of \gls{SVD} without retraining. \\
First, we examine the extent to which the gradient-based method GRASP selects different singular values than a normal SVD, which selects them by magnitude. This experiment focuses exclusively on the compression of double-stream blocks from Flux.1-dev as they contain the most parameters. Exclusively for this setup a pre-existing importance ranking for the Flux.1-dev transformer blocks from \cite{FastFlux} (see Sec. \ref{app:GRASP_BlockImportance}) is utilized. In total four different compression levels are tested. The compression rate for the double-stream blocks is fixed to 76\% such that the levels differs in the number of compressed blocks. For the exact ranks used during \gls{SVD} for the individual components please refer to Tab. \ref{tab:GRASPSVD_ranks}. \\
Fig. \ref{fig:MagnitudeVsImportanceSVD} illustrates exemplarily the magnitude and the importance estimated by \gls{GRASP} for each singular value of the image modulation matrix from double-stream block number four. The total view (Fig. \ref{fig:MagnitudeVsImportanceSVDa}) reveals a few significant outliers with very high magnitude and importance. Therefore, to investigate the correlation between magnitude and importance for the majority of singulare values Fig. \ref{fig:MagnitudeVsImportanceSVDb} provides a zoomed-in version. Here, one can observe a general positive trend where higher magnitude correlates with higher importance. However, there are numerous singular values with low magnitude but high importance and high magnitude values with negligible important values. This discrepancy indicates that \gls{GRASP} might effectively identify important features that magnitude-based methods would miss, potentially leading to an improvement over ordinary \gls{SVD}.




    \begin{figure}[H]
	\centering
	% --- First Image ---
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/Experimente/Flux/Training_Free/double_blocks_4_img_mod_lin_grasp.png}
		\caption{Total view}
		\label{fig:MagnitudeVsImportanceSVDa}
	\end{subfigure}% <--- WICHTIG: Dieses % verhindert ein ungewolltes Leerzeichen!
	\hfill
	% --- Second Image ---
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/Experimente/Flux/Training_Free/double_blocks_4_img_mod_lin_grasp_zoom.png}
		\caption{Zoomed in visualization}
		\label{fig:MagnitudeVsImportanceSVDb}
	\end{subfigure}
	
	\caption[Magnitude versus Importance Analysis of Singular Values]{Comparing the magnitude (x-axis) with the gradient-based importance (y-axis) of singular values from \gls{SVD}. The visualization is from the $4^\text{th}$ block of the image modulation matrix. }
	\label{fig:MagnitudeVsImportanceSVD}
\end{figure}


The potential for improvements over \gls{SVD} implied by the analysis of the correlation between magnitude and importance is confirmed by the evaluation of both methods on the HPSv2 and GenEval benchmarks (see Fig. \ref{fig:GRASPvsSVDBenchmarks}). The results show that models obtained using the most important singular values (\gls{GRASP}) instead of those with the highest magnitude outperform standard \gls{SVD} across all compression ratios on both benchmarks. Fig. \ref{fig:ImagesGRASPvsSVD} displays example images, demonstrating that especially for high compression (68\% and 60\%) the divergence between both methods is pronounced. \gls{SVD} based models exhibit significant degradation in image quality and detail at these compression ratios. Furthermore, their text-generation capabilities degrades more severely than those of \gls{GRASP} based models as the kangaroo image still correctly displays "Welcome Friends" at the strongest compression for \gls{GRASP} but not for \gls{SVD} (see second column in Fig. \ref{fig:ImagesGRASPvsSVD}). However, models compressed with \gls{GRASP} also struggles to generate text consistently correctly at high compression (see fourth columns) and for the highest compression rate some severe artifacts (see third column) can be observed.  Nevertheless, both image quality and text generation capabilities are superior with the \gls{GRASP} method. As image details are typically high-frequency information the results suggest that the high-magnitude singular values primarily encode low-frequency information and the low-magnitude singular values often discarded by ordinary \gls{SVD} store the high-frequency information. By prioritizing gradients, \gls{GRASP} better preserves these subtle but important singular values and, thereby mainiting superior text-image alignment and preserving finer details. 


    \begin{figure}[H]
	\centering
	% --- First Image ---
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/Experimente/Flux/Training_Free/GRASP_vs_SVD_hpsv2.png}
		\caption{HPSv2 Benchmark}
		\label{fig:image1}
	\end{subfigure}% <--- WICHTIG: Dieses % verhindert ein ungewolltes Leerzeichen!
	\hfill
	% --- Second Image ---
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/Experimente/Flux/Training_Free/GRASP_vs_SVD_geneval.png}
		\caption{GenEval Benchmark}
		\label{fig:image2}
	\end{subfigure}
	
	\caption[Benchmark Performance: \gls{SVD} versus \gls{GRASP} Compression Method]{Impact of singular value selection based on magnitude (\gls{SVD}) or importance (\gls{GRASP}) on performance across varying compression ratios.}
	\label{fig:GRASPvsSVDBenchmarks}
\end{figure}



    \begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/Experimente/Flux/Training_Free/SVD_vs_GRASP_Images.pdf}
	\caption[Flux.1-dev Images: \gls{SVD} versus \gls{GRASP} Compression]{Images from compression Flux.1-dev with \gls{SVD} or \gls{GRASP} respectively.}
	\label{fig:ImagesGRASPvsSVD}
\end{figure}



Finally, \gls{GRASP} and \gls{SVD} are compared to EcoDiff (see Tab. \ref{tab:TrainingFreeMethods}) demonstrating that these approaches are significantly superior in maintaining image quality for the same degree of parameter reduction. For a parameter reduction of 20\%, the \gls{SVD}-based methods obtain a GenEval score of 0.58 and 0.62 wheras EcoDiff drops to 0.18 indicating a loss of text-alignment. Fig. \ref{fig:GRASP_SVD_EcoDiff} presents example images from GenEval Benchmark providing visual proof of the strong degeneration and poor text-alignment for compressed Flux.1-dev being smaller than 80\% of the original size using EcoDiff. Similarly, the HPSv2 score of EcoDiff measuring human preferences is only 23.6 while \gls{SVD} and \gls{GRASP} reach 30.45 and 30.82.  Notably, when compressing Flux.1-dev by 30\% utilizing the EcoDiff method, the model completely collapses. Visual inspection show that the model ceases to generate meaningful images, producing grey images with undefined texture, in contrast to the performance observed with \gls{GRASP}. This performance gap indicates that strucutrally removing entire neurons breaks the Markovian generation trajectory of Flux.1-dev and validates the effectiveness of the general concept of \gls{SVD} that approximates the original information flow instead of relying on the hard removal of neurons as in EcoDiff. \\
Nevertheless, while all methods effectively reduce the VRAM consumption during inference, standard implementations of \gls{SVD} and \gls{GRASP} do not inherently reduce the inference time as EcoDiff. This is because low-rank decomposition replaces one large matrix by two smaller matrices which doubles the number of matrix multiplications.

	
	\begin{table}[H]
		\centering
		\caption[Benchmark Performance: Training-Free Compression Methods]{Performance Comparison of Compression Methods across HPSv2, GenEval, and DPG Benchmarks}
		\label{tab:TrainingFreeMethods}
		\begin{tabular}{@{}lcccc@{}}
			\toprule
			\textbf{Ratio} & \textbf{Method} & \textbf{HPSv2} $\uparrow$ & \textbf{GenEval} $\uparrow$ & \textbf{DPG} $\uparrow$ \\ \midrule
			
			\multirow{3}{*}{90\%}  
			& EcoDiff & 29.67 & - & - \\
			& SVD &  &  &  \\
			& GRASP  & \textbf{} & \textbf{} & \textbf{} \\ \midrule
			
			\multirow{3}{*}{80\%}  
			& EcoDiff & 23.633 & 0.1873 &  \\
			& SVD & 30.45 & 0.5845 & 81.30 \\
			& GRASP  & \textbf{30.82} & \textbf{0.6232} & \textbf{83.12} \\ \midrule
			
			\multirow{3}{*}{70\%}  
			& EcoDiff & 7.26 &  &  \\
			& SVD & 28.87 & 0.45516 & \\
			& GRASP  & \textbf{30.08} & \textbf{0.48951
			} & \textbf{} \\ \bottomrule
		\end{tabular}
	\end{table}



\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/Experimente/Flux/Training_Free/GRASP_SVD_EcoDiff.pdf}
\caption[Flux.1-dev Images: \gls{SVD} versus \gls{GRASP} versus EcoDiff Compression]{Examples from GenEval benchmark for Flux.1-dev compressed by 10\%, 20\% and 30\% using EcoDiff, \gls{SVD} and \gls{GRASP}.}
	\label{fig:GRASP_SVD_EcoDiff}
\end{figure}


\subsection{Linear Progressive SVD Compression}