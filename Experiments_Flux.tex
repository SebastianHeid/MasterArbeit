\section{Flux.1-dev Pruning}
Flux.1-dev is a 11.9 billion parameter rectified flow model capable of generating detailed, high-quality, and high-aesthetic images. Due to its high parameter count, it is one primary target for model reduction techniques in the literature and should be used to demonstrate the effectiveness of the best settings found for PixArt-$\Sigma$ by comparing it to various competitors. In addition, as training of Flux.1-dev is extremely computatinally intensive, the training-free GRASP method (see sec. \ref{sec:GRASP}) originally developped for \gls{LLM} compression is transfered to Flux1.dev.

\subsection{Training Details}
All Experiments in this section are build upon the training and inference code for flux.1-dev published in the following git repo \cite{kohya_ss_sd_scripts_2024}. The hyperparameters used for finetuning Flux.1-dev after pruning are specified in Tab. \ref{tab:flux_training_config}. For pruning Flux.1-dev the best settings found for PixArt-$\Sigma$ are leveraged and specified here again for completeness:

\begin{itemize}
	\item \textbf{Block Importance Analysis} \\
	To determine the importance of individual blocks in Flux.1-dev, every block is individually compressed by 60\% and a small reference dataset is generated which is leveraged to compute the \gls{CMMD} score. Afterwards, the \gls{CMMD} score is weighted by the number of removed parameters, e.g. a single-stream block contains less parameters than a double-stream block. In contrast to the experiments with PixArt-$\Sigma$, the scoring is computed only once per iteration and not iteratively due to the high computational demands of Flux.1-dev.
	
	\item \textbf{Pruning Method} \\
	Progressive structural model pruning is performed.
	
	\item \textbf{Knowledge Distillation Loss} \\
	The original flow matching loss as well as the normalized intermediate feature and output losses are utilized whereby the normalization is computed for features from double-stream and single-stream block seperately
	\begin{equation}
		\mathcal{L} = \mathcal{L_\text{Task}} + \lambda_\text{OutKD} \mathcal{L_\text{OutKD}} + \lambda_\text{SingleFeatKD} \mathcal{L_\text{SingleFeatKD}} + \lambda_\text{DoubleFeatKD} \mathcal{L_\text{DoubleFeatKD}} \quad.
	\end{equation}
	with $\lambda_\text{OutKD}  = 10$, $\lambda_\text{SingleFeatKD}=0.01$ and $\lambda_\text{DoubleFeatKD}=0.1$.
	
	\item \textbf{Finetuning Protocol} \\
	Only the LAION-Flux dataset is utilized, and a complete finetuning is performed directly. 
	
	\item \textbf{Structural Compression Strategy} \\
	The \gls{SVD} compression is leveraged for compressing Flux.1-dev blocks. The individual components of double-stream and single-stream blocks and their corresponding parameters can be found in Tab. \ref{tab:Flux_parameter_count}.
	
	\item \textbf{\gls{SVD} Compression Scheduling} \\
	
\end{itemize} 

A specific 
\begin{table}[H]
	\centering
	\caption{\textbf{Flux Training and Compression Hyperparameters.} Configuration details for the compressed model fine-tuning run, including optimizer settings and Flow Matching specific parameters. This parameters represent my config setup for the training using the git repository \cite{kohya_ss_sd_scripts_2024}.} As Flux.1-dev is trained on a high-quality but unkown dataset and it was observed for that retraining of compressed models also benefit from high-quality data (see Sec. \ref{sec:FinetuningProtocol}), the LAION-Flux dataset is utilized for finetuing which is a synthetic dataset generated by Flux.1-dev.
	\label{tab:flux_training_config}
	\begin{tabular}{l l}
		\toprule
		\textbf{Hyperparameter} & \textbf{Value} \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Optimization \& Scheduler}}} \\
		Optimizer Type & Adafactor \\
		Optimizer Arguments & \small{relative\_step=False, scale\_parameter=False, warmup\_init=False} \\
		Learning Rate & $1.8 \times 10^{-6}$ \\
		LR Scheduler & constant\_with\_warmup \\
		LR Warmup Steps & 4240 \\
		Gradient Accumulation & 4 \\
		Batch Size & 1 \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Model Precision \& Hardware}}} \\
		Mixed Precision & bf16 \\
		Weight Precision (Base) & fp8 \\
		Full bfloat16 Training & True \\
		Gradient Checkpointing & True \\
		SDPA (Scaled Dot Product Attn) & True \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Flow Matching \& Loss Settings}}} \\
		Discrete Flow Shift & 3 \\
		Timestep Sampling & sigmoid \\
		Max Timestep & 1000 \\
		Loss Type & L2 \\
		Huber Parameter ($c$) & 0.1 \\
		Huber Schedule & snr \\
		Model Prediction Type & raw \\
		Guidance Scale & 1.0 \\
		Sample Sampler & euler\_a \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Architecture \& Data Settings}}} \\
		T5 Max Token Length & 512 \\
		Clip Skip & 1 \\
		Apply T5 Attn Mask & True \\
		Data Loader Workers & 6 \\
		Seed & 42 \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Training-Free Pruning}
In this section, it is investigated if \gls{GRASP} is transferable from \gls{LLM}s to flow-based image generation models like Flux.1-dev. The goal is to identify singular values based on their importance for the image generation task and not based on their magnitude how it is done using ordinary \gls{SVD} compression. \\


double block: reduced by 76\%

\subsection{Linear Progressive SVD Compression}