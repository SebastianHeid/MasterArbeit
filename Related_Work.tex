\chapter{Related Work}

\textbf{Methods to reduce model size}
\begin{itemize}
	\item  pruning (strucured, unstructured, transformer pruning?)
	\item qunatization (SVDQuant)
	\item distillation
	\item weight sharing
	\item token pruning
	\item architecture search (NAS)
\end{itemize}


\textbf{Competitor Methods Flux}
\begin{itemize}
	\item Plug and Play
	\item Flux Mini
	\item Flux Light
	\item Fast Flux
	\item Dense2MoE
	\item Hierarchical Pruning
\end{itemize}


\textbf{Knowledge Distillation Method}
\begin{itemize}
	\item Koala
	\item Laptop
	\item BK-SDM
	\item Progressive Knowledge Distillation -> iteratively remove layers
	\item Tiny Fusion -> learn which block to remove (on DiT)
\end{itemize}

\textbf{Using SVD}
\begin{itemize}
	\item GRASP (for LLMs)
	\item "ASVD: Activation Aware Singular value Decompostion For Compressing Large Language Models" (LLMs)
	\item SVD-LLM V2
	\item (SVD-Delta, SVDQuant) -> machen etwas anderes
	\item "Efficient Low-Rank Diffuison Model Training for Text-to-Image Generation" (Uses exactly like us SVD for Unet diffusion model; was ist OpenReview? Keine Autoren genannt; kein richitges paper)
	\item "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition" 
\end{itemize}




Advances in model performance are achieved not only through new algorithms and architectures, but also through the continuous scaling of parameter counts. This scaling paradigm is facilitated due to the continuous advancement in computation hardware and the availability of internet scale training datasets. While massive architecture yields superior generative capabilities, there is a growing need to achieve comparable performance with smaller models for certain applications and due to environmental concerns, as the hardware required for such large models is extremely expensive and power-intensive leading to a substantial carbon footprint. Furthermore, deplyong these models is often infeasible in ressource constraint setup, e.g. on edge devices. Consequently, a diverse body of literature has emerged around model compression techniques, aiming to minimize the parameter count and the memory footprint during inference and, ideally, also reducing the inference time. This chapter provides a comprehensive overview of various model compression methods, with a particular focus on methodologies adapted for diffusion and flow models.\\

\subsubsection{Quantization}
Quantization is a widely adapted technique to reduce model size and speed up the inference process \cite{cai2020zeroq,jacob2018quantization,han2015deep}. In quantization, the high-precision weight values e.g., FP32, are mapped to discrete or lower-precision values e.g., INT8, thereby significantly reducing the memory footprint required to store the weights. Quantization methods can be categorized in mainly two different approaches.\\
First, post-training quantization (\gls{PTQ}) \cite{he2023ptqd,jacob2018quantization, nagel2020up} describes all methods which operates on fully trained models. They only rely on a small calibration dataset to determine the scaling and zero-point parameters for mapping the high-precision values to their discrete low-precision counterparts. These methods are computationally highly efficient as no training of the complete, potentially very large model is necessary. \\
Second, quantization-aware training (\gls{QAT}) \cite{bengio2013estimating,esser2019learned,he2023efficientdm} integrates the quantization process into the training or finetuning process of the model. During the forward pass the weights are approximated with the chosen low-precision format. However, the rounding function is not differentiable, which is why in the backward pass a straight-through estimator (\gls{STE}) is employed to approximate the gradients. Although the quantized weights are simulated during the forward pass, the high-precision weights are updated during training. Upon completion of the training, the high-precision weights are permanently replaced with the quantized weights. While generally \gls{QAT} leads to superior performance compared to \gls{PTQ} the full retraining of the model makes it computationally demanding.\\
The application of quantization to diffusion and flow-matching models presents unique challenges due to the iterative denoising process, wherein activations are time dependent. Moreover, even minor quantization errors can exert a significant influence because of their accumulation over several inference steps. Consequently, diffusion-specific quantization methods have been developed to address these specifc challenges. Methods such as post-training quantization for diffusion models (\gls{PTQD}) \cite{PTQD} or Q-Diffusion \cite{QDiffusion} leveraging a timestep aware calibration dataset sampling features across the entire reverse diffusion process to compute optimal scaling factors have been proposed. Building on this \cite{QDiT} successfully transfered these advanced quantization methods to \gls{DiT}. A critical challenge inherited from transformer-based architectures is the emergence of extreme activation outliers which is addressed by \cite{li2024svdquant}. They absorbe these anomalous values into high-precision, low-rank components successfully compression models like Flux.1-dev to 4-bit precision. Most recently, \cite{FluxBit} demonstrated that the weights of the Flux.1-dev can be even further compressed to 1.58-bit. \\
Ultimately, quantization serves as an orthogonal technique to model pruning. Once a model's architecture is structurally pruned, post-training quantization can be applied to further reduce its memory footpring.

\subsubsection{VRAM Reduction Only}
Alongside methods that reduce the model parameter count, several strategies have been developed to minimize VRAM consumption during inference. \\
An integral part of modern model architectures is the attention mechanism, which scales quadratically with the sequence length, leading to severe memory bottlenecks. Consequently, hardware-aware algorithms such as FlashAttention \cite{FlashAttention} or xFormers \cite{lefaudeux2022xformers} were introduced to optimize memory access patterns and realize more efficient computation for large attention matrices. \\
Furthermore, modern models such as Flux.1-dev are standardly deployed using 16-bit half-precision foramts. A primary limitation of FP16 is its restricted dynamic range such that gradient or activation outliers can easily exceed the maximum representable value, leading to overflow that crashes both inference and training. For this reason, modern GPUs, e.g. the NVIDIA A100, H100, and H200, natively support the BFloat16 format (BF16). BF16 utilizes the same 16-bit memory footprint as FP16 but preserves the dynamic range of FP32 by sacrificing decimal precision \cite{BF16}. Fortunately, neural networks have proven to be highly robust against slight precision loss. \\
 A third option for reducing VRAM consumption is to load only the currently active layers into the VRAM (GPU), leaving the rest of the model in the RAM (CPU). This results in slower inference and training, as there is overhead involved in moving the model components to the VRAM and back to the RAM. 
 
 \subsubsection{Model Pruning}