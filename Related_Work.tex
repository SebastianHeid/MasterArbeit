\chapter{Related Work}

Advances in model performance are achieved not only through new algorithms and architectures, but also through the continuous scaling of parameter counts. This scaling paradigm is facilitated due to the continuous advancement in computation hardware and the availability of internet scale training datasets. While massive architecture yields superior generative capabilities, there is a growing need to achieve comparable performance with smaller models. The hardware required for such large models is extremely expensive and power-intensive. The latter aspects leads to a substantial carbon footprint and causes environmental concerns. Furthermore, deploying these models is often infeasible in a ressource constraint setup, e.g. on edge devices. Consequently, a diverse body of literature has emerged around model compression techniques, aiming to minimize the parameter count and the memory footprint during inference and, ideally, also reducing the inference time. This chapter provides a comprehensive overview of various model compression methods, with a particular focus on methodologies adapted for diffusion and flow models as well as methods to accelerate inference. Both is critical for edge device deployment.\\

 \subsubsection{Step Distillation}
Diffusion and flow models suffer from long inference times due to their iterative image generation process, which requires repeated application of the model. Step distillation methods achieve a significant speed-up of the image generation process by reducing the number of required inference steps down to four or even a single step \cite{LADD,ADD,ProgressiveDistillation,DistillationMatching,ConsistencyModel,LatentConsistencyModel}. In step distillation frameworks, the teacher and the student model often share the same architecture. The teacher is leveraged to retrain the student to generate images in a few inference steps. To this end, several different approaches have been proposed.\\
One foundational approach is progressive distillation \cite{ProgressiveDistillation} where the number of inference steps is iteratively halved and the previous student model is utilized as the teacher model in the next iteration. Consistency models \cite{ConsistencyModel,LatentConsistencyModel} even achieve a one-step image generation process by enforcing the self-consistency property which dictates that all points on a given diffusion trajectory must be mapped to the same clean image. 
Alternatively, rather than strictly following the teacher's trajectory, recent methods rely on adversarial or statistical matching. Latent adversarial diffusion distillation (\gls{LADD}) \cite{ADD,LADD} incorporates a discriminator in a GAN-like training setup to provide an additional learning signal. Similarly, moving away from exact sample regression,  distribution matching distillation \cite{DistillationMatching} focuses on matching the entire image distribution. Concretely, it optimizes the student model to produce samples that collectively match the statistical properties of real data, and not just the output of the teacher on specific noise-image pairs.\\
Importantly, these step distillation approaches are orthogonal to structural model compression. While this Master's thesis focuses on architectural model reduction via pruning and \gls{SVD}, step distillation can be seamlessly integrated with compressed models to achieve even higher inference efficiency which is demonstrated at the example of Flux.1-schnell in Sec. \cite{bibid}.

\subsubsection{Quantization}
Quantization is a widely adapted technique to reduce model size and speed up the inference process \cite{cai2020zeroq,jacob2018quantization,han2015deep}. In quantization, the high-precision weight values e.g., FP32, are mapped to discrete or lower-precision values e.g., INT8, thereby significantly reducing the memory footprint required to store the weights. Quantization methods can be categorized in mainly two different approaches.\\
First, post-training quantization (\gls{PTQ}) \cite{he2023ptqd,jacob2018quantization, nagel2020up} describes all methods which operate on fully trained models. They only rely on a small calibration dataset to determine the scaling and zero-point parameters for mapping the high-precision values to their discrete low-precision counterparts. These methods are computationally highly efficient as no training of the complete, potentially very large model is necessary. \\
Second, quantization-aware training (\gls{QAT}) \cite{bengio2013estimating,esser2019learned,he2023efficientdm} integrates the quantization process into the training or finetuning process of the model. During the forward pass the weights are approximated with the chosen low-precision format. However, the rounding function is not differentiable. That is why in the backward pass a straight-through estimator (\gls{STE}) is employed to approximate the gradients. Although the quantized weights are simulated during the forward pass, the high-precision weights are updated during training. Upon completion of the training, the high-precision weights are permanently replaced with the quantized weights. While generally \gls{QAT} leads to superior performance compared to \gls{PTQ} the full retraining of the model makes it computationally demanding.\\
The application of quantization to diffusion and flow-matching models presents unique challenges due to the iterative denoising process, wherein activations are time dependent. Moreover, even minor quantization errors can exert a significant influence because of their accumulation over several inference steps. Consequently, diffusion-specific quantization methods have been developed to address these specifc challenges. Methods such as post-training quantization for diffusion models (\gls{PTQD}) \cite{PTQD} or Q-Diffusion \cite{QDiffusion} leveraging a timestep aware calibration dataset sampling features across the entire reverse diffusion process to compute optimal scaling factors have been proposed. Building on this \cite{QDiT} successfully transfered these advanced quantization methods to \gls{DiT}s. A critical challenge inherited from transformer-based architectures is the emergence of extreme activation outliers which is addressed by \cite{li2024svdquant}. They absorbe these anomalous values into high-precision, low-rank components successfully compressing models like Flux.1-dev to 4-bit precision. Most recently, \cite{FluxBit} demonstrated that the weights of the Flux.1-dev can be even further compressed to 1.58-bit. \\\\ 
Ultimately, quantization serves as an orthogonal technique to model pruning. Once a model's architecture is structurally pruned, post-training quantization can be applied to further reduce its memory footpring.

\subsubsection{Non-Architectural VRAM Reduction}
Alongside methods that reduce the model parameter count, several strategies have been developed to minimize VRAM consumption during inference. \\
An integral part of modern model architectures is the attention mechanism, which scales quadratically with the sequence length, leading to severe memory bottlenecks. Consequently, hardware-aware algorithms such as FlashAttention \cite{FlashAttention} or xFormers \cite{lefaudeux2022xformers} were introduced to optimize memory access patterns and realize more efficient computation for large attention matrices. \\
Furthermore, modern models such as Flux.1-dev are standardly deployed using 16-bit half-precision formats. A primary limitation of FP16 is its restricted dynamic range such that gradient or activation outliers can easily exceed the maximum representable value, leading to overflow that crashes both inference and training. For this reason, modern GPUs, e.g. the NVIDIA A100, H100, and H200, natively support the BFloat16 format (BF16). BF16 utilizes the same 16-bit memory footprint as FP16 but preserves the dynamic range of FP32 by sacrificing decimal precision \cite{BF16}. Fortunately, neural networks have proven to be highly robust against slight precision loss. \\
 A third option for reducing VRAM consumption is to load only the currently active layers into the VRAM (GPU), leaving the rest of the model in the RAM (CPU). This results in slower inference and training, as there is overhead involved in moving the model components to the VRAM and back to the RAM. Moreover, during training the optimizer states and gradients can be offloaded to CPU saving memory. These memory amnagement strategies are natively supported by modern training frameworks from huggingface \cite{accelerate} or deepspeed \cite{ren2021zero,rajbhandari2021zero}. 
 
 \subsubsection{Model Pruning}
 Model pruning \cite{liu2020topological,janowsky1989pruning,mozer1989using} focuses on reducing the actual parameter count of models. It can be disected in mainly two categories: structural and unstrcutural model pruning. In unstructured model pruning \cite{nordstrom2022unstructured,frantar2023sparsegpt,han2015learning} individual, less important weights across the model's architecture are masked out such that they do not contribute to the inference process anymore. The process to identify parameters to prune can be based on several criteria, e.g. random weight selection \cite{liu2022unreasonable} or magnitude-based pruning \cite{han2015deep,frankle2018lottery}. While this allows removing many parameters while maintaining high performance and theoretically fewer operations (FLOPs) are executed per inference, it does not necessarily speed up inference or reduce the \gls{VRAM} consumption in practice \cite{wen2016learning,gale2020sparse,ma2018shufflenet}. The reason is that to achieve a reduction in \gls{VRAM} consumption the pruned, sparse matrices need to be converted into a sparse tensor format, e.g. compressed sparse row (CSR) \cite{bell2008efficient}, which stores the non-zero entries and their coordinates leading to a memory overhead that contradicts the efforts to reduce \gls{VRAM}. Therefore, a high degree of sparsity is required to compensate for the overhead and ensure low memory requirements. Moreover, modern GPUs are highly optimized for dense matrix multiplication which is why running inference with sparse tensors on standard hardware may be slower than using dense matrix multiplication. Nevertheless there are emerging custom kernels and specialized hardware to accelerate unstructured sparsity calculations \cite{gale2020sparse}. \\
 This master thesis focuses on the second pruning method, namely structured pruning for transformer-based diffusion and flow models. Unlike unstructured pruning, this method targets coherent model components, such as attention heads, channels, or entire transformer blocks. To recover degraded performance and lost generation capabilities, most methods subsequently apply retraining, specifically knowledge distillation.\\
 Based on the modification strategy the literature can be divided into two main categories, namely approaches that remove model components entirely and those which compresses them.  \\
Most research on compressing Unet-based diffusion models, such as SDXL \cite{SDXL}, adopts the approach of structual component removal  \cite{Koala,Laptop,Bksdm,ProgressiveDistillation,tang2023lightweight}, yet differs significantly in pruning granularity. For instance, \cite{Bksdm} removed predominantly entire middle transformer blocks from the SDXL architecture, identifying them as highly redundant. In contrast, \cite{Laptop} focused on pruning individual transformer layers within the blocks, whereas \cite{Koala} employed a hybrid strategy that combines block removal with layer-wise pruning for higher compression ratios. Notably, \cite{Koala} observed that the decoder plays a more critical role than the encoder of the U-Net for the generation process and should therefore be subject to less aggressive compression.
 To recover the model's fidelity after pruning, most approaches leverage knowledge distillation \cite{KnowledgeDistillation} to retrain their pruned models. They mainly employ the unpruned model as a teacher to guide the recovery process of the pruned student model. Adressing optimization challenges in this phase, \cite{Laptop} demonstrated that using normalized feature loss terms is advantageous as the magnitude of individual features from different layers may vary significantly. Furthermore, \cite{ProgressiveDistillation} demonstrated that a progressive pruning schedule for SDXL leads to superior results compared to one-shot pruning. \\
 Recently, the landscape of diffusion models has shifted from U-Net to purely transformer-based architectures where compression methods become increasingly critical due to the steep increase in model size. The primary advantage of these architectures is their dimensional homogeneity.  Unlike U-Net architectures which may require down- or upsampling and channel adjustements, entire transformer blocks can be removed without adjusting the feature dimension. Besides straightforward transformer block removal strategies \cite{flux1-lite, flux_mini_2024,TinyFusion}, several compression techniques have been proposed using a hybrid depth- and width-pruning paradigma \cite{PPCL,HierarchicalPrune}. Regarding depth reduction, complete transformer blocks are removed. To simultaneously address width \cite{PPCL} identified that especially the text-stream and the \gls{MLP}s of both streams exhibit high redundancy and replaced them with lightweight linear layers. Similarly, \cite{FastFlux} proposes replacing blocks with linear layers and instead of performing a full model finetuning only \gls{LoRA}s on the preceding and subsequent blocks are trained to mitigate the performance loss.\\
 In a distinct approach, \cite{Dense2MoE} leverages a mixture-of-expert (\gls{MoE}) framework to dynamically activate only a small fraction of the model's parameters during each inference step. The core advantage of this approach is that it exploits the temporal variability of feature importance, recognizing that different transformer blocks are citical at different timesteps. Their routing mechanism adaptively selects the most important blocks and experts for the current time step, making the model more expressive than standard static pruning methods. However, while the reduction in activated parameters per inference step significantly accelerates inference, it does not necessarily alleviate the \gls{VRAM} consumption bottleneck. Crucially, since the complete model (including both activated and inactivated parameters) typically remains in memory, the total memory footprint remains close to the original model size, which distinguishes this method significantly from memory-focused pruning methods.\\
 Finally, a distinct category of component compression is low-rank factorizaiton, commonly implemented via singular value decomposition (\gls{SVD}) 
 \cite{zhang2015singular}. This method approximates a dense weight matrix by decomposing it into products of two low-rank matrices, effectively reducing the parameter count while preserving the original architectural topology.  \\
In the domain of \gls{LLM} \gls{SVD} has been extensively investigated as static post-training compression strategy \cite{DLowRankEst,wang2025svd}. However, the straightforward application of \gls{SVD} has proven to lead to significant performance drops even at low compression ratios, since \gls{LLM}s exhibit massive activation outliers and \gls{SVD} removes components critical for these activations as the singular value selection is solely based on their magnitude \cite{yuan2023asvd,GRASP}. \\
In this work, \gls{SVD} is revisited within the context of diffusion and flow models. Unlike the zero-shot compression often applied in \gls{LLM}s, \gls{SVD} is identified as a highly-effective initialization strategy for compressed model weight matrices before retraining. This work demonstrates that a progressive linear compression scheme utilizing \gls{SVD} can effectively compress Flux.1-dev, outperforming \gls{SOTA} pruning approaches on several benchmarks. In addition, it is demonstrated that \gls{SVD} can be effectively deployed as a training-free compression approach for moderate compression ratios while maintaining acceptable performance and that \cite{GRASP} can be transfered from \gls{LLM}s to flow models like Flux.1-dev.
 

 