\chapter{Related Work}

\textbf{Methods to reduce model size}
\begin{itemize}
	\item  pruning (strucured, unstructured, transformer pruning?)
	\item qunatization (SVDQuant)
	\item distillation
	\item weight sharing
	\item token pruning
	\item architecture search (NAS)
\end{itemize}


\textbf{Competitor Methods Flux}
\begin{itemize}
	\item Plug and Play
	\item Flux Mini
	\item Flux Light
	\item Fast Flux
	\item Dense2MoE
	\item Hierarchical Pruning
\end{itemize}


\textbf{Knowledge Distillation Method}
\begin{itemize}
	\item Koala
	\item Laptop
	\item BK-SDM
	\item Progressive Knowledge Distillation -> iteratively remove layers
	\item Tiny Fusion -> learn which block to remove (on DiT)
\end{itemize}

\textbf{Using SVD}
\begin{itemize}
	\item GRASP (for LLMs)
	\item "ASVD: Activation Aware Singular value Decompostion For Compressing Large Language Models" (LLMs)
	\item SVD-LLM V2
	\item (SVD-Delta, SVDQuant) -> machen etwas anderes
	\item "Efficient Low-Rank Diffuison Model Training for Text-to-Image Generation" (Uses exactly like us SVD for Unet diffusion model; was ist OpenReview? Keine Autoren genannt; kein richitges paper)
	\item "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition" 
\end{itemize}




Advances in model performance are achieved not only through new algorithms and architectures, but also through the continuous scaling of parameter counts. This scaling paradigm is facilitated due to the continuous advancement in computation hardware and the availability of internet scale training datasets. While massive architecture yields superior generative capabilities, there is a growing need to achieve comparable performance with smaller models for certain applications and due to environmental concerns, as the hardware required for such large models is extremely expensive and power-intensive leading to a substantial carbon footprint. Furthermore, deplyong these models is often infeasible in ressource constraint setup, e.g. on edge devices. Consequently, a diverse body of literature has emerged around model compression techniques, aiming to minimize the parameter count and the memory footprint during inference and, ideally, also reducing the inference time. This chapter provides a comprehensive overview of various model compression methods, with a particular focus on methodologies adapted for diffusion and flow models.\\

\subsubsection{Quantization}
Quantization is a widely adapted technique to reduce model size and speed up the inference process \cite{cai2020zeroq,jacob2018quantization,han2015deep}. In quantization, the high-precision weight values e.g., FP32, are mapped to discrete or lower-precision values e.g., INT8, thereby significantly reducing the memory footprint required to store the weights. Quantization methods can be categorized in mainly two different approaches.\\
First, post-training quantization (\gls{PTQ}) \cite{he2023ptqd,jacob2018quantization, nagel2020up} describes all methods which operates on fully trained models. They only rely on a small calibration dataset to determine the scaling and zero-point parameters for mapping the high-precision values to their discrete low-precision counterparts. These methods are computationally highly efficient as no training of the complete, potentially very large model is necessary. \\
Second, quantization-aware training (\gls{QAT}) \cite{bengio2013estimating,esser2019learned,he2023efficientdm} integrates the quantization process into the training or finetuning process of the model. During the forward pass the weights are approximated with the chosen low-precision format. However, the rounding function is not differentiable, which is why in the backward pass a straight-through estimator (\gls{STE}) is employed to approximate the gradients. Although the quantized weights are simulated during the forward pass, the high-precision weights are updated during training. Upon completion of the training, the high-precision weights are permanently replaced with the quantized weights. While generally \gls{QAT} leads to superior performance compared to \gls{PTQ} the full retraining of the model makes it computationally demanding.\\
The application of quantization to diffusion and flow-matching models presents unique challenges due to the iterative denoising process, wherein activations are time dependent. Moreover, even minor quantization errors can exert a significant influence because of their accumulation over several inference steps. Consequently, diffusion-specific quantization methods have been developed to address these specifc challenges. Methods such as post-training quantization for diffusion models (\gls{PTQD}) \cite{PTQD} or Q-Diffusion \cite{QDiffusion} leveraging a timestep aware calibration dataset sampling features across the entire reverse diffusion process to compute optimal scaling factors have been proposed. Building on this \cite{QDiT} successfully transfered these advanced quantization methods to \gls{DiT}. A critical challenge inherited from transformer-based architectures is the emergence of extreme activation outliers which is addressed by \cite{li2024svdquant}. They absorbe these anomalous values into high-precision, low-rank components successfully compression models like Flux.1-dev to 4-bit precision. Most recently, \cite{FluxBit} demonstrated that the weights of the Flux.1-dev can be even further compressed to 1.58-bit. \\
Ultimately, quantization serves as an orthogonal technique to model pruning. Once a model's architecture is structurally pruned, post-training quantization can be applied to further reduce its memory footpring.

\subsubsection{VRAM Reduction Only}
Alongside methods that reduce the model parameter count, several strategies have been developed to minimize VRAM consumption during inference. \\
An integral part of modern model architectures is the attention mechanism, which scales quadratically with the sequence length, leading to severe memory bottlenecks. Consequently, hardware-aware algorithms such as FlashAttention \cite{FlashAttention} or xFormers \cite{lefaudeux2022xformers} were introduced to optimize memory access patterns and realize more efficient computation for large attention matrices. \\
Furthermore, modern models such as Flux.1-dev are standardly deployed using 16-bit half-precision foramts. A primary limitation of FP16 is its restricted dynamic range such that gradient or activation outliers can easily exceed the maximum representable value, leading to overflow that crashes both inference and training. For this reason, modern GPUs, e.g. the NVIDIA A100, H100, and H200, natively support the BFloat16 format (BF16). BF16 utilizes the same 16-bit memory footprint as FP16 but preserves the dynamic range of FP32 by sacrificing decimal precision \cite{BF16}. Fortunately, neural networks have proven to be highly robust against slight precision loss. \\
 A third option for reducing VRAM consumption is to load only the currently active layers into the VRAM (GPU), leaving the rest of the model in the RAM (CPU). This results in slower inference and training, as there is overhead involved in moving the model components to the VRAM and back to the RAM. 
 
 \subsubsection{Model Pruning}
 Model pruning focuses on reducing the actual parameter count of models. It can be disected in mainly two categories: structural and unstrcutural model pruning. In unstructural model pruning individual, less important neurons across the model's architecture are identified, e.g. magnitude-based selection, and are masked out and do not contribute anymore to the inference process. The process to identify neurons to prune can be based on a metric, e.g. magnitude based pruning, or they can be chosen at random. While this allows to remove many neurons by maintaining high performance, it does not necessarily reduces the \gls{VRAM} consumption during inference as the weight matrices are sparsified but keep their original physical shape and dimensions. To actually achieve a reduction in \gls{VRAM} consumption the pruned, sparse matrices needs to be converted into a sparse tensor format, e.g. compressed sparse row (CSR) which stores the non-zero entries and its coordinates which lead to a memory overhead contradicting the efforts to reduce \gls{VRAM}. Therefore, a high degree of sparsity is required to compensate for the overhead and ensure low memory requirements. Moreover, modern GPUs are highly optimized for dense matrix mutliplication which is why running inference with sparse tensor on standard hardware is slower than using dense matrix multiplication. Nevertheless there are emerging custom kernels and specialized hardware to accelerate unstructured sparsity. \\
 This master thesis focuses on the second pruning method, namely structured pruning for transformer-based diffusion and flow models. Unlike unstructured pruning, this method targets coherent model components, such as attention heads, channels, or entire transformer blocks. To recover degraded performance and lost generation capabilities, most methods subsequently apply retraining, specifically knowledge distillation.\\
 Based on the modification strategy the literature can be divided into two main categories, namely approaches that remove model components entirely and those which compresses them.  \\
Most research on compressing Unet-based diffusion models, such as SDXL \cite{SDXL}, adopts the approach of structual component removal  \cite{Koala,Laptop,Bksdm,ProgressiveDistillation,tang2023lightweight}, yet differs significantly in pruning granularity. For instance, \cite{Bksdm} removed predominantly entire middle transformer blocks from the SDXL architecture, identifying them as highly redundant. In contrast, \cite{Laptop} focused on pruning individual transformer layers within the blocks, whereas \cite{Koala} employed a hybrid strategy that combines block removal with layer-wise pruning for higher compression ratios. Notably, \cite{Koala} observed that the decoder plays a more critical role than the encoder of the U-Net for the generation process and should therefore be subject to less aggressive compression.
 To recover the model's fidelity after pruning, most approaches leverage knowledge distillation \cite{KnowledgeDistillation} to retrain their pruned models. They mainly employ the unpruned model as a teacher to guide the recovery process of the pruned student model. Adressing optimization challenges in this phase, \cite{Laptop} demonstrated that using normalized features loss terms is advantageous as the magnitude of individual features from different layers may vary significantly. Furthermore, \cite{ProgressiveDistillation} demonstrated that a progressive pruning schedule for SDXL leads to superior results compared to one-shot pruning. \\
 Recently, the landscape of diffusion models has shifted from U-Net to purely transformer-based architectures where compression methods become increasingly critical due to the steep increase in model size. The primary advantage of these architectures is their dimensional homogeneity.  Unlike U-Net architectures which may require down- or upsampling and channel adjustements, entire transformer blocks can be removed without adjusting the feature dimension. Besides straightforward transformer block removal strategies \cite{flux1-lite, flux_mini_2024,TinyFusion}, several compression techniques have been proposed using a hybrid depth- and width-pruning paradigma \cite{PPCL,HierarchicalPrune}. Regarding depth reduction, complete transformer blocks are removed. To simultaneously address width \cite{PPCL} identified that especially the text-stream and the \gls{MLP}s of both streams exhibit high redundancy and replaced them with lightweight linear layers. Similarly, \cite{FastFlux} proposes replacing blocks with linear layers and instead of performing a full model fine-tuning only \gls{LoRA}s on the preceding and subsequent blocks are trained to mitigate the performance loss.\\
 In a distinct approach, \cite{Dense2MoE} leverages a mixture-of-expert (\gls{MoE}) framework to dynamically activate only a small fraction of the model's parameters during each inference step. The core advantage of this approach is that it exploits the temporal variability of feature importance, recognizing that different transformer blocks are citical at different timesteps. Their routing mechanism adaptively selects the most important blocks and experts for the current time step, making the model more expressive than standard static pruning methods. However, while the reduction in activated parameters per inference step significantly accelerates inference, it does not necessarily alleviate the \gls{VRAM} consumption bottleneck. Crucially, since the complete model (including both activated and inactivated parameters) typically remains in memory, the total memory footprint remains closte to the original model size, which distinguishes this method significantly from memory-focused pruning methods.\\
 Finally, a distinct category of component compression is low-rank factorizaiton, commonly implemented via singular value decomposition (\gls{SVD}). This method approximates a dense weight matrix by decomposing it into products of two low-rank matrices, effectively reducing the parameter count while preserving the original architectural topology.  \\
In the domain of \gls{LLM} \gls{SVD} has been extensively investigated as static post-training compression strategy \cite{DLowRankEst,wang2025svd}. However, the straightforward application of \gls{SVD} has proven to lead to significant performance drops even at low compression ratios, since \gls{LLM}s exhibit massive activation outliers and \gls{SVD} removes components critical for these activations as the singular value selection is solely based on their magnitude \cite{yuan2023asvd,GRASP}. \\
In this work, \gls{SVD} is revisited within the context of diffusion and flow models. Unlike the zero-shot compression often applied in \gls{LLM}s, \gls{SVD} is identified as a highly-effective initialization strategy for compressed model weight matrices before retraining. This work demonstrates that a progressive linear compression scheme utilizing \gls{SVD} can effectively compress Flux.1-dev, outperforming \gls{SOTA} pruning approaches on several benchmarks. In addition, it is demonstrated that \gls{SVD} can be effectively deployed as a training-free compression approach for moderate compression ratios while maintaining acceptable performance and that \cite{GRASP} can be transfered from \gls{LLM}s to flow models like Flux.1-dev.
 
 \subsubsection{Making Faster}
 
 
 Caching