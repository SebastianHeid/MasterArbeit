%\section{PixArt-$\Sigma$ Distillation}
%This master thesis investigate structural model distillation techniques, specifically focusing on the removal and compression of transformer blocks within the PixArt-$\Sigma$ and Flux-dev architectures. PixArt-$\Sigma$ is employed as an experimental baseline to evaluate various design choices in the distillation frameworks due to its relatively low parameter count enabling rapid iteration and extensive ablation studies. Subsequently, the most effective strategies are applied to Flux-dev to assess their efficacy in high-parameter regimes. 


\section{Block Importance Analysis}
\subsection{Block Selction Criterias}
\subsubsection{Magnitude-Pruning}
In magnitude pruning \cite{han2015learning,filters2016pruning,lee2020layer}, the magnitudes of the weights of the individual components of a model are computed to serve as a proxy for their importance. The underlying assumption is that small weight magnitudes correspond to a low overall impact on the model performance, implying that model parts with lower weight magnitude should be removed first. The aggregated magnitude $M_i$ of a transformer block $B_i$ is defined as the total Frobenius norm of its constituent weight matrices $W \in B_i$. To account for the multiple sub-layers within a block, we compute the block-wise magnitude as
 \begin{equation} 
 	M_i = \sqrt{\sum_{W \in B_i} |W|_F^2} \quad \text{with} \quad |W|_F = \sqrt{\sum_j \sum_k |w_{jk}|^2} \quad.
 \end{equation} The block with the smallest magnitude $M_s = \min_i(M_i)$ is then identified as the primary candidate for removal or compression.

\subsubsection{Representational Similarity Analysis}
Another approach to identify redundant model components is to compare their input representations with their corresponding outputs representation \cite{men2024shortgpt}. If the transformation between the input and output is marginal, indicating that the specific component performs an approximate identity mapping, the model part is considered a candidate for removal. To quantify the similarity between the input and output features of a transformer block, the central kernel alignment (\gls{CKA}) metric \cite{kornblith2019similarity, pons2024effective} is employed. Let $X \in \mathbb{R}^{n\times d}$ denote the input and $Y \in \mathbb{R}^{n\times d}$ the output features which should be compared. First, the kernels $K$ and $L$ need to be chosen which are the inner product of the input $K=XX^T$ and the output $L=YY^T$ in the linear case. To ensure invariance to mean shifts, the centering matrix $H=I - \frac{1}{n} 11^T$ is applied. The final \gls{CKA} score is given by 
\begin{equation}
	\text{CKA}(K,L) = \frac{\text{HSIC}(K,L)}{\sqrt{\text{HSIC}(K,K) \text{HSIC}(L,L)}} \quad.
\end{equation}
This calculation utilizes the Hilbert-Schmidt Independence Criterion (\gls{HSIC}) \cite{gretton2005measuring} $\text{HSIC}(K,L) = \frac{1}{(n-1)^2}\text{tr}(KHLH)$ \cite{davari2022reliability}.


\subsection{Block Selection Algorithms}
\label{sec:BlockSelectionAlgo}
\subsubsection{Black-Box Optimization with Optuna}
Optuna \cite{akiba2019optuna} is an open-source hyperparameter optimization framework. It primarily leverages a tree-strucured parzen estimator \cite{bergstra2011algorithms}, a bayesian optimization algorithms, to search the space of all possible hyperparmeter settings. It partitions the observed hyperparameter configurations into promising and non-promising candidates based on a user defined objective function. By modeling these two groups using probability density functions it draws a new set of hyperparameters that belongs to the promising group  with high probability. Through this informed search the global optimum can be found much faster than with non-adaptive methods like random or grid search. For an in-depth descritption of the Optuna algorithm the reader is refered to the original paper \cite{akiba2019optuna} because this would go beyond the scope of this work. \\


\subsubsection{TinyFusion}
The authors of TinyFusion \cite{fang2025tinyfusion} propose a learning based block selection method for pruning transformer based image generation models. They hypothesize that instead of focusing on the error/loss in image quality resulting from the model compression it is advantageous to focus on the recoverability capabilities of the compressed model meaning on the post-finetuning performance. Initially, the model is divided into $K$ subparts $\mathbf{\phi} = [\mathbf{\phi}_1,\dots, \mathbf{\phi}_K]$.  Each subpart consists of $M$ transformer blocks $\mathbf{\phi}_k = [\phi_{k1}, \dots, \phi_{kM}]$. Specifying the number $N$ of blocks retained in each subpart a local binary mask $\mathbf{m}_k \in \{0,1\}^M$ is introduced to determine the inclusion or exclusion of the individual blocks. A categorical distribution $p(\mathbf{m}_k)$ is assigned to each mask to model the probability for a specific combination of blocks to be removed. The goal is to jointly optimize the model and the categorical mask distributions.  To ensure that the sampling process of the mask remains differentiable during training the Gumbel-Softmax \cite{jang2016categorical} is utilized. The objective function for jointly optimizing weights and masks is given by
\begin{equation}
	\min_{\{p(\mathbf{m}_k)\}} \underbrace{\min_{\Delta \Phi} \mathbb{E}_{x, \{\mathbf{m}_k \sim p(\mathbf{m}_k)\}} \left[ \mathcal{L}(x, \Phi + \Delta \Phi, \{\mathbf{m}_k\}) \right]}_{\text{\textit{Recoverability: Post-Fine-Tuning Performance}}} 
\end{equation}
where $\Delta \Phi$ represents the updates for the model parameters $\Phi$.
Due to the potential high computational effort of a complete finetuning of a large model, the authors showed that low-rank adaptation (\gls{LoRA}) \cite{hu2022lora} finetuning is sufficient. The joint optimization results in a mask distribution taking into account how removing blocks would influence the post-finetuned model.
\section{Training Data and Image Synthesis}
\subsection{Training Datasets}
\subsubsection{LAION Aesthetic}
LAION-5B \cite{schuhmann2022laion} is a dataset containing 5.85 billion web images of varying sizes, of which 2.32 billion have english captions. All images are filtered using \gls{CLIP} embeddings to prevent that harmful and inappropriate images appear in the dataset. LAION-5B is partitioned into several subsets. Specifically, an aesthetic subset \cite{schuhmann2022laionaesthetic} comprising 120 million images was created by training a linear model on top of the \gls{CLIP} embeddings to predict an aesthetic score. Only images exceeding a certain aesthetic score were added to this subset. In this work, 608k images from the aesthtic subset are utilized as the training dataset. Due to poor quality of the web-scrabed captions which come along with the LAION-5B dataset, new synthetic prompts were generated using the visual language model JoyCaption \cite{fpgaminer2024joycaption}. Five example images of the dataset are displayed in fig. \ref{fig:LaionDataset}.\\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Original_Laion_Dataset.pdf}
	\caption[Example of LAION Dataset]{Examples of LAION dataset. }

	
	\label{fig:LaionDataset}
\end{figure}


\subsubsection{LAION Pixart-$\Sigma$ Dataset}
The pre-trained Pixart-$\Sigma$ model is utilized to generate a synthetic dataset of 100k images conditioned on prompts generated by the JoyCaptions model from the aesthetic LAION subset. Tab. \ref{tab:PixartGeneratedDataset} presents the configuration parameters used in the generation process. Fig. \ref{fig:PixartGeneratedDataset} shows example images based on the JoyCaption prompts providing a direct visual comparison to the original LAION amples presented in fig. \ref{fig:LaionDataset}.
\begin{table}[ht]
	\centering
	\caption{Inference Parameters}
	\label{tab:PixartGeneratedDataset}
	% Wir setzen die Breite auf 50% der Textbreite (0.5\textwidth)
	\begin{tabularx}{0.5\textwidth}{l >{\raggedleft\arraybackslash}X} 
		\toprule
		\textbf{Parameters} & \textbf{Value} \\
		\midrule
		Image Height              & 512 \\
		Image Width               & 512 \\
		Guidance Scale      & 3.5  \\
		No. Inference Steps & 20   \\
		Max Sequence Length & 300  \\
		\bottomrule
	\end{tabularx}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Pixart_Generated_Dataset.pdf}
	\caption[PixArt-$\Sigma$ generated dataset]{Examples of PixArt-$\Sigma$ generated dataset based on JoyCaptions prompts from LAION aesthetic dataset. }
	\label{fig:PixartGeneratedDataset}
\end{figure}

\subsubsection{LAION Flux Dataset}
Following the same methodology as for the LAION-PixArt dataset a LAION Flux dataset was created. Specifically, 190k prompts generated with JoyCaptions based on the aesthetic subset of LAION-5B were utilized to generate synthetic images using the Flux-dev model. The configuration used during the image generation process are detailed in tab. \ref{tab:FluxLaionDataset}. In addition, example images based on the same prompts as thouse used for PixArt dataset (see \ref{fig:PixartGeneratedDataset}) are presented in fig. \ref{fig:FluxLaionDataset}.
\begin{table}[ht]
	\centering
	\caption{Inference Parameters}
	\label{tab:FluxLaionDataset}
	% Wir setzen die Breite auf 50% der Textbreite (0.5\textwidth)
	\begin{tabularx}{0.5\textwidth}{l >{\raggedleft\arraybackslash}X} 
		\toprule
		\textbf{Parameters} & \textbf{Value} \\
		\midrule
		Image Height              & 1024 \\
		Image Width               & 1024 \\
		Guidance Scale      & 3.5  \\
		No. Inference Steps & 50   \\
		Max Sequence Length & 512  \\
		\bottomrule
	\end{tabularx}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Flux_Generated_Dataset.pdf}
	\caption[Flux-dev generated dataset]{Examples of Flux-dev generated dataset based on JoyCaptions prompts from LAION aesthetic dataset. }
	\label{fig:FluxLaionDataset}
	
	\label{fig:PixArtBlockMagnitudeCLIP}
\end{figure}
\subsection{Evaluation Datasets}
\subsubsection{Mapillary}
The Mapillary \cite{neuhold2017mapillary} dataset consists of 25,000 street level images. Additionally, it contains fine-grained semantic masks for 66 different object classes. In order to achieve high diversity, the images originate from various geographical regions, e.g., Europe, North and South America, Africa, Wider Geographic Oceania, Asia, and were captured during different seasons and at varying times of day with diverse lighting conditions. In addition, no image sequences, which are often recorded by car cameras, were integrated, only individual and unique images to avoid redundancy. Most of the images were taken from the street or sidewalk. These were supplemented by images taken on highways, in rural areas, and off-road environments. The use of different camera sensors and perspectives contribute to the diversity of the dataset. For quality assurance, only images with at least full HD resolution ($1920 \times 1080$) were selected that contain only a small amount of motion blurr. Due to privacy protection faces and number plates are blurred.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Mapillary_Dataset.pdf}
	\caption[Examples of Mapillary Dataset]{Examples of Mapillary dataset. }
	\label{fig:MapillaryDataset}
\end{figure}

\subsubsection{Cityscapes}
The Cityscapes \cite{cordts2016cityscapes} dataset contains a total of 25,000 images ($1024 \times 2048$) extracted from stereo-video-sequences of urban street scenes. The images were captured at 50 different cities, in spring, summer or fall. However, there are only images taken during daylight hours and in good to moderate weather conditions. While 20,000 images provide coarse annotations, 5,000 images of the dataset are densely annotated at pixel level covering 30 different object classes.  The data set is highly relevant for the automotive sector, e.g., autonomous driving but is due to its high degree of standardization less diverse than the Mapillary dataset.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Cityscapes_Dataset.pdf}
	\caption[Examples of Cityscapes Dataset]{Examples of Cityscapes dataset. }
	\label{fig:CityscapesDataset}
\end{figure}
\subsubsection{MJHQ-30k}
The MJHQ-30k \cite{li2024playground} dataset consists of 30,000 synthetic images from ten different categories (animals, art, fashion, food, indoor, landscape, logo, people, plants, vehicles) each containing 3,000 images to ensure a balanced distribution. These images were generated using Midjourney 5.2 \cite{li2024playground} and selected based on their text-alignmen (\gls{CLIP}) and their aesthetic \cite{kirstain2023pick} scores. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/MHJQ_30k_Dataset.pdf}
	\caption[Examples of MHJQ-30k Dataset]{Examples of MHJQ-30k dataset. }
	\label{fig:CMHJQDataset}
\end{figure}

\section{Structural Compression Strategies}
There are two primary strategies in structural model distillation. On the one hand, individual parts can be removed completly from the model, or, on the other hand, individual parts can be compressed. The selection strategies of the individual parts are presented in sec. \ref{sec:BlockSelectionAlgo}. In this work, we investigated both compression strategies.
\subsection{Block Removal}
In structural model distillation, entire functional components of a model are often removed completely \cite{Bksdm, Laptop, Koala} while others that are considered as highly relevant for the models performance are maintained. In transformer-based architectures like PixArt-$\Sigma$ and Flux-dev, a natural choice is to dissect the model according to the individual transformer blocks and remove them entirely if they are identified as the least important. Concretely, if block $B_i$ is removed the output features of block $B_{i-1}$ denoted as $x_{i-1}$ are directly passed to block $B_{i+1}$

\begin{equation}
x_{i+1} = B_{i+1}(x_{i-1}) \quad.
\end{equation}


Due to consistent dimensionality of the hidden features across the model's depth no additional dimension adaption needs to be done. 
\subsection{SVD Compression}
Singular value decomposition (\gls{SVD}) \cite{zhang2015singular} factorizes a matrix $W \in \mathbb{R}^{n \times m}$ into three matrices $U, \Sigma, V$ such that 
\begin{equation}
W = U \Sigma V^T
\end{equation}
where $U \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{m \times m}$ are orthonormal and $\Sigma \in \mathbb{R}^{n \times m}$ a diagonal matrix. The values of $\Sigma$ are called singular values and are sorted in descending order $\sigma_1 \geq \sigma_2 \geq \sigma_3 \dots \sigma_p$ with $p = \min(n,m)$. The magnitude of the singular value serves as proxy for the importance of the corresponding rows and columns of $U$ and $V$. In model compression \cite{SVD_LLM_V2,ASVD}, \gls{SVD} can be leveraged to approximate the weight matrices by just using the $k <p$ largest singular values such that $U_\text{red} \in \mathbb{R}^{n \times k}$, $V_\text{red} \in \mathbb{R}^{m \times k}$ and $\Sigma_\text{red} \in \mathbb{R}^{k \times k}$ reducing the total parameter count in dependence of the choice of $k$. Specifically, the matrix $W$ is approximated by two matrices $A=U_\text{red}\sqrt{\Sigma_\text{red}}$ and $B=\sqrt{\Sigma_\text{red}}V_\text{red}^T$. Let $\mathbf{x}$ be the features then instead of $\mathbf{x}_\text{next} = W\mathbf{x} + \mathbf{b} $ the features are updated via 
\begin{equation}
	\mathbf{x}_\text{next} =AB \mathbf{x} +  \mathbf{b}
\end{equation}
where $ \mathbf{b}$ is the bias term which is kept unmodified.
Instead of specifying a specific rank $k$ we want to specify the ratio $r$ of parameters to remove. Assuming the weight matrix $W \in \mathbb{R}^{n \times m}$ the total number of parameters is given by $P_\text{orig} = n \times m$. The target number of parameters can be computed via $P_{\text{targ}} = P_{\text{orig}} \times (1.0 - r)$. The target rank is given by 
\begin{equation}
	k_{\text{targ}} = \max\left(1, \text{int}\left(\frac{P_{\text{targ}}}{n+m}\right)\right) \quad.
\end{equation}


\begin{figure}[t]
	\centering
		\includegraphics[width=\textwidth]{images/Methoden/CompressionRemovalScheme.pdf}
		\caption{Example of removal (left) and compression (right) strategy based on PixArt-$\Sigma$ architecture.}
		\label{fig:CompRemScheme}
\end{figure}

\section{Knowledge Distillation Loss Functions}
\subsection{Objective Functions for Performance Recovery}
\subsection{Feature Alignment Strategies}


\section{Distillation Algorithms}
\subsection{Direct Distillation}
\subsection{Iterative Distillation}
\subsection{Linear Compression Ratio}
