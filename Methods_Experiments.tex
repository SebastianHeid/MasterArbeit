%\section{PixArt-$\Sigma$ Distillation}
%This master thesis investigate structural model distillation techniques, specifically focusing on the removal and compression of transformer blocks within the PixArt-$\Sigma$ and Flux-dev architectures. PixArt-$\Sigma$ is employed as an experimental baseline to evaluate various design choices in the distillation frameworks due to its relatively low parameter count enabling rapid iteration and extensive ablation studies. Subsequently, the most effective strategies are applied to Flux-dev to assess their efficacy in high-parameter regimes. 


\section{Block Importance Analysis}
\label{sec:BlockImportanceAnalyses}
\subsection{Block Selction Criterias}
\subsubsection{Magnitude-Pruning}
\label{sec:MagnitudePruning}
In magnitude pruning \cite{han2015learning,filters2016pruning,lee2020layer}, the magnitudes of the weights of the individual components of a model are computed to serve as a proxy for their importance. The underlying assumption is that small weight magnitudes correspond to a low overall impact on the model performance, implying that model parts with lower weight magnitude should be removed first. The aggregated magnitude $M_i$ of a transformer block $B_i$ is defined as the total Frobenius norm of its constituent weight matrices $W \in B_i$. To account for the multiple sub-layers within a block, we compute the block-wise magnitude as
 \begin{equation} 
 	M_i = \sqrt{\sum_{W \in B_i} |W|_F^2} \quad \text{with} \quad |W|_F = \sqrt{\sum_j \sum_k |w_{jk}|^2} \quad.
 \end{equation} The block with the smallest magnitude $M_s = \min_i(M_i)$ is then identified as the primary candidate for removal or compression.

\subsubsection{Representational Similarity Analysis}
\label{sec:CKA}
Another approach to identify redundant model components is to compare their input representations with their corresponding outputs representations \cite{men2024shortgpt}. If the transformation between the input and output is marginal, indicating that the specific component performs an approximate identity mapping, the model part is considered a candidate for removal. To quantify the similarity between the input and output features of a transformer block, the central kernel alignment (\gls{CKA}) metric \cite{kornblith2019similarity, pons2024effective} is employed. Let $X \in \mathbb{R}^{n\times d}$ denote the input and $Y \in \mathbb{R}^{n\times d}$ the output features which should be compared. First, the kernels $K$ and $L$ need to be chosen which are the inner product of the input $K=XX^T$ and the output $L=YY^T$ in the linear case. To ensure invariance to mean shifts, the centering matrix $H=I - \frac{1}{n} 11^T$ is applied. The final \gls{CKA} score is given by 
\begin{equation}
	\text{CKA}(K,L) = \frac{\text{HSIC}(K,L)}{\sqrt{\text{HSIC}(K,K) \text{HSIC}(L,L)}} \quad.
\end{equation}
This calculation utilizes the Hilbert-Schmidt Independence Criterion (\gls{HSIC}) \cite{gretton2005measuring} $\text{HSIC}(K,L) = \frac{1}{(n-1)^2}\text{tr}(KHLH)$ \cite{davari2022reliability}.

\subsubsection{Metric Based Selection}
\label{sec:MetricBasedSelection}
A third approach to identify the least important model components is to remove or compress all components individually, e.g. each transformer block, and compute for each configuration image quality metrics on a small reference dataset. Specifically, let $\{B_1, \dots, B_L\}$ denote the individual blocks of a transformer based model architecture. Each block is modified seperately leading to $L$ different model configuration $\{\{\hat{B}_1, \dots, B_L\}$, $\dots$, $\{B_1, \dots, \hat{B}_i, \dots,  B_L\}$, $\dots$,$\{B_1, \dots, \hat{B}_L\}\}$ where $\hat{B}_i$ indicates that the $i^\text{th}$ block is modified. For each model configuration the metrics are computed. In this work, we employ \gls{CMMD} and/or \gls{CLIP}-score to rank the importance of the individual model components. While \gls{CLIP} is used to evaluate the semantic prompt-image coherence, \gls{CMMD} serves as a measure for the general image quality and distributional fidelity. Since \gls{CMMD} and \gls{CLIP}-score produce values in different ranges and have an inverse optimization direction (minimization vs maximizaiton), for both metrics a separate ranking of the blocks is created where $r_{\text{CMMD},i}$ and  $r_{\text{CLiP},i}$ are the corresponding ranks of block $B_i$. The final importance score is obtained by computing the aggregated rank $r_{\text{final},i}$ from both metrics
\begin{equation}
	r_{\text{final},i} = r_{\text{CMMD},i} + r_{\text{CLIP},i} \quad.
	\label{eq:combined_importance}
\end{equation}
In our linear progressive compression approach (see sec. \ref{sec:LinearProgressiveCompression}) the compression-scheme of the individual model parts leads to different numbers of removed parameters due to repeated compression of individual blocks and different compression ratios per block. Therefore, only the \gls{CMMD} value per removed parameters $\frac{\text{CMMD}_i}{\text{Removed Parameters}}$ is leveraged to quantify the degradation of image quality per removed parameter and to ensure fair comparison.
\subsection{Block Selection Algorithms}
One challenge in structural model compression using metric based selection is that the importance of individual model components, e.g. transformer blocks, are normally determined in isolation meaning only one block is modified and the remaing parts of the model are fixed for computing the metrics.  However, in most cases, several components are compressed simultaneously. It cannot necessarily be assumed that removing the model components that individually had the least impact on model performance will also result in the best combination with the least reduction in image quality. In the following, methods to select the best subset of the model components are described and investigated in sec. \ref{label}.
\label{sec:BlockSelectionAlgo}
\subsubsection{Greedy Block Selection}
\label{sec:GreedyBlockSelection}
The greedy algorithm is applied alongside the metric-based selection criteria. It is an iterative procedure where the importance of all uncompressed blocks is determined to identify and compress the least critical one. Following this step, the block importance is re-evaluated for all remaining uncompressed blocks. Concretely, let the model consist of $L$ transformer blocks $B=\{B_1, \dots, B_L\}$. 
In the first iteration, for all $L$ combinations of model configurations $\mathcal{M}^{(1)} = \{ (B \setminus \{B_i\}) \cup \{\hat{B}_i\} \mid 1 \leq i \leq L \}$ the metrics are computed and the best block $B_j$ is selected according to the ranking. Let $\bar{B}_j$ denote the compressed version of block $B_j$.
 In the second iteration, the metrics are re-computed for all $L-1$ uncompressed blocks
 $\mathcal{M}^{(2)} = \{ (B \setminus \{B_i, B_j\}) \cup \{\hat{B}_i, \bar{B}_j\} \mid 1 \leq i \leq L, i \neq j \}$.  For the $n^\text{th}$ iteration $L-(n-l)$ uncompressed blocks remains. Let $B_\text{comp}$ denote the set of all previously compressed blocks then the potential candidates for compression are given by  $\mathcal{M}^{(n)} = \{ (B \setminus (B_\text{comp} \cup {B_i})) \cup B_\text{comp} \cup {\hat{B}_i} \mid B_i \in B \setminus B_\text{comp} \}$.
 This process is repeated until the desired number of blocks to be compresssed has been identified.
%  $\{\{\hat{B}_1, \dots, \bar{B}_j, \dots, B_L\}$, $\dots$, $\{B_1, \dots, \hat{B}_{j-1}, \bar{B}_j, \dots,  B_L\}$, $\{B_1, \dots, \bar{B}_j, \hat{B}_{j+1}, \dots,  B_L\}$, $\dots$$\{B_1, \dots, \bar{B}_j, \dots, \hat{B}_L\}\}$. 
%  
  % $\{\{\hat{B}_1, \dots, B_L\}$, $\dots$, $\{B_1, \dots, \hat{B}_i, \dots,  B_L\}$, $\dots$,$\{B_1, \dots, \hat{B}_L\}\}$ 
  
\subsubsection{Black-Box Optimization with Optuna}
\label{sec:Optuna}
Optuna \cite{akiba2019optuna} is an open-source hyperparameter optimization framework. It primarily leverages a tree-strucured parzen estimator \cite{bergstra2011algorithms}, a bayesian optimization algorithms, to search the space of all possible hyperparmeter settings. It partitions the observed hyperparameter configurations into promising and non-promising candidates based on a user defined objective function. By modeling these two groups using probability density functions it draws a new set of hyperparameters that belongs to the promising group  with high probability. Through this informed search the global optimum can be found much faster than with non-adaptive methods like random or grid search. For an in-depth descritption of the Optuna algorithm the reader is refered to the original paper \cite{akiba2019optuna} because this would go beyond the scope of this work. \\


\subsubsection{TinyFusion}
\label{sec:TinyFusion}
The authors of TinyFusion \cite{fang2025tinyfusion} propose a learning based block selection method for pruning transformer based image generation models. They hypothesize that instead of focusing on the error/loss in image quality resulting from the model compression it is advantageous to focus on the recoverability capabilities of the compressed model meaning on the post-finetuning performance. Initially, the model is divided into $K$ subparts $\mathbf{\phi} = [\mathbf{\phi}_1,\dots, \mathbf{\phi}_K]$.  Each subpart consists of $M$ transformer blocks $\mathbf{\phi}_k = [\phi_{k1}, \dots, \phi_{kM}]$. Specifying the number $N$ of blocks retained in each subpart a local binary mask $\mathbf{m}_k \in \{0,1\}^M$ is introduced to determine the inclusion or exclusion of the individual blocks. A categorical distribution $p(\mathbf{m}_k)$ is assigned to each mask to model the probability for a specific combination of blocks to be removed. The goal is to jointly optimize the model and the categorical mask distributions.  To ensure that the sampling process of the mask remains differentiable during training the Gumbel-Softmax \cite{jang2016categorical} is utilized. The objective function for jointly optimizing weights and masks is given by
\begin{equation}
	\min_{\{p(\mathbf{m}_k)\}} \underbrace{\min_{\Delta \Phi} \mathbb{E}_{x, \{\mathbf{m}_k \sim p(\mathbf{m}_k)\}} \left[ \mathcal{L}(x, \Phi + \Delta \Phi, \{\mathbf{m}_k\}) \right]}_{\text{\textit{Recoverability: Post-Fine-Tuning Performance}}} 
\end{equation}
where $\Delta \Phi$ represents the updates for the model parameters $\Phi$. The learned mask provides the infromation which transformer blocks are essential for the recoverability of the network.
Due to the potential high computational effort of a complete finetuning of a large model, the authors showed that low-rank adaptation (\gls{LoRA}) \cite{hu2022lora} finetuning is sufficient. The joint optimization results in a mask distribution taking into account how removing blocks would influence the post-finetuned model.
\section{Training Data and Image Synthesis}
\subsection{Training Datasets}
\subsubsection{LAION Aesthetic}
LAION-5B \cite{schuhmann2022laion} is a dataset containing 5.85 billion web images of varying sizes, of which 2.32 billion have english captions. All images are filtered using \gls{CLIP} embeddings to prevent that harmful and inappropriate images appear in the dataset. LAION-5B is partitioned into several subsets. Specifically, an aesthetic subset \cite{schuhmann2022laionaesthetic} comprising 120 million images was created by training a linear model on top of the \gls{CLIP} embeddings to predict an aesthetic score. Only images exceeding a certain aesthetic score were added to this subset. In this work, 608k images from the aesthtic subset are utilized as the training dataset. Due to poor quality of the web-scrabed captions which come along with the LAION-5B dataset, new synthetic prompts were generated using the visual language model JoyCaption \cite{fpgaminer2024joycaption}. Five example images of the dataset are displayed in fig. \ref{fig:LaionDataset}.\\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Original_Laion_Dataset.pdf}
	\caption[Example of LAION Dataset]{Examples of LAION dataset. }

	
	\label{fig:LaionDataset}
\end{figure}


\subsubsection{LAION Pixart-$\Sigma$ Dataset}
The pre-trained Pixart-$\Sigma$ model is utilized to generate a synthetic dataset of 100k images conditioned on prompts generated by the JoyCaptions model from the aesthetic LAION subset. Tab. \ref{tab:PixartGeneratedDataset} presents the configuration parameters used in the generation process. Fig. \ref{fig:PixartGeneratedDataset} shows example images based on the JoyCaption prompts providing a direct visual comparison to the original LAION samples presented in fig. \ref{fig:LaionDataset}.
\begin{table}[ht]
	\centering
	\caption{Inference Parameters}
	\label{tab:PixartGeneratedDataset}
	% Wir setzen die Breite auf 50% der Textbreite (0.5\textwidth)
	\begin{tabularx}{0.5\textwidth}{l >{\raggedleft\arraybackslash}X} 
		\toprule
		\textbf{Parameters} & \textbf{Value} \\
		\midrule
		Image Height              & 512 \\
		Image Width               & 512 \\
		Guidance Scale      & 3.5  \\
		No. Inference Steps & 20   \\
		Max Sequence Length & 300  \\
		\bottomrule
	\end{tabularx}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Pixart_Generated_Dataset.pdf}
	\caption[PixArt-$\Sigma$ generated dataset]{Examples of PixArt-$\Sigma$ generated dataset based on JoyCaptions prompts from LAION aesthetic dataset. }
	\label{fig:PixartGeneratedDataset}
\end{figure}

\subsubsection{LAION-Flux Dataset}
Following the same methodology as for the LAION-PixArt dataset a LAION-Flux dataset was created. Specifically, 190k prompts generated with JoyCaptions based on the aesthetic subset of LAION-5B were utilized to generate synthetic images using the Flux-dev model. The configuration used during the image generation process are detailed in tab. \ref{tab:FluxLaionDataset}. In addition, example images based on the same prompts as those used for the LAION- PixArt dataset (see \ref{fig:PixartGeneratedDataset}) are presented in fig. \ref{fig:FluxLaionDataset}.
\begin{table}[ht]
	\centering
	\caption{Inference Parameters}
	\label{tab:FluxLaionDataset}
	% Wir setzen die Breite auf 50% der Textbreite (0.5\textwidth)
	\begin{tabularx}{0.5\textwidth}{l >{\raggedleft\arraybackslash}X} 
		\toprule
		\textbf{Parameters} & \textbf{Value} \\
		\midrule
		Image Height              & 1024 \\
		Image Width               & 1024 \\
		Guidance Scale      & 3.5  \\
		No. Inference Steps & 50   \\
		Max Sequence Length & 512  \\
		\bottomrule
	\end{tabularx}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Flux_Generated_Dataset.pdf}
	\caption[Flux-dev generated dataset]{Examples of Flux-dev generated dataset based on JoyCaptions prompts from LAION aesthetic dataset. }
	\label{fig:FluxLaionDataset}
	
	\label{fig:PixArtBlockMagnitudeCLIP}
\end{figure}
\subsection{Evaluation Datasets}
\subsubsection{Mapillary}
The Mapillary \cite{neuhold2017mapillary} dataset consists of 25,000 street level images. Additionally, it contains fine-grained semantic masks for 66 different object classes. In order to achieve high diversity, the images originate from various geographical regions, e.g., Europe, North and South America, Africa, Wider Geographic Oceania, Asia, and were captured during different seasons and at varying times of day with diverse lighting conditions. In addition, no image sequences, which are often recorded by car cameras, were integrated, only individual and unique images to avoid redundancy. Most of the images were taken from the street or sidewalk. These were supplemented by images taken on highways, in rural areas, and off-road environments. The use of different camera sensors and perspectives contribute to the diversity of the dataset. For quality assurance, only images with at least full HD resolution ($1920 \times 1080$) were selected that contain only a small amount of motion blurr. Due to privacy protection faces and number plates are blurred.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Mapillary_Dataset.pdf}
	\caption[Examples of Mapillary Dataset]{Examples of Mapillary dataset. }
	\label{fig:MapillaryDataset}
\end{figure}

\subsubsection{Cityscapes}
The Cityscapes \cite{cordts2016cityscapes} dataset contains a total of 25,000 images ($1024 \times 2048$) extracted from stereo-video-sequences of urban street scenes. The images were captured at 50 different cities, in spring, summer or fall. However, there are only images taken during daylight hours and in good to moderate weather conditions. While 20,000 images provide coarse annotations, 5,000 images of the dataset are densely annotated at pixel level covering 30 different object classes.  The dataset is highly relevant for the automotive sector, e.g., autonomous driving but is due to its high degree of standardization less diverse than the Mapillary dataset.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/Cityscapes_Dataset.pdf}
	\caption[Examples of Cityscapes Dataset]{Examples of Cityscapes dataset. }
	\label{fig:CityscapesDataset}
\end{figure}
\subsubsection{MJHQ-30k}
The MJHQ-30k \cite{li2024playground} dataset consists of 30,000 synthetic images from ten different categories (animals, art, fashion, food, indoor, landscape, logo, people, plants, vehicles) each containing 3,000 images to ensure a balanced distribution. These images were generated using Midjourney 5.2 \cite{li2024playground} and selected based on their text-alignmen (\gls{CLIP}) and their aesthetic \cite{kirstain2023pick} scores. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Methoden/MHJQ_30k_Dataset.pdf}
	\caption[Examples of MHJQ-30k Dataset]{Examples of MHJQ-30k dataset. }
	\label{fig:CMHJQDataset}
\end{figure}

\section{Structural Compression Strategies}
\label{sec:StructuralCompressionStrategy}
There are two primary strategies in structural model pruning. On the one hand, individual model parts can be removed entirely, or, on the other hand, they can be compressed. In transformer-based model architectures like PixArt-$\Sigma$ and Flux-dev, a natural choice is to dissect the model according to the individual transformer blocks. Therefore, when referring to model parts in the following, this always refers to the individual transformer blocks. Both strategies are described in the following. 
\subsection{Block Removal}
In structural model pruning, entire functional components of a model are often removed completely \cite{Bksdm, Laptop, Koala} while others that are considered as highly relevant for the models performance are maintained. Concretely, if the transformer block $B_i$ is removed the output features of block $B_{i-1}$ denoted as $x_{i-1}$ are directly passed to block $B_{i+1}$

\begin{equation}
x_{i+1} = B_{i+1}(x_{i-1}) \quad.
\end{equation}


Due to consistent dimensionality of the hidden features across the model's depth no additional dimension adaption needs to be done. 
\subsection{SVD Compression}
Singular value decomposition (\gls{SVD}) \cite{zhang2015singular} factorizes a matrix $W \in \mathbb{R}^{n \times m}$ into three matrices $U, \Sigma, V$ such that 
\begin{equation}
W = U \Sigma V^T
\end{equation}
where $U \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{m \times m}$ are orthonormal and $\Sigma \in \mathbb{R}^{n \times m}$ a diagonal matrix. The entries of $\Sigma$ are called singular values and are sorted in descending order $\sigma_1 \geq \sigma_2 \geq \sigma_3 \dots \sigma_p$ with $p = \min(n,m)$. The magnitude of the singular value serves as proxy for the importance of the corresponding rows and columns of $U$ and $V$. In model compression \cite{SVD_LLM_V2,ASVD}, \gls{SVD} can be leveraged to approximate the weight matrics by just using the $k <p$ largest singular values such that $U_\text{red} \in \mathbb{R}^{n \times k}$, $V_\text{red} \in \mathbb{R}^{m \times k}$ and $\Sigma_\text{red} \in \mathbb{R}^{k \times k}$ reducing the total parameter count in dependence of the choice of $k$. Specifically, the matrix $W$ is approximated by two matrices $A=U_\text{red}\sqrt{\Sigma_\text{red}}$ and $B=\sqrt{\Sigma_\text{red}}V_\text{red}^T$. Let $\mathbf{x}$ be the features then instead of $\mathbf{x}_\text{next} = W\mathbf{x} + \mathbf{b} $ the features are updated via 
\begin{equation}
	\mathbf{x}_\text{next} =AB \mathbf{x} +  \mathbf{b}
\end{equation}
where $ \mathbf{b}$ is the bias term which is kept unmodified.
Instead of specifying a specific rank $k$ we want to specify the percentage $c$ of parameters to remove. Assuming the weight matrix $W \in \mathbb{R}^{n \times m}$ the total number of parameters is given by $P_\text{orig} = n \times m$. The target number of parameters can be computed via $P_{\text{targ}} = P_{\text{orig}} \times (1.0 - c)$. The target rank is given by 
\begin{equation}
	k_{\text{targ}} = \max\left(1, \text{int}\left(\frac{P_{\text{targ}}}{n+m}\right)\right) \quad.
\end{equation}


\begin{figure}[H]
	\centering
		\includegraphics[width=\textwidth]{images/Methoden/CompressionRemovalScheme.pdf}
		\caption{Example of removal (left) and compression (right) strategy based on a simplified representation of PixArt-$\Sigma$ architecture. The depictions show the case that the blocks 8,11,15,17 and 20 are removed or compressed. }
		\label{fig:CompRemScheme}
\end{figure}



\subsection{Iteratively Repeated SVD Compression}
\label{sec:IterativeRepeatedSVDCompression}
In our final compression setup (see sec. \ref{sec:LinearProgressiveCompression}), individual transformer blocks are compressed and subsequently retrained multiple times using \gls{SVD}. Initially, an original weight matrix $W$ is decomposed into matrices $A$ and $B$ such as $W \approx AB$. During the subsequent retraining the matrices $A$ and $B$ are updated to $A_t$ and $B_t$. Therefore, $W \approx A_tB_t$ does not necessarily need to hold anymore w.r.t. the initial $W$ as the model adapts during training to the low rank constraint. Therefore, if the transformer block which originally contained the weight matrix $W$ is to be compressed once more, first, the current matrix $W_t = A_t B_t$ is reconstructed and afterward the \gls{SVD} algorithm is performed on it $W_t = U^{(2)} \Sigma^{(2)} V^{(2)^T}$. Similar to the initial step, after choosing a further reduced rank, the new smaller matrices $A^{(2)}$ and $B^{(2)}$ are constructed via $A^{(2)} = U^{(2)}_\text{red} \sqrt{\Sigma_\text{red}^{(2)}}$  and $B^{(2)} =  \sqrt{\Sigma_\text{red}^{(2)}}V_\text{red}^{(2)^T}$.  By following this procedure, reconstrucing $W^{(j)}_t = A^{(j)}_t B^{(j)}_t$, chosing a new smaller rank and obtaining the new $A^{(j+1)} = U^{(j+1)}_\text{red} \sqrt{\Sigma_\text{red}^{(j+1)}}$  and $B^{(j+1)} =  \sqrt{\Sigma_\text{red}^{(j+1)}}V_\text{red}^{(j+1)^T}$, a transformer block can be gradually compressed across multiple iterations.


\subsection{GRASP}
The method gradient-based retention of adaptive singular parameters \gls{GRASP} was originally proposed for compressing \gls{LLM}s \cite{liu2025grasp}. It is based on the idea of exploiting the low-rank structure of redundant layers in \gls{LLM}s. Moreover, the authors hypothesized that the standard \gls{SVD} approach where the singular values are selected solely based on their magnitude is not optimal because even small singular values may be important for the final task of the model and should be preserved. Therefore, they proposed a gradient-based selection instead of a magnitude-based selection for the singular values. Here, we adapted the scheme to Flux-dev to investigate its applicability to flow models. First, the candidate blocks for pruning are identified with a method from sec. ref{sec:BlockImportanceAnalyses}. Next, the identified blocks are compressed via \gls{SVD}. Afterwards, all parameters are frozen and only the singular values are treated as the parameters for which the gradients are calculated. We utilized a calibration dataset consisting of 200 images. For each image, the gradients of each singular value based on all timesteps 1 to 1000 were computed and summed together. Finally, the gradients for all 200 images were averaged to obtain a final estimate of the importance of each singular value. Instead of choosing the $k$ largest singular values, the $k$ singular values with the largest gradients were chosen for the low-rank matrix approximating the original weight matrix. 
%Assuming the original weight matrix $W$ is already compressed $j^\text{th}$ times such that it is replaced with the two matrices $A^{(j)}$ and $B^{(j)}$. Note, that $A^{(j)}$ and $B^{(j)}$ were finetuned during the compression process which means   and it should be reduced once again. During retraining  $W \approx AB$
%
%
%
%
%we adopt an iterative \gls{SVD} approach, which involves an iterative compression and finetuning scheme. In this approach, the original weight matrix $W$ is gradually compressed. Let $j$ denote the number of compressions already performed for the corresponding weight matrix. To compress it a $(j+1)^\text{th}$ time, first the full matrix based on the finetuned matrices $A^{(j)}$ and $B^{(j)}$, namely $W^{(j)} = A^{(j)}B^{(j)}$ is constructed. Then, \gls{SVD} is performed to obtain $W^{(j)} = U^{(j+1)} \Sigma^{(j+1)} V^{(j+1)^T}$. Assuming $k^{(j)}$ is the rank of the $j^{\text{th}}$ compression, a new smaller rank $k^{(j+1)} < k^{(j)} $ is used to obtain $A^{(j+1)} = U^{(j+1)}_\text{red} \sqrt{\Sigma_\text{red}^{(j+1)}}$  and $B^{(j+1)} =  \sqrt{\Sigma_\text{red}^{(j+1)}}V_\text{red}^{(j+1)^T}  $ which are then leveraged as the new approximation for the weight matrix. This strategy allows the compression of individual blocks multiple times less intensely. 

\section{Knowledge Distillation Loss Functions}
The general learning objective for knowledge distillation as described in eq. \ref{eq:KnowledgeDistillationLoss} is
\begin{equation}
	\mathcal{L} = \mathcal{L_\text{Task}} + \lambda_\text{OutKD} \mathcal{L_\text{OutKD}} + \lambda_\text{FeatKD} \mathcal{L_\text{FeatKD}} \quad.
\end{equation}
In the experiment section \ref{label} we investigate the importance of the individual components.
\subsection{Normalization}
An important point to note is that the features of different blocks may have varying magnitudes wherefore \cite{Laptop} found that applying a normalization to the features is beneficial. Therefore, after computing the difference between the teacher and the studen features a normalization is applied such as the individual blocks contribute equally to $\mathcal{L_\text{FeatKD}}$. The normalization which is utilized for this works is taken from \cite{Dense2MoE}.  Let $T_l$ and $S_l$ denote the features of the teacher and the student from transformer block $l \in \{1,\dots,L\}$. For a batch size of B, each feature tensor has the dimension $(B,D_l)$ where $D_l$ is the total numbers of flattened features. The $i$-th sample in the batch for layer $l$ is denoted as $T_{i,l}$ and respectively $S_{i,l}$.  
First, the \gls{MSE} is computed between the teacher and the student featurs 
\begin{equation}
\mathcal{L}_{i,l} = \frac{1}{D_l} || T_{i,l} - S_{i,l}||_2^2
\end{equation}
Next, the root-mean square of the teacher features is computed acting as measure for the feature magnitude
\begin{equation}
 \eta_{i,l} = \sqrt{\frac{1}{D_l} ||T_{i,l}||^2_2}
\end{equation}
Additionally, the loss is scaled by the magnitude of the task loss $\mathcal{L}_{i,\text{Task}}$. The normalization factor is then given by

\begin{equation}
	w_{i,l} = \underbrace{\left( \frac{\mathcal{L}_{i,\text{Task}}}{\mathcal{L}_{i,l} + \epsilon} \right)}_{\text{Loss Balancing}} \cdot \underbrace{\left( \frac{\sum_{k=1}^{L} \eta_{i,k}}{L \cdot \eta_{i,l} + \epsilon} \right)}_{\text{Norm Normalization}}
\end{equation}
where a small $\epsilon$ is added to the denominator to prevent devision by zero. The final feature loss is computed by
\begin{equation}
	\mathcal{L}_\text{FeatKD} = \frac{1}{B}\sum_{i=1}^{B} \left(\frac{1}{L} \sum_{l=1}^{L} w_{i,l} \cdot \mathcal{L}_{i,l}\right) \quad.
\end{equation}
Please note, if the normalization is applied it is not only applied to $\mathcal{L}_\text{FeatKD}$ but also to $\mathcal{L_\text{OutKD}}$ which compares the final output of the teacher with the studen model.
\subsection{Feature Mapping for Loss Computation}
The assignment of the learning signals based on the intermediate features varies between the complete block removal and the \gls{SVD} block compression approach (see fig. \ref{fig:LossRemovalCompression}). In the \gls{SVD} compression approach, the intermediate features of every student block $S_l$ are directly compared with the feature of the teachers' block $T_l$ with $l \in \{1,\dots,L\}$ assuming the model consists of $L$ transformer blocks. \\
In contrast, the selection of intermediate features requires more care in the block removal approach where a dynamic mapping strategy is needed. If a block $B_l$ is removed, the features of the preceding student block $S_{l-1}$ is compared with the features of the teacher's block $T_l$. This effectively forces block $B_{l-1}$ to approximate the removed block $B_l$. Consequently, if several consecutive blocks $B_{l-i}$ through $B_l$ (with $i\geq1$) are removed the student feature $S_{l-(i+1)}$ is compared to $T_l$.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/Methoden/LossRemovalCompression.pdf}
	\caption{ }
	\label{fig:LossRemovalCompression}
\end{figure}


\

\section{Pruning Schedule}
Various approaches can be chosen for structural model pruning. 
\label{sec:distillationAlgos}
\subsection{One-Shot Model Compression}
The standard approach of compressing transformer based diffusion and flow models \cite{Laptop, Koala, flux1-lite, flux_mini_2024} involves three main steps. First, the transformer blocks $\{B_1, \dots, B_L\}$ are sorted according to their importance, e.g. using \gls{CLIP}-score and \gls{CMMD} as importance proxy. Let $\{B^*_1, \dots, B^*_L\}$ denote the sorted blocks, where $B^*_1$ represents the least important block. Next, based on the desired compression ratio of the total model, a number $k \leq L$ of blocks is determined. Consequently, the blocks $\{B^*_1, \dots, B^*_k\}$ are modified based on the chosen compression strategy (see sec. \ref{sec:StructuralCompressionStrategy}). Finally, the compressed model is re-trained, e.g. using knowledge distillation, to recover the generation capabilities. This approach introduce a significant, abrupt structural alteration of the model, leading to temporary severe degeneration in image generation capabilities.

\subsection{Progressive Model Compression}
In contrast to one-shot model compression, progressive model compression, e.g. \cite{ProgressiveDistillation}, iteratively applies the three-step procedure described above. In each iteration the importance of the blocks is re-evaluated, the number of parameters to be removed specifies how many blocks need to be modified, and finally the model is retrained. The crucial difference is that the number of parameters removed in a single step is significantly smaller than in one-shot model compression. This leads to gradual and less severe performance degradation in one iteration which is easier recovarable. In the subsequent iteration, the block importance is re-assessed for all remaining unmodified blocks $\{B_1, \dots, B_L\}\setminus \{B^*_1, \dots, B^*_k\}$, as the relative block ranking may have changed as a result of the retraining. Another design choice concerns the number of parameters that are removed per iteration. On the one hand, the number of parameters can be set to a fixed value or, on the other hand, it could be chosen dynamically based on the importance metrics, e.g. by defining a threshold \gls{CMMD} value and performing retraining as if the model newly compressed model reaches it. This typically implies that in early iterations more parameters are removed than in later iterations.  


\subsection{Linear Progressive Compression}
\label{sec:LinearProgressiveCompression}
Our approach, linear progressive compression, is a further development of progressive model compression. Instead of compressing all blocks uniformly and each block only once, the compression ratio of each block is determined depending on the importance of the block and every block can be modified in several different iterations. \\
 The approach consists of the same three main steps. First, the block importance is determined by leveraging the \gls{CMMD} value per removed parameter. Let $P_i$ $(i \in \{1, \dots, L\})$ denote the current number of paramters of block $B_i$ which may differ between blocks as the blocks may have been already compressed during previous iterations. For generating the images based on those the \gls{CMMD} values are computed, each block is compressed by a fixed percentage $c_\text{fixed}$. Specifically, this means that potentially different number of parameters are removed for each block. Therefore, the final importance score for each block $B_i$ is given by 
 \begin{equation}
 	s_i = \frac{\text{CMMD}_i}{P_i \cdot c_\text{fixed}}
 \end{equation}
where $\text{CMMD}_i$ is the \gls{CMMD} value obtained by the dataset generated with the model where block $B_i$ is compressed leading to the sorted order of block $\{B^*_1, \dots, B^*_L\}$ where $B_1^*$ has the smallest importance score. To determine the compression ratio for each block we propose a linear compression schedule where the least important block is compressed most, and blocks of higher importance less. Concretely, three parameters need to be specified by the user to compute the individual compression ratios $c_i$, namely the total number of paramters to be removed $N$, the number of blocks that are compressed during the current iteration $k$ and the compression ratio of the least important block $c_1 = S$ to which we refere to as base compression ratio. To determine the compression ratios $c_i$ of the remaining blocks $B_i^* \in \{B^*_1, \dots, B^*_k\}$,   

%
%Therefore, we rely on the \gls{SVD} compression scheme as described in sec. \ref{sec:IterativeRepeatedSVDCompression}.  Let again $\{B^*_1, \dots, B^*_L\}$ denote the sorted blocks, one specify by how much the least important block is compressed denoted as $S$, e.g. $40\%$, the number of blocks that should be compressed $k$ and the number of total parameters $N$ that should be removed. The compression rate is reduced linearly for each additional block, meaning the least important block $B_1^*$ is compressed most, the second least import block $B_2^*$ is compressed a little bit less  and block $B_k^*$ the most important block of all blocks that are compressed is compressed least. Concretely, the linear compression schedule works as follows: 

 the base compression ratio $S$ is decrease by a slope $x$ between every block. Therefore, the compression ratio for every block $B_i^*$ is given by
\begin{equation}
	\label{eq:CompressionRatio}
	c_i = S - ((i-1) \cdot x)
\end{equation}
with $i \in \{1, \dots, k\}$. The goal is to identify $x$ based on the parameters $S, N, k$. The total number of parameter that are removed is computed by
\begin{equation}
	\label{eq:TotalParam}
	P_\text{total} = \sum_{i=1}^{k} P^*_i \cdot c_i
\end{equation}
where $P^*_i$ is the current parameter count of block $B_i^*$. To solve for $x$, eq. \ref{eq:CompressionRatio} and eq. \ref{eq:TotalParam} are combined

\begin{equation}
	\begin{split}
		& \sum_{i=1}^{k} P^*_i (S - (i-1) \cdot x) = N \\
		\Leftrightarrow \quad & \sum_{i=1}^{k} (P^*_i \cdot S) - \sum_{i=1}^{k} (P^*_i \cdot (i-1) \cdot x) = N \\
		\Leftrightarrow \quad & \sum_{i=1}^{k} (P^*_i \cdot S) - N = x \cdot \sum_{i=1}^{k} (P^*_i \cdot (i-1))
	\end{split}
	\label{eq:linear_compression_derivation}
\end{equation}


ending in the final equation for the slope
\begin{equation}
	x = \frac{\sum_{i=1}^{k}\left(P^*_i \cdot S \right) - N}{ \sum_{i=1}^{k}\left(P^*_i \cdot (i-1)  \right)} \quad.
\end{equation}
The input parameters $S,N,k$ specified by the user have to meet two conditions. First, $N \leq S \cdot \sum_{i=1}^{k} P^*_i$ meaning the total number of parameters that should be removed during this iteration must not be larger than the number of parameters one obtain when using $S$ as compression ratio for all blocks $B_i^*$. Otherwise, the compression ratio would increase for more important blocks which would contradict the ranking strategy. Second, the compression ratio of the last block $c_k$ must not be negative which results in the following condition
\begin{equation}
	\begin{split}
		& 0 \leq c_k = S - (k-1) \cdot x \\
		\Leftrightarrow \quad & 0 \leq S - (k-1) \frac{S \sum_{i=1}^{k} P^*_i - N}{\sum_{i=1}^{k}(P^*_i \cdot (i-1))} \\
		\Leftrightarrow \quad & N \geq S \sum_{i=1}^{k} P^*_i - \frac{S \sum_{i=1}^{k}(P^*_i \cdot (i-1)) }{k-1} \quad.
	\end{split}
\end{equation}

In total we get the following condition for the total number of paramters to remove in one iteration based on the choice of $S$ and $k$
\begin{equation}
	\ S \sum_{i=1}^{k} P^*_i - \frac{S \sum_{i=1}^{k}(P^*_i \cdot (i-1)) }{k-1}  \leq N \leq S \cdot \sum_{i=1}^{k} P^*_i \quad.
\end{equation}
After obtaining the the compression ratios $c_i$ the model can be compressed based on the \gls{SVD} compression scheme as described in sec. \ref{sec:IterativeRepeatedSVDCompression} and then be retrained. Afterwards, the procedure is repeated until the desired model size is achieved.