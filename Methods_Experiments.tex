%\section{PixArt-$\Sigma$ Distillation}
%This master thesis investigate structural model distillation techniques, specifically focusing on the removal and compression of transformer blocks within the PixArt-$\Sigma$ and Flux-dev architectures. PixArt-$\Sigma$ is employed as an experimental baseline to evaluate various design choices in the distillation frameworks due to its relatively low parameter count enabling rapid iteration and extensive ablation studies. Subsequently, the most effective strategies are applied to Flux-dev to assess their efficacy in high-parameter regimes. 


\section{Block Importance Analysis}
\subsection{Block Selction Criterias}
\subsubsection{Magnitude-Pruning}
In magnitude pruning \cite{han2015learning,filters2016pruning,lee2020layer}, the magnitudes of the weights of the individual components of a model are computed to serve as a proxy for their importance. The underlying assumption is that small weight magnitudes correspond to a low overall impact on the model performance, implying that model parts with lower weight magnitude should be removed first. The aggregated magnitude $M_i$ of a transformer block $B_i$ is defined as the total Frobenius norm of its constituent weight matrices $W \in B_i$. To account for the multiple sub-layers within a block, we compute the block-wise magnitude as
 \begin{equation} 
 	M_i = \sqrt{\sum_{W \in B_i} |W|_F^2} \quad \text{with} \quad |W|_F = \sqrt{\sum_j \sum_k |w_{jk}|^2} \quad.
 \end{equation} The block with the smallest magnitude $M_s = \min_i(M_i)$ is then identified as the primary candidate for removal or compression.

\subsubsection{Representational Similarity Analysis}
Another approach to identify redundant model components is to compare their input representations with their corresponding outputs representation \cite{men2024shortgpt}. If the transformation between the input and output is marginal, indicating that the specific component performs an approximate identity mapping, the model part is considered a candidate for removal. To quantify the similarity between the input and output features of a transformer block, the central kernel alignment (\gls{CKA}) metric \cite{kornblith2019similarity, pons2024effective} is employed. Let $X \in \mathbb{R}^{n\times d}$ denote the input and $Y \in \mathbb{R}^{n\times d}$ the output features which should be compared. First, the kernels $K$ and $L$ need to be chosen which are the inner product of the input $K=XX^T$ and the output $L=YY^T$ in the linear case. To ensure invariance to mean shifts, the centering matrix $H=I - \frac{1}{n} 11^T$ is applied. The final \gls{CKA} score is given by 
\begin{equation}
	\text{CKA}(K,L) = \frac{\text{HSIC}(K,L)}{\sqrt{\text{HSIC}(K,K) \text{HSIC}(L,L)}} \quad.
\end{equation}
This calculation utilizes the Hilbert-Schmidt Independence Criterion (\gls{HSIC}) \cite{gretton2005measuring} $\text{HSIC}(K,L) = \frac{1}{(n-1)^2}\text{tr}(KHLH)$ \cite{davari2022reliability}.



\subsubsection{Black-Box Optimization with Optuna}
Optuna \cite{akiba2019optuna} is an open-source hyperparameter optimization framework. It primarily leverages a tree-strucured parzen estimator \cite{bergstra2011algorithms}, a bayesian optimization algorithms, to search the space of all possible hyperparmeter settings. It partitions the observed hyperparameter configurations into promising and non-promising candidates based on a user defined objective function. By modeling these two groups using probability density functions it draws a new set of hyperparameters that belongs to the promising group  with high probability. Through this informed search the global optimum can be found much faster than with non-adaptive methods like random or grid search. For an in-depth descritption of the Optuna algorithm the reader is refered to the original paper \cite{akiba2019optuna} because this would go beyond the scope of this work. \\


\subsection{TinyFusion}
The authors of TinyFusion \cite{fang2025tinyfusion} propose a learning based block selection method for pruning transformer based image generation models. They hypothesize that instead of focusing on the error/loss in image quality resulting from the model compression it is advantageous to focus on the recoverability capabilities of the compressed model meaning on the post-finetuning performance. Initially, the model is divided into $K$ subparts $\mathbf{\phi} = [\mathbf{\phi}_1,\dots, \mathbf{\phi}_K]$.  Each subpart consists of $M$ transformer blocks $\mathbf{\phi}_k = [\phi_{k1}, \dots, \phi_{kM}]$. Specifying the number $N$ of blocks retained in each subpart a local binary mask $\mathbf{m}_k \in \{0,1\}^M$ is introduced to determine the inclusion or exclusion of the individual blocks. A categorical distribution $p(\mathbf{m}_k)$ is assigned to each mask to model the probability for a specific combination of blocks to be removed. The goal is to jointly optimize the model and the categorical mask distributions.  To ensure that the sampling process of the mask remains differentiable during training the Gumbel-Softmax \cite{jang2016categorical} is utilized. The objective function for jointly optimizing weights and masks is given by
\begin{equation}
	\min_{\{p(\mathbf{m}_k)\}} \underbrace{\min_{\Delta \Phi} \mathbb{E}_{x, \{\mathbf{m}_k \sim p(\mathbf{m}_k)\}} \left[ \mathcal{L}(x, \Phi + \Delta \Phi, \{\mathbf{m}_k\}) \right]}_{\text{\textit{Recoverability: Post-Fine-Tuning Performance}}} 
\end{equation}
where $\Delta \Phi$ represents the updates for the model parameters $\Phi$.
Due to the potential high computational effort of a complete finetuning of a large model, the authors showed that low-rank adaptation (\gls{LoRA}) \cite{hu2022lora} finetuning is sufficient. The joint optimization results in a mask distribution taking into account how removing blocks would influence the post-finetuned model.
\section{Training Data and Image Synthesis}
\subsection{Training Data}
\subsubsection{LAION}
\subsection{Evaluation Data}


\section{Structural Compression Strategies}
\subsection{SVD Compression}



\section{Knowledge Distillation Loss Functions}
\subsection{Objective Functions for Performance Recovery}
\subsection{Feature Alignment Strategies}
