\section{Fundamentals of Neural Network Architectures}
In this section, the focus lies on the key fundamental building blocks of the neural networks. 

\subsection{Transformer}
The introduction of the transformer architecture \cite{AttentionIsAllYouNeed} was a decisive moment for the whole \gls{AI} community. It significantly pushed the development in natural language and image processing forward. Therefore, it is no surprise that the latest models are based on this architecture type \cite{Claude, Gemini, Flux, HiDream}. \\
The transformer was introduced as an alternative to recurrent neural networks  \cite{RNN} for processing sequential data like text. In the original paper, the transformer architecture leverages an encoder-decoder structure, however, there are many variants like decoder-only \cite{GPT4, Lama} or encoder-only transformer \cite{BERT}. \\
Transformers, as introduced in the original paper (Fig. \ref{transformer_architecture}), take text as input, which must first be converted into numerical form through a process called tokenization. This produces a sequence of tokens that are a numerical representation of words or subwords of the text. To provide the model with information about the position of each token in the sequence, positional encodings are added. These preprocessing steps occur before the input is fed into the transformer architecture. 
Each encoder block consists of two sub-layers. First, a multi-head self-attention mechanism is followed by a residual connection \cite{ResidualConnection} and layer normalization \cite{LayerNorm}. Second, a feedforward network processes the output further. The decoder blocks contain three sub-layers. First, a masked multi-head self-attention prevents the model from attending to future tokens during the training process, second, a cross-attention mechanism introduces the output of the encoder in the decoder and third an additional feedforward network processes the output of the cross-attention layer. All three sub-layers are followed by a residual connection and layer normalization. The final output is projected to vocabulary-sized logits representing a probability distribution over possible next tokens.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{images/Background/transformer.png}  % adjust filename and width
    \caption[Transformer Architecture]{Visualization of the transformer architecture. Taken from \cite{AttentionIsAllYouNeed}.}
    \label{transformer_architecture}
\end{figure}

\subsection{Attention Mechanism}
\label{AttentionMechanism}
The attention mechanism is an integral part of the transformer architecture. It enables the model to focus on the input elements that are most important for the task at hand and ignore the less important ones. This mechanism was first introduced for \gls{RNN}s \cite{OrgAttention} to overcome the information bottleneck, as traditionally in RNNs only the last hidden state is given as input for the next prediction. In \cite{OrgAttention}, all previous hidden states were presented to the model as additional input, and the task of the attention mechanism was to filter out those that were relevant for the prediction of the next state. The authors of \cite{AttentionIsAllYouNeed} leveraged the formalism and reformulated it for the transformer architecture that will be discussed in the following. \\
In the attention mechanism, so-called queries, keys and values are computed. The attention mechanism can be understood as a soft lookup table where queries search for relevant keys, and the corresponding values are retrieved and weighted by similarity.
 A distinction is made between self-attention (fig. \ref{SelfAttentionFigure}) and cross-attention (fig. \ref{CrossAttentionFigure}). In self-attention, the interaction of the tokens within a sequence is calculated. In contrast, in cross-attention you have two input sequences with the goal that the model aligns and relates information between the two different sequences.

\subsubsection{Self-Attention}
Let $X \in \mathbb{R}^{n\times d}$ be the embedding matrix of the tokens. Then the queries $Q$, keys $K$ and values $V$ are computed by linear projection of  the embedding matrix
\begin{align}
    &Q = X \cdot W_q^T  \\
    &K = X \cdot W_k^T \\
    &V = X \cdot W_v^T
\end{align}
with $W_q^T \in \mathbb{R}^{d\times d_k}$, $W_k^T \in \mathbb{R}^{d\times d_k}$ and $W_v^T \in \mathbb{R}^{d\times d_v}$. Now, so-called scaled-dot product attention is used where the attention matrix $A$ that assigns how much focus is put on the different parts of the token sequence is computed by the softmax of the scaled dot product between the queries and the keys 
\begin{equation}
    A = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)
\end{equation}
The softmax function is defined as the following for a vector $\textbf{z} = \left( z_1, z_2, ..., z_N \right)$
\begin{equation}
    \text{softmax}\left(z_i\right) = \frac{e^{z_i}}{\sum_j e^{z_j}}
\end{equation}
The scaling of the dot product $QK^T$ with the factor $\frac{1}{\sqrt{d_k}}$ is done in order to prevent vanishing gradients in the softmax function which occurs if the dot product results in very large values.
Finally, the attention matrix is applied with the values. This can be summarized in the following formulation
\begin{equation}
    Z=\text{Attention}\left( Q, K, V \right)= \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
    \label{AttentionEq}
\end{equation}


One main challenge of the discussed attention mechanism is that it has quadratic complexity $\mathcal{O}(n^2)$ in computation and memory consumption which is tackled by several approaches \cite{Performer, Reformer, SparseTransformer, FlashAttention}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/Background/SelfAttention.png}  % adjust filename and width
    \caption[Self-Attention Mechanism]{Visualization of the self-attention mechanism. Based on \cite{SelfAttentionFigure}.}
    \label{SelfAttentionFigure}
\end{figure}
 
\subsubsection{Cross-Attention}
The goal of cross-attention is to relate two different sequences of tokens \textbf{$\mathbf{x}_1$} and \textbf{$\mathbf{x}_2$}. Commonly, the queries come from the target sequence ,e.g., the decoder of the transformer, while the keys and values come from the source sequence, e.g., encoder output of transformer. Apart from this difference, the computation is identical to self-attention.


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/Background/CrossAttention.png}  % adjust filename and width
    \caption[Cross-Attention Mechanism]{Visualization of the cross-attention mechanism. Based on \cite{SelfAttentionFigure}.}
    \label{CrossAttentionFigure}
\end{figure}

\subsubsection{Multi-Head Attention}
Multi-head attention is another trick used in transformers to make the results of the attention operations even more expressive. Instead of only performing individual self- or cross-attentions in a transformer block, so-called multi-head attentions are used. Here, based on the input embeddings, not just one but N times keys, queries and values are calculated, each with different weight matrices $W_{q_i}^T \in \mathbb{R}^{d\times d_k}$, $W_{k_i}^T \in \mathbb{R}^{d\times d_k}$ and $W_{v_i}^T \in \mathbb{R}^{d\times d_v}$ with $i = 1,...,N$, and the attention mechanism is executed. Each attention that is carried out is designated as a $\text{head}_i$. The results of the N attentions are concatenated and combined with another linear projection $W^O \in \mathbb{R}^{Nd_v\times d}$.
\begin{align}
    &\text{MultiHead}\left( Q,K,V  \right) = \text{Concat}\left( \text{head}_1, ..., \text{head}_N   \right)W^O \\
    &\text{head}_i = \text{Attention}\left(XW_{q_i}^T,XW_{k_i}^T,XW_{v_i}^T \right)
\end{align}

This is advantageous because the individual heads can concentrate on different tasks, thus reducing the complexity for each head compared to the case where only a single attention has to capture everything.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{images/Background/multi-head-attention.png}  % adjust filename and width
    \caption[Multi-Head Attention]{Visualization of the multi-head attention mechanism. Taken from \cite{AttentionIsAllYouNeed}.}
    \label{MultiHeadAttention}
\end{figure}

\subsection{Positional Encoding}
\label{PositionalEmbedding}
Before the embedded token sequence is fed as input to the transformer, a positional information is added to each token to provide the network with information about the order of the tokens. There are different methods, which can be divided into the categories relative \cite{RoPE, ALiBi, FIRE} versus absolute \cite{AttentionIsAllYouNeed, Floater, Cape, Shape} positional embedding or deterministic \cite{AttentionIsAllYouNeed, Shape, Cape, ALiBi, RoPE} versus learnable \cite{Floater, FIRE} positional embedding. 

\subsubsection{Sinusoidal Positional Encoding}
Sinusoidal positional encoding is used in the original transformer paper \cite{AttentionIsAllYouNeed} which leverages the sinus and cosine function  to inject positional information into the model. It belongs to the category of absolute and deterministic positional encoding.
\begin{align}
    &PE\left( pos, 2i  \right) = \sin \left(  \frac{pos}{10000^{\frac{2i}{d}}} \right) \\
    &PE \left( pos, 2i+1  \right) = \cos \left(  \frac{pos}{10000^{\frac{2i}{d}}} \right)
\end{align}
where $d$ is the embedding dimension, $pos$ is the position of the token in the sequence and  $i$ is the dimension index which is used to compute different frequencies for different parts of the embedding vector. The computed positional embedding has the same dimension like the token embedding vector and is added to it.

%\subsubsection{Rotary Position Embedding}
%In recent years, the focus shifted more and more from absolute positional embedding to relative positional embedding because one hypothesized that relative order of tokens matter and that these methods are better in extrapolating compared to absolute positional embedding methods \cite{PositionalEmbeddingSurvey}. Rotary Positiona Embeeding (\gls{RoPE}) \cite{RoPE} is a a relative positional embedding method used in FLUX-dev \cite{Flux}. The idea is to encode the position via rotation in the attention mechanism, to be more explicit in the dot product between the queries $Q$ and the keys $K$. Starting with the 2d case, they propose that queries $Q_m$ which are soley based on the mth embedding vector $x_m$ and its position $m$ via 
%\begin{align}
%    Q_m \left( x_m, m \right) &= 
%    \begin{bmatrix}
%\cos \left( m\theta  \right) & -\sin \left( m\theta  \right)\\
%\sin \left( m\theta  \right) & \cos \left( m\theta  \right)  \\
%\end{bmatrix}
%\begin{bmatrix}
% W^{11}_q   & W^{12}_q\\
%W^{21}_q   & W^{22}_q  \\
%\end{bmatrix}
%\begin{bmatrix}
% x^{1}_m\\
%x^{2}_m   \\
%\end{bmatrix} \\
%&= R^d_{\theta,m} W_q \mathbf{x_m}
%\end{align}
%with a preset non-zero constant $\theta$.
%\textbf{Hier wurde die andere Konvention zum Schreiben des Attention Mechanismus verwendet als oben -> vereinheitlichen???}
%The keys $K_m$ are computed in exactly the same way. A visualization can be found in fig. \ref{RoPE_fig}. In the multi-dimensional case the rotation matrix becomes
%
%\begin{equation}
%    R^d_{\theta,m} =
%\begin{bmatrix}
%\cos(m\theta_1) & -\sin(m\theta_1)  & \cdots & 0 & 0 \\
%\sin(m\theta_1) &  \cos(m\theta_1)  & \cdots & 0 & 0 \\
%0 & 0  & \cdots & 0 & 0 \\
%0 & 0 &  \cdots & 0 & 0 \\
%\vdots & \vdots  & \ddots & \vdots & \vdots \\
%0 & 0 &  \cdots & \cos(m\theta_{d/2}) & -\sin(m\theta_{d/2}) \\
%0 & 0 &  \cdots & \sin(m\theta_{d/2}) &  \cos(m\theta_{d/2})
%\end{bmatrix}
%\end{equation}
%
%By computing the dot product between the queries and the keys one obtain a single rotation matrix just depending on the relative positions 
%\begin{equation}
%    Q_m^TK_n = \left( R^d_{\theta,m} W_q \mathbf{x_m}  \right)^T R^d_{\theta,n} W_k \mathbf{x_n} = \mathbf{x_m^T} W_q^T R^d_{\theta,n-m}W_k \mathbf{x_n}
%\end{equation}
%
%because $R^d_{\theta,n-m} = (R^d_{\theta,m})^T R^d_{\theta,n}$. In contrast to sinusoidal positional encoding being additive, rotary positional encoding is multiplicative. Moreover, it has the property of long-term decay that means the further away two embeddings are the smaller the dot product of the keys and queries will be.
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.8\textwidth]{images/Background/RoPE.png}  % adjust filename and width
%    \caption{Scheme of implementation of rotational position encoding from \cite{RoPE}.}
%    \label{RoPE_fig}
%\end{figure}

\subsection{Vision Transformer}
Original transformers were introduced for language processing tasks. There were some attempts to transfer the methodology of the transformer architecture to computer vision problems \cite{StandAloneAttentionVisonModels, Ccnet, ImageTransformer}, but for a long time, convolutional neural networks (\gls{CNN}s) \cite{CNN} remained the dominant architecture. \gls{CNN}s have an inherent advantage over transformers because they possess an image-related inductive bias. On the one hand, \gls{CNN}s are translationally equivariant by construction and, on the other hand, the concept of locality is embedded in the way they are built. In contrast, these two concepts must be learned by a transformer, as they are not predefined by its architecture, which makes a sufficiently large dataset essential to achieve competitive performance. \\


The introduction of vision transformer (\gls{ViT}) \cite{VisionTransformer} for a classification task showed that given a sufficiently large dataset transformers can match or even surpass \gls{CNN}s for vision tasks. The first key aspect in \gls{ViT} is that not every single pixel is related to all other pixels because this would be computationally infeasible due to the quadratic complexity of the attention mechanism. Instead, the image is split $\mathbf{x} \in \mathbb{}{R}^{H \times W \times C}$ into a sequence of smaller patches $\mathbf{x_p} \in \mathbb{}{R}^{N \times (P^2 \cdot C)}$ where $C$ is the number of channels, $H$ the height of the image, $W$ the width of the image, $(P,P)$ the resolution of a patch and $N=\frac{HW}{P^2}$ the number of patches. At the beginning of the token sequence an additional classification (\gls{CLS}) token is prepended. The patches are flattened and linearly projected to the encoder dimension. After that a learned positional encoding is added before they are fed into the encoder-only transformer architecture. On top of the transformer encoder a \gls{MLP} head processes the embedded version of the classification token and provides probability for the different classes from the classification problem. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Background/VisionTransformer.png}  % adjust filename and width
    \caption[Vision Transformer Architecture]{Visualization of vision transformer architecture. Taken from \cite{VisionTransformer}.}
    \label{VisionTransformer}
\end{figure}

\subsection{Adaptive Layer Normalization}
\label{AdaptiveLayerNormalization}
Layer normalization \cite{LayerNorm} was introduced as an alternative to batch normalization \cite{BatchNorm} as a normalization method to reduce the internal covariate shift in deep neural networks independently of the batch size. The covariate shift describes the phenomenon whereby the range of the input to a layer which is the ouptut of the previous layer changes permanently, which hinders effective learning because the layer needs to constantly adapt to a new input distribution. To stabilize and accelerate the training process normalizations are applied in the neural network. In layer normalization, the activations of each layer $\mathbf{z}_{l}$ are normalized to a zero mean and unit variance. In addition, a scale $\gamma$ and a shift $\eta$ parameter are learned to maintain the flexibility and expressiveness of the network and to ease the constrained range due to the normalization. The normalized layer output is given by 
\begin{equation}
	\hat{\mathbf{z}}_l = \gamma_l \frac{\mathbf{z}_l - \mu_l(\mathbf{z}_l)}{\sigma_l(\mathbf{z}_l)} + \eta_l \quad.
\end{equation}
A further development of layer normalization occured when \cite{adaLN} showed that the scale and shift parameters could be used to incorporate additional information into the model by parameterizing them as functions of an additional input $c$ such that $\gamma \to \gamma(c)$ and shift $\eta \to \eta(c)$. This method is known as adaptive layer normalization (\gls{adaLN}) and can be leveraged in diffusion models to incorporate the timestep or class conditioning \cite{DiT}.
