\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{AttentionIsAllYouNeed}
\citation{Claude,Gemini,Flux,HiDream}
\citation{RNN}
\citation{GPT4,Lama}
\citation{BERT}
\citation{ResidualConnection}
\citation{LayerNorm}
\citation{AttentionIsAllYouNeed}
\citation{AttentionIsAllYouNeed}
\citation{OrgAttention}
\citation{OrgAttention}
\citation{AttentionIsAllYouNeed}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Fundamentals of Neural Network Architectures}{17}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Transformer}{17}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Transformer architecture from \cite  {AttentionIsAllYouNeed}.}}{17}{figure.2.6}\protected@file@percent }
\newlabel{transformer_architecture}{{2.6}{17}{Transformer architecture from \cite {AttentionIsAllYouNeed}}{figure.2.6}{}}
\citation{Performer,Reformer,SparseTransformer,FlashAttention}
\citation{SelfAttentionFigure}
\citation{SelfAttentionFigure}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Attention Mechanism}{18}{subsection.2.3.2}\protected@file@percent }
\newlabel{AttentionMechanism}{{2.3.2}{18}{Attention Mechanism}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Self-Attention}{18}{section*.14}\protected@file@percent }
\newlabel{AttentionEq}{{2.82}{18}{Self-Attention}{equation.2.82}{}}
\citation{SelfAttentionFigure}
\citation{SelfAttentionFigure}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Self-attention mechanism based on \cite  {SelfAttentionFigure}.}}{19}{figure.2.7}\protected@file@percent }
\newlabel{SelfAttentionFigure}{{2.7}{19}{Self-attention mechanism based on \cite {SelfAttentionFigure}}{figure.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Cross-Attention}{19}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Cross-attention mechanism based on \cite  {SelfAttentionFigure}.}}{19}{figure.2.8}\protected@file@percent }
\newlabel{CrossAttentionFigure}{{2.8}{19}{Cross-attention mechanism based on \cite {SelfAttentionFigure}}{figure.2.8}{}}
\citation{AttentionIsAllYouNeed}
\citation{AttentionIsAllYouNeed}
\citation{RoPE,ALiBi,FIRE}
\citation{AttentionIsAllYouNeed,Floater,Cape,Shape}
\citation{AttentionIsAllYouNeed,Shape,Cape,ALiBi,RoPE}
\citation{Floater,FIRE}
\citation{AttentionIsAllYouNeed}
\@writefile{toc}{\contentsline {subsubsection}{Multi-Head Attention}{20}{section*.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Multi-head attention from \cite  {AttentionIsAllYouNeed}.}}{20}{figure.2.9}\protected@file@percent }
\newlabel{MultiHeadAttention}{{2.9}{20}{Multi-head attention from \cite {AttentionIsAllYouNeed}}{figure.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Positional Encoding}{20}{subsection.2.3.3}\protected@file@percent }
\newlabel{PositionalEmbedding}{{2.3.3}{20}{Positional Encoding}{subsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sinusoidal Positional Encoding}{20}{section*.17}\protected@file@percent }
\citation{StandAloneAttentionVisonModels,Ccnet,ImageTransformer}
\citation{CNN}
\citation{VisionTransformer}
\citation{VisionTransformer}
\citation{VisionTransformer}
\citation{LayerNorm}
\citation{BatchNorm}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Vision Transformer}{21}{subsection.2.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Vision transformer from \cite  {VisionTransformer}.}}{21}{figure.2.10}\protected@file@percent }
\newlabel{VisionTransformer}{{2.10}{21}{Vision transformer from \cite {VisionTransformer}}{figure.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Adaptive Layer Normalization}{21}{subsection.2.3.5}\protected@file@percent }
\newlabel{AdaptiveLayerNormalization}{{2.3.5}{21}{Adaptive Layer Normalization}{subsection.2.3.5}{}}
\citation{adaLN}
\citation{DiT}
\@setckpt{theory_fundamental_NN}{
\setcounter{page}{23}
\setcounter{equation}{87}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{NAT@ctr}{0}
\setcounter{section@level}{2}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{24}
\setcounter{algorithm}{3}
\setcounter{ALG@line}{8}
\setcounter{ALG@rem}{8}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
}
