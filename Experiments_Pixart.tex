%!TEX root = ./main.tex

\chapter{Experiments}
This master thesis investigate structural model pruning techniques, specifically focusing on the removal and compression of transformer blocks within the PixArt-$\Sigma$ and Flux-dev architectures. PixArt-$\Sigma$ is employed as an experimental baseline to evaluate various design choices in the structural pruning framework due to its relatively low parameter count enabling rapid iteration and extensive ablation studies. Subsequently, the most effective strategies are applied to Flux-dev to assess their efficacy in high-parameter regimes. 
\section{PixArt-$\Sigma$ Pruning}

\subsection{Experimental Default Setup}
\label{sec:DefaultSetup}
In the following section, individual components of the compression pipeline such as loss function, block selection algorithm, compression strategy, ..., are systematically investigated. To isolate the impact of each component, a default experimental setup is defined below. Unless stated otherwise all subsequent experiments utilize the baseline configuration. It is important to note that this setup serves as consistent starting point for the experiments and does not necessarily represent the final, best setting which is derived later in this work.

\begin{itemize}
	\item \textbf{Loss}: The learning objectiv is the sum of the unormalized feature and output distillation loss $\mathcal{L_\text{OutKD}} + \mathcal{L_\text{FeatKD}}$.
	\item \textbf{Pruning Schedul}: The progressive compression strategy is employed. As soon as the \gls{CMMD} value becomes larger than 0.1 in the block selection algorithm, the model was retrained to recover its generation capbilities.
		\item \textbf{Structural Compression Strategy}: Complete block removal is used to reduce the model size.
	\item \textbf{Block Selection Algorithm}: The Greedy algorithm based on \gls{CMMD} and \gls{CLIP}-score was utilized computed on 100 images.
	\item \textbf{Training Datasets}: Training is performed sequentially starting on the LAION dataset followed by further training on the LAION-PixArt dataset. For each iteration, the models are trained for two epochs on each datasets.
	\item \textbf{Training Hyperparameters}: All other training hyperparameters are summarized in tab. \ref{tab:hyperparameterTraining}.
	\item  \textbf{Finetuning Protocol}: The recovery of the pruned model follows a structured three stage protocol. First, for local stabilization, optimization is restricted to  the block $B_{l-1}$ immediately preceding the removed block $B_l$ for one epoch (38k steps) on the LAION dataset. In case where several blocks are removed simultanousely, all corresponding preceding blocks are updated while the rest of the network is frozen. Subsequently, the complete model is optimized for an additional epoch on the LAION dataset. Finally for domain-specific refinement, the complete model is finetuned for two epochs (12.5k steps) on the LAION-PixArt dataset.  

\end{itemize}

The implementation of all experiments build up on the official git repository for PixArt-$\Sigma$ \cite{pixartsigma_code}.


\begin{table}[H]
	\centering
	\caption{Default hyperparameters for the retraining of compressed models.}
	\label{tab:hyperparameterTraining}
	\begin{tabular}{l c}
		\toprule
		\textbf{Hyperparameter} & \textbf{Value} \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Optimization}}} \\
		\midrule
		Optimizer Type & CAME \\
		Learning Rate & $2 \times 10^{-5}$ (Block Removal)/ $2 \times 10^{-6}$ (SVD Compression)\\
		Weight Decay & $0.03$ \\
		Betas & $(0.9, 0.999, 0.9999)$ \\
		Epsilon & $(10^{-30}, 10^{-16})$ \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Training Schedule}}} \\
		\midrule
		Warmup Steps & 1000 \\
		Batch Size & 16 \\
		Gradient Clipping & $0.001$ \\
		\midrule
		\multicolumn{2}{l}{\textit{\textbf{Model \& Data}}} \\
		\midrule
		Precision & bf16 \\
		Resolution & $512 \times 512$ \\
		Class Dropout Prob. & $0.1$ \\
		Max Prompt Length & 300 \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Block Importance Analysis}
In structural pruning, the selection of components to be removed or compressed is essential to ensure that the compressed model retains the highest possible image generation quality. Qualitatively, fig. \ref{fig:PixArtBlockImportanceQualitatively} shows the effect of removing every transformer block from PixArt-$\Sigma$ individually on its image generation capabilities without subsequent retraining. It is clearly visible that removing the first and last transformer blocks (blocks 1-4 and blocks 27-28) exerts the most significant influence on the image quality. This confirms the observation from previous work \cite{men2024shortgpt} that redundancies are primarily find in the middle part of transformer based models. To identify which blocks are best to remove meaning which blocks have a minimal contribution on the total image generation process, several different metrics can be employed. In the following section, different approaches are investigated to determine the importance of the individual transformer blocks based on completely removed blocks. First, the traditional magnitude-based (see sec. \ref{sec:MagnitudePruning}) pruning and represential dissimilarity by computing the central kernal alignment score (\gls{CKA}) (see sec. \ref{sec:CKA}) between the input and output of each transformer block are evaluated. Moreover, they are compared to standard performance metrics such as the \gls{CLIP}-score and \gls{CMMD} (see sec. \ref{sec:MetricBasedSelection}). \\
 Normally, several transformer blocks are removed or compressed simultaneously. Due to block interdependencies, we investigate three different options to identify the best grouping for blocks to remove. First, a greedy-algorithm is applied (see sec. \ref{sec:GreedyBlockSelection}), iteratively searching for the best next block to compress based on the already compressed model from previous iterations. Second, the block selection process is formulated as hyperparameter search leveraging the Optuna framework (see sec. \ref{sec:Optuna}). Last, a procedure of learning a mask indicating the blocks with least impact on the model performance is applied (see sec. \ref{sec:TinyFusion}).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{images/Experimente/PixArt_Block_Analysis/BlockImportance.pdf}  % adjust filename and width
	\caption[Qualitative analyse of the block sensitivity of PixArt-$\Sigma$ for block removal]{The figure shows the influence of selectively removing individual transformer blocks (1–28) on the final generation result. While the removal of middle blocks causes only minor visual deviations, interventions in early, structure-forming layers (blocks 1–4) and in the final layers (blocks 27–28) lead to a significant loss of image coherence and massive compression artifacts, respectively.}
	
	\label{fig:PixArtBlockImportanceQualitatively}
\end{figure}

\subsubsection{Qualitative Block Analysis}
Fig. \ref{fig:PixArtBlockImportanceQualitatively} displays an example for the qualitative influence of the removal of individual transformer blocks from the PixArt-$\Sigma$ architecture. For each image a different block was completely removed. The removal of the first four blocks and the last two blocks exhibit a strong degeneration in image quality which confirms the finding that the first and last layers are more critical for transformer-based models from \cite{men2024shortgpt}. In diffusion or flow models the first layers aggregate global context while the middle layers do a refinement for details. The last layers are acting as important for high frequency details as seen in fig. \ref{fig:PixArtBlockImportanceQualitatively} where for removed block 27 or 28 the concept of a man is present in the image but verry blurry.

\subsubsection{Compression Criteria}
In magnitude based pruning strategies the model parts with the lowest averaged weight magnitude are considered to be less important for the overall model performance. Interestingly, in PixArt-$\Sigma$ the weight magnitudes of the initial layers, which have a substantial influence on the final image synthesis (see fig. \ref{fig:PixArtBlockImportanceQualitatively}), have the lowest weight magnitudes across the architecture (see fig. \ref{fig:PixArtBlockMagnitudeCLIP} and \ref{fig:PixArtBlockMagnitudeCMMD} which both show the weight magnitudes for every block compared to the \gls{CLIP}-score or \gls{CMMD}). This observation contradicts with the assumption of low impact by low weights magnitudes. It seems that despite the small weight magnitude the first layers act as important initial feature translator having a massive influence on model performance. Furthermore, the weight magnitudes of the final layers are only marginally higher than those of the middle layers, while the middle layers' weight magnitudes exhibit a high degree of similarity. This would make a precise selection of blocks for removal based solely on this heuristic difficult. Consequently, using the weight magnitude to identify redundant blocks within PixArt-$\Sigma$ is not a suitable criterion for strucural pruning in this context.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/PixArt_Block_Analysis/magnitude_vs_clip.png}
	\caption[Magnitude-base pruning vs \gls{CLIP}-score]{The histogram shows the magnitudes of the weights of the individual transformer blocks (1-28). The blocks which have shown qualitatively (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) to be very important for the image generation capabilities of PixArt-$\Sigma$ are marked with red. In addition, the \gls{CLIP}-scores for each block is displayed. The \gls{CLIP}-scores follow the qualitative observations in contrast to the magnitude-based analysis. }
	\label{fig:magnitude_clip}
	
	\label{fig:PixArtBlockMagnitudeCLIP}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/PixArt_Block_Analysis/magnitude_vs_cmmd.png}
	\caption[Magnitude-base pruning vs \gls{CMMD}]{The histogram shows the magnitudes of the weights of the individual transformer blocks (1-28). The blocks which have shown qualitatively (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) to be very important for the image generation capabilities of PixArt-$\Sigma$ are marked with red. In addition, the \gls{CMMD} for each block is displayed. The \gls{CMMD} follow the qualitative observations in contrast to the magnitude-based analysis. }
	
	\label{fig:PixArtBlockMagnitudeCMMD}
\end{figure}

The \gls{CKA} score is computed for every block and every sampling step across 100 example prompts taken from the LAION dataset. Subsequently, these scores are averaged across all samples and time steps resulting in a mean similarity score $\mu_{CKA,i}$ for each individual PixArt-$\Sigma$ block. Averaging over the time steps should provide an estimate of the general importance of the block. However, the importance of a block at different time steps can vary drastically (see Appendix A). A low value indicates that the input and output of a specific block differs significantly, suggesting higher importance of the block. \\
Both fig. \ref{fig:PixArtBlockImportanceCLIP} and \ref{fig:PixArtBlockImportanceCMMD} show the transformation intensity $1-\mu_{CKA,i}$ for every block compared with \gls{CMMD} or \gls{CLIP} score, a greate value signifies a more substantial feature transformaton. While blocks 17 and 22 stand out with their high values, the implied importance of these blocks cannot be confirmed by qualitative analysis (see fig. \ref{fig:PixArtBlockImportanceQualitatively}). Furthermore, blocks 1, 24, 27, and 28 are not uniquely identified as critical blocks for the performance of the model, as the values for blocks 19, 20, and 21 are higher, which would mark them as more relevant. As this ranking also fails to align with the qualitative review of block influence on the final image, the use of representational similarity for block selection in PixArt-$\Sigma$ must be viewed critically too.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/PixArt_Block_Analysis/cka_vs_clip.png}  % adjust filename and width
	\caption[\gls{CKA} transformation intensity vs \gls{CLIP}-score]{The histogram shows the transformation intensity based on \gls{CKA} comparing the input and output features of the individual transformer blocks (1-28). The blocks which have shown qualitatively (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) to be very important for the image generation capabilities of PixArt-$\Sigma$ are marked with red. In addition, the \gls{CLIP}-scores for each block is displayed. The \gls{CLIP}-scores follow the qualitative observations in contrast to the transformation intensity analysis. }

	\label{fig:PixArtBlockImportanceCLIP}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/PixArt_Block_Analysis/cka_vs_cmmd.png}  adjust filename and width	\caption[\gls{CKA} transformation intensity vs \gls{CMMD}]{The histogram shows the transformation intensity based on \gls{CKA} comparing the input and output features of the individual transformer blocks (1-28). The blocks which have shown qualitatively (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) to be very important for the image generation capabilities of PixArt-$\Sigma$ are marked with red. In addition, the \gls{CMMD} for each block is displayed. The \gls{CMMD} follow the qualitative observations in contrast to the transformation intensity analysis. }

	\label{fig:PixArtBlockImportanceCMMD}
\end{figure}

Due to the contradictory results of magnitude-based pruning and \gls{CKA}-scores when compared with the qualitative observations, a third methodology was investigated, namely quantifying the importance of individual blocks based on standard performance metrics \cite{FastFlux}. Specifically, the \gls{CLIP}-score and \gls{CMMD} are computed on a small reference dataset consisting of 100 images from the LAION dataset. It should be noted that the standard protocols for computing statistically robust \gls{CMMD} and \gls{CLIP} scores typically require larger datasets, e.g. upwards of 10,000. However, given the iterative nature of the greedy search (see sec. \ref{sec:GreedyBlockSelection}) and the high computational overhead associated with repeated evaluations, using such a large reference set was prohibitively expensive. Nevertheless, a sample size of 100 images serves as a sufficient proxy to capture the relative importance ranking of the transformer blocks, providing the necessary directional guidance for the distillation process. As illustrated in the fig. \ref{fig:PixArtBlockImportanceCLIP} and \ref{fig:PixArtBlockImportanceCMMD} (and likewise in fig. \ref{fig:PixArtBlockMagnitudeCLIP} and \ref{fig:PixArtBlockMagnitudeCMMD}) both metrics closely align with the qualitative evaluation, confirming that the first four and the last two blocks exert the greatest impact on the final image quality. Consequently, these metrics are selected as primary criteria for identifying redundant blocks in PixArt-$\Sigma$.
This ranking serves as foundation for the subsequent, more more sophisticated block selection algorithms and also for most experiments regarding block compression instead of block removal.\\

\subsubsection{Block Analysis for Compression Instead of Removal}
In the experiments in which the blocks were compressed via \gls{SVD}, the scores for block selection were computed based on compressed blocks. In fig. \ref{fig:PixArtBlockImportanceCompressionQualitatively} the results of compressing every transformer block individually by $60\%$ are displayed. One important point to note is that the compressing the first blocks (1-4) which have a large impact when the blocks are completely removed (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) resulting only in minimal image changes compared to the original image generated with the uncompressed model. Also the compression of block 28 results in less severe image quality degeneration than completely removing it. This demonstrates that these blocks are critical for the overall performans but contain high degree of redundancy. Notably, the compression block 27 also exhibit a strong smoothing of details like it was already observed by removing it. This indicates that block 27 is crucial and does not contain a high degree of redundancies. Compressing the middle blocks does not lead to a big drop of image quality but they differ even more from the original image compared to the first blocks. A possible explanation is that for setting the global structure which is done by the first blocks a low rank structure is sufficient. The middle blocks which are responsible for the details, e.g. direction in which the eagle looks, are more sensitive for compression.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{images/Experimente/PixArt_Block_Analysis/BlockAnalysisCompression06.pdf}  % adjust filename and width
	\caption[Qualitative analyse of the block sensitivity of PixArt-$\Sigma$ of block compression]{The figure shows the influence of selectively compressing individual transformer blocks (1–28) by $60 \%$ on the final generation result.}
	
	\label{fig:PixArtBlockImportanceCompressionQualitatively}
\end{figure}

\subsubsection{Block Selection Algorithms}
A further key challenge after establishing the pruning criteria as the combination of \gls{CLIP}-score and \gls{CMMD} in structural model pruning is identifying the optimal combinations of blocks to remove simultaneously. When removing only a single block, there are merely 28 possibilities for PixArt-$\Sigma$. However, if the goal is to reduce the model size by $50\%$ necessitating the removal of 14 blocks, the number of possible block combinations escalates to $\binom{28}{14}$. Therefore, it is computationally infeasible to evaluate the \gls{CLIP}-score and \gls{CMMD} for every possible combination.\\

\subsubsection{Optuna}
A natural alternative is to perform a greedy search algorithm. In this approach, the importance ranking for all blocks is initially computed, after which the least important block is identified and compressed. Next, the \gls{CLIP}-scores and \gls{CMMD} are re-evaluated for the remaining blocks within the newly reduced architecture. By iteratively evaluating and compressing the least important block the model architecture is progressively reduced while accounting for the interdependencies between blocks. \\
Another approach is to reformulate the block selection problem as hyperparameter search using the hyperparameter framework Optuna. The number of blocks to compress $N_c$, the blocks that are compressed $\{B_i\}_{i=1,\dots,N_c}$ and the compression ratios of each block $\{ \pi_i \}_{i=1,\dots,N_c}$ can be potential hyperparameters to optimize. We focused on identifying the blocks $\{B_i\}_{i=1,\dots,N_c}$ given $N_c$ and  $\{ \pi_i \}_{i=1,\dots,N_c}$ to reduce the search space. As objective function the \gls{CMMD} metric is computed similarly like for the greedy algorithm on 100 reference images from LAION dataset. To further speed up the search process, the results of the greedy-algorithm are given as prior to the Optuna optimization framework such that the first trails are not picked randomly which would increase the search time drastically until a good setting is found. \\
The Optuna hyperparameter search was tested for the iterative SVD distillation framework with fixed compression ratios $\pi_i = 0.6$ $ \forall i \in \{1, \dots, N_c\}$, where the number of compressed transformer blocks was chosen for each iteration individually. The procedure to determine the best transformer blocks to compress was as follows. First, a greedy algorithm was leveraged to find a prior of potentially good candidates for compression. Afterwards, the Optuna hyperparameter framework was applied to refine the block selection. In total 300 different configurations were tried out by Optuna where the decision criterion was solely the \gls{CMMD} score. The pruned model was then retrained. This procedure  was executed repeatedly for four iterations. Fig. \ref{fig:AdvantageOfOPtunaPreTraining} shows the prior blocks from the greedy algorithm as well as the final block selection of the Optuna algorithm. On the right side the corresponding \gls{CMMD} values which were computed on a small reference dataset of 100 images are displayed for each configuration. Note this is the \gls{CMMD} value before retraining to compare the prior with the final selection. The grey blocks, represent the consensus where the Optuna algorithm did not change the blocks from the prior whereas the yellow and green blocks mark the exclusive blocks selected by the greedy algorithm  and the Optuna algorithm respectively. For iterations one to three, one observes that the majority of blocks from the greedy prior are maintained by the optuna algorithm, only in iteration four do two out of three blocks differ. The large overlap is a first indication that the greedy algorithm already performs well. However, because the greedy result was used as prior the optuna framework search is focused around the greedy result which might also explain the high similarity. Overall, one can conclude that the \gls{CMMD} values obtained by the Optuna algorithm are consistently smaller (better) than the ones obtained by the greedy algorithm. This indicates that at least the pre-training performance is enhanced by the refinement step. \\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/optuna/optuna_greedy_comb.png}  
	\caption[Comparison of Pruning Strategies: Optuna vs. Greedy Algorithm]{Comparison of removed blocks per iteration by the Greedy algorithm versus the Optuna framework. 
		In each iteration, the Greedy algorithm's selection served as the initialization prior for the Optuna hyperparameter search. 
		The heatmap visualizes the divergence in block selection:.While gray blocks represent consensus, the colored blocks highlight where Optuna (green) found a superior structural reduction compared to the Greedy baseline (yellow). 
		The model was retrained between iterations based on the optimized configuration. 
		The annotations demonstrate that Optuna consistently achieves lower (better) \gls{CMMD} scores. 
	}
	\label{fig:AdvantageOfOPtunaPreTraining}
\end{figure}
Above, we observed that in each iteration, the Optuna framework was able to identify a block combination for compression which resulted in a smaller \gls{CMMD} value compared to the prior greedy algorithm.
 In the next step, we compressed the model twice, once based on the greedy algorithm and once based on the Optuna algorithm as described above, each time targeting the same number of parameters (same compression rates $\pi_i = 0.6$ and same number of blocks in each iteration). Fig. \ref{fig:AdvantageOfOPtunaPostTraining} shows the corresponding distribution of blocks being compressed in each iteration. Please note that here "Greedy" specifies the compression scheme where the greedy algorithm is strictly utilized as the block selection algorithm, in contrast to the previous section, where "Greedy" just represented the prior for the Optuna algorithm.
  Looking at the block distribution, one observes that the early layers are compressed heavily in both schemes first, indicating a consensus on the high redundancy of these initial blocks. In the later layers, no clear structure is visible and the selections diverge. This lack of a consistent pattern suggests that the deeper blocks are largely interchangeable regarding their impact on the model's capacity. 
 The evaluation of the retrained compressed models (see fig. \ref{fig:BenchmarksOptuna}) does not provide clear evidence that the Optuna block selection algorithm led to better performing models post-retraining. One observes that for the smallest model, the HPSv2 score is even better for the greedy algorithm, and a similar trend is observed for the second and third iteration regarding the GenEval score. This demonstrates that the retraining process acts as an equalizer, effectively eliminating the small primary advantage of the Optuna algorithm. 






\begin{figure}[H]
	\centering
	% --- First Image ---
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/Experimente/optuna/optuna_hpsv2.png}
		\caption{HPSv2 Benchmark}
		\label{fig:image1}
	\end{subfigure}% <--- WICHTIG: Dieses % verhindert ein ungewolltes Leerzeichen!
	\hfill
	% --- Second Image ---
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/Experimente/optuna/optuna_geneval.png}
		\caption{GenEval Benchmark}
		\label{fig:image2}
	\end{subfigure}
	
	\caption[Comparison of Pruning Strategies]{Comparison versus Optuna framework and greedy algorithm... (a) shows...}
	\label{fig:BenchmarksOptuna}
\end{figure}

 \begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{images/Experimente/optuna/blocks_optuna_greedy.png}  
\caption[Comparison of Pruning Strategies: Optuna vs. Greedy Algorithm Regarding Training]{Here, all compressed blocks based on a greedy and optuna pruning scheme are displayed per iteration. 
}
\label{fig:AdvantageOfOPtunaPostTraining}
\end{figure}
\subsubsection{Learnable Block Selection}
The learnable block selection algorithm described in sec. \ref{sec:TinyFusion} is exploit on PixArt-$\Sigma$. As suggested in the corresponding paper \cite{TinyFusion} \gls{LoRA} finetuning was utilized for jointly learning the mask and updating the model. The exact training hyperparameters are provided in tab. \ref{tab:TinyFusionHyperparameters}. The 28 transformer blocks from PixArt-$\Sigma$ are disected into four subgroups (1-7, 8-14, 15-21, 22-28) and within each three blocks are masked out. \\
Fig. \ref{fig:TinyFusionConfidence} displays the confidence score across all four block groups. In the first group, containing blocks 1-7, the confidence score initially remains low before exhibiting a rapid increase after 1,500 training steps. As shown in fig. \ref{fig:TinyFusionBlockSelection} this point marks the convergence to a final block configuration. Similarly, the fourth group, containing blocks 22-28, converges to a final block configuration with relatively high confidence. In contrast, the confidence for groups two and three fluctuates around 50\% . This result suggests that the blocks in the outer groups (one and four) fulfill distinct tasks, enabling to identify a clear hierarchy of importance. On the contrary, the confidence score of the middle groups (two and three)  is low suggesting that these blocks exhibit a high degree of redundancy, explaining the absence of a single optimal pruning configuration. This observation aligns with the qualitative investigation of the block importance, where early and late layers were identified as crucial for the image quality. Notably, however, the first, third, and twenty seventh blocks were selected for removal in this experiment although fig. \ref{fig:PixArtBlockImportanceQualitatively} suggests that their removal lead to severe quality degradation. 

 \begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{images/Experimente/TinyFusion/ConfidenceScore.png}  
	\caption[Confidence Score for Learned Mask]{Confidence score for the mask configuration of each subset.
	}
	\label{fig:TinyFusionConfidence}
\end{figure}

 \begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{images/Experimente/TinyFusion/BlockDistribution.png}  
	\caption[Learned Block Removal over Time]{Evolution of selected blocks for removal during mask optimization process.
	}
	\label{fig:TinyFusionBlockSelection}
\end{figure}

\begin{table}[H]
	\centering
	\caption{Training hyperparameters for PixArt \gls{LoRA} finetuning and mask learning}
	\label{tab:TinyFusionHyperparameters}
	\begin{tabular}{l c l}
		\toprule
		\textbf{Hyperparameter} & \textbf{Value} & \textbf{Description} \\
		\midrule
		Resolution & 512 & Input image resolution \\
		Dataset & LAION-HD-Subset & high-resolution subset of the LAION-5B dataset \tablefootnote{https://huggingface.co/datasets/yuvalkirstain/laion-hd-subset/viewer/default/train} \\
		Batch Size & 16 & Samples per batch \\
		Num Epochs & 10 & Total training duration \\
		Learning Rate & $1 \times 10^{-5}$ & Constant learning rate \\
		LoRA Rank & 16 & Rank for low-rank adaptation \\
		Tau Range ($\tau$) & 4.0 -- 0.1 & Temperature range for Gumbel-Softmax \\
		Gate LR Multiplier & 10.0 & Learning rate multiplier for gate parameters \\
		Scaling Range & 100 -- 100 & Scaling range for gate logits \\
		\bottomrule
	\end{tabular}
\end{table}


\subsection{Progressive Model Pruning vs One-Shot Model Pruning}
\label{sec:OneShot_vs_Progrosseve_Pruning}
One-shot model pruning compresses the model to target size in a single step, while progressive pruning employs an iterative scheme of incrementally removing parameters followed by recovery training phases. Both frameworks are tested using the complete block removal approach. The number of blocks removed for each model and the number of training steps are summarized in tab. \ref{tab:SettingOneShotvsProgressive} and the exact blocks which are removed for each model can be found in the appendix \ref{sec:OneShotvsProgressiveBlockDistribution} (see fig. \ref{fig:OneShotvsProgressiveBlockDistribution}). Fig. \ref{fig:OneShotvsProgressive} displays the performance of the one-shot versus progressive pruning on the HPSv2 and GenEval benchmarks across various compression ratios. To ensure a fair comparison, the models for both methods are trained for the same amount of training steps and on exactly the same data. Both benchmarks demonstrate that one-shot compression is a viable strategy in the low compression regime (10\% to 20\% parameter reduction). However, as the compression ratio increases the progressive approache becomes advantageous because the incremental removal of parameters allows the model to recover its generative capabilities more effectively compared to the abrupt removal of large amount of parameters at once. Based on this finding the following experiments are executed in the progressive model distillation framework.

\begin{figure}[H]
	\centering
	% --- First Image ---
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/Experimente/Iterative_vs_Direct/iterative_vs_direct_hpsv2.png}
		\caption{HPSv2 Benchmark}
		\label{fig:image1}
	\end{subfigure}% <--- WICHTIG: Dieses % verhindert ein ungewolltes Leerzeichen!
	\hfill
	% --- Second Image ---
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/Experimente/Iterative_vs_Direct/iterative_vs_direct_geneval.png}
		\caption{GenEval Benchmark}
		\label{fig:image2}
	\end{subfigure}
	
	\caption[Comparison of Pruning Strategies]{Comparison versus Optuna framework and greedy algorithm... (a) shows...}
	\label{fig:OneShotvsProgressive}
\end{figure}

	
	


\begin{table}[htbp]
	\centering
	\caption{Training and Model Specifications for Compressed Variants.}
	\label{tab:SettingOneShotvsProgressive}
	\begin{tabular}{@{} l c c c @{}}
		\toprule
		\textbf{Model Variant} & \textbf{Number of}  & \textbf{Training Steps} \\
		\textbf{(Remaining \%)} & \textbf{Removed Blocks}  & \textbf{(in Thousands)} \\
		\midrule
		Model 90\% & 3 & 88.5 \\
		Model 80\% & 6   & 265.5 \\
		Model 70\% & 8   & 531.0 \\
		Model 60\% & 11  & 708.0 \\
		Model 50\% & 14  & 973.5 \\
		\bottomrule
	\end{tabular}
	
\end{table}


\subsection{Knowledge Distillation Loss}
\label{sec:ExpKDLoss}
In this section the standard setting (see sec. \ref{sec:DefaultSetup}) except the learning objective is varied to investigate the effectiveness of the individual components of the knowledge distillation loss (see eq. \ref{eq:KnowledgeDistillationLoss}). In total, five different loss functions are testet: 


 \begin{itemize}
 	\item \textbf{Output-Only Distillation} ($\bm{\mathcal{L_\textbf{OutKD}}}$): \\ The \gls{MSE} between the final output of the student and the teacher serves as the sole learning signal.
 	\item \textbf{Standard Distillation} ($\bm{\mathcal{L_\textbf{OutKD}}} + \bm{\mathcal{L_\textbf{FeatKD}}}$): \\ The unweighted sum of the final output loss and the  intermediate feature loss is leveraged as learning objective.
 	\item   \textbf{Hybrid Distillation} ($\bm{\mathcal{L_\textbf{OutKD}}} + \bm{\mathcal{L_\textbf{FeatKD}}} + \bm{\mathcal{L_\textbf{Task}}}$): \\ The original diffusion loss is added to the standard distillation loss. 
 	\item \textbf{Normalized Distillation} ($\bm{\mathcal{L_\textbf{OutKD,norm}}} + \bm{\mathcal{L_\textbf{FeatKD,norm}}}$): \\ The final output and the intermediate feature loss are normalized (see sec. \ref{sec:KDLNormalization}) before summed together to ensure that individual layers do not dominate the total loss. 
 	\item  \textbf{Normalized Hybrid Distillation} ($\bm{\mathcal{L_\textbf{OutKD,norm}}} + \bm{\mathcal{L_\textbf{FeatKD,norm}}}+ \bm{\mathcal{L_\textbf{Task}}}$): \\ This configuration extends the normalized distillation objective by incorporating the original diffusion task loss.
 \end{itemize}
 For all loss configurations models of different size (90\% to 50\%) are trained whereas the same settings as described in tab. \ref{tab:SettingOneShotvsProgressive} are utilized. Fig. \ref{fig:LossInvestigation} presents the performance for the performance of models with varying degrees of compression based on the different learning objective. \\
 According to the HPSv2 benchmark which measures overall image quality, the normalization of the intermediate features and the final output loss proves advantageous (blue and red lines). Both loss configurations which utilize normalization achieve a consistently higher HPSv2 score compared to the unnormalized counterparts. The normalization ensures that the contribution of each intermediate feature to the overall loss is balanced. This prevents the optimization process from prioritizing the later layers where the errors are numerically larger to the detriment of the earlier layer. The numerically larger discrepancies between the teacher and the student stem from the larger feature magnitudes in deeper layers as displayed in fig. \ref{fig:MagnitudeIntermediateFeatures} which presents the \gls{RMS} of the intermediate features after every transformer block in PixArt-$\Sigma$. The increase in feature magnitude results from to the signal accumulation inherent to the PixArt-$\Sigma$ architecture which follows the residual formulation $\mathbf{x}_{l+1} = \mathbf{x}_l + B_l(\mathbf{x}_l)$ where $B_l$ representing the $l^\text{th}$ transformer block. \\
 Of the two loss configurations applying normalizaiton, the normalized hybrid distillation performs best. It is advantageous for the pruned model if besides the teacher's learning signal the original diffusion loss is leveraged too. The original loss becomes critical when the teacher is uncertain itself about a prediction as it anchors the student to the ground truth. \\
  The benefit of the original diffusion loss is also confirmed by the unnormalized settings where the hybrid distillation consistently achieves a higher HPSv2 score compared to the standard distillation approach.\\
  Interestingly, the output-only approach outperforms both standard and hybrid distillation settings for shallow compressed models. This suggests that the unbalanced feature loss prevents effective learning. However, for strongly compressed models the output-only loss configuration leads to an abrupt drop in quality. This shows that the intermediate feature loss despite being suboptimally distributed serves as a critical stabilizer of the training process.
  
  The GenEval benchmark mainly confirms the observations from the HPSv2 benchmark, that the normalized hybrid distillation approach performs best overall and that adding the original diffusion loss boosts performance. Notably, the output-only configuration results are even more pronounced by performing comparably good or better to the normalized hybrid distillation approach in the low compression regime (90\% to 70\%). This suggests that at low compression, removing the constraints of feature matching allows the student to optimize semantic alignment very effectively. However perfrmance drops severly for larger compression rates. Furthermore, it is worth noting, that for the high compression of 60\% or 50\% the unnormalized hybrid distillation approach slightly outperforms the normalized distillation approach. This indicates that in the high compression regime the original diffusion loss is critical for maintaining the text-to-image alignment.  \\
  
 \begin{figure}[H]
 	\centering
 	% --- First Image ---
 	\begin{subfigure}[b]{0.48\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{images/Experimente/loss_investigation/loss_investigation_hpsv2.png}
 		\caption{HPSv2 Benchmark}
 		\label{fig:image1}
 	\end{subfigure}% <--- WICHTIG: Dieses % verhindert ein ungewolltes Leerzeichen!
 	\hfill
 	% --- Second Image ---
 	\begin{subfigure}[b]{0.48\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{images/Experimente/loss_investigation/loss_investigation_geneval.png}
 		\caption{GenEval Benchmark}
 		\label{fig:image2}
 	\end{subfigure}
 	
 	\caption[Investigation of Knowledge Distialltion Loss]{Investigation of the importance of the individual components of the knowledge distillation loss.}
 	\label{fig:LossInvestigation}
 \end{figure}
 
  \begin{figure}[H]
 	\centering
 	\includegraphics[width=0.6\textwidth]{images/Experimente/loss_investigation/magnitude_block_features.png}  
 	\caption[Magnitude of Intermediate Features in PixArt-$\Sigma$]{The \gls{RMS} norm of the output features of the individual transformer blocks from PixArt-$\Sigma$ varies significantly. Features from later layers have a larger magnitude than earlier layers. The \gls{RMS} norm is computed on 100 prompts from LAION dataset and averaged. 
 	}
 	\label{fig:MagnitudeIntermediateFeatures}
 \end{figure}
 
 
 \subsection{Finetuning Protocol}
 \label{sec:FinetuningProtocol}
 After removing blocks from the PixArt-$\Sigma$ architecture, the reduced model is finetuned to recover its image generation capabilities. In total, three different finetuning protocols are tested to identify the most effecive one. In all protocols, the model was trained for 88.5k optimization steps to enable fair comparison.
 \begin{itemize}
 	\item \textbf{Three Stage Finetuning Protocol} \\
 	This is the default finetuning protocol described in sec. \ref{sec:DefaultSetup}. It is repeated for the reader's convinience.
 	\begin{enumerate}
 		\item Local Stabilization: The blocks immediately preceding the removed blocks are finetuned for 38k steps on the LAION dataset while all other parts of the model are frozen.
 		\item Global Optimization: The complete model is optimized for one epoch (38k steps) on the LAION dataset.
 		\item Domain Specific Refinement: The LAION-Pixart dataset is leveraged to finetune the pruned model for 12.5k steps on its teachers domain.
 	\end{enumerate}
 	
 	\item \textbf{Two Stage Finetunig Protocol} \\
 	In the following protocol, instead of performing local finetuning, the entire model is directly finetuned for 2 epochs.
 	\begin{enumerate}
 		 	\item Global Optimization: The complete model is optimized for two epoch (76k steps) on the LAION dataset.
 		\item Domain Specific Refinement: The LAION-Pixart dataset is leveraged to finetune the pruned model for 12.5k steps on its teachers domain.
 	\end{enumerate}

 	\item \textbf{Two Stage LAION only Finetuning Protocol} \\
 	The absence of the domain specific refinement is investigated in this protocol.
 	\begin{enumerate}
 		\item Local Stabilization: The blocks immediately preceding the removed blocks are finetuned for 38k steps on the LAION dataset while all other parts of the model are frozen.
 		\item Global Optimization: The complete model is optimized for two epoch (50.5k steps) on the LAION dataset.
 	\end{enumerate}
 \end{itemize}
 
 The evaluation of the performance obtained for models of different pruninng ratios (90\% to 50\%) can be found in fig. \ref{fig:FinetuningProtocols}. For both the HPSv2 and GenEval benchmarks, fine-tuning solely on the LAION dataset (yellow line) leads to a sharp decline for strongly compressed models. This is a strong indication that the image quality being higher in the LAION-PixArt dataset compared to the LAION dataset is highly relevant for the recoverage of the generation capability of a pruned transformer-based diffusion model. Moreover, it could be observed that for shallow compressed models (10\% to 20\%) the two stage finetuning protocol (green line) performs better than the three stage protocol (purple line). This indicates that in the low compression regime the model benefits from the extended global finetuning and no local repair is needed. For the middle compressed models (70\%-60\%), the three stage finetuning is consistently better or on par with the two stage protocol. This performance gap suggests that the local stabilization step provides necessary structural regularization when the model capacity is reduced to a critical threshold. For strong compression there are contradictory results. According to the HPSv2 benchmark the three stage approach is still marginally better than the two stage approach. However, the GenEval benchmark suggests that it is reverse. Given the high degree of degradation in both models at this stage, these marginal differences may also reflect the inherent instability of training extremely pruned networks rather than a definitive superiority of one method over the other.
 
 
  \begin{figure}[H]
 	\centering
 	% --- First Image ---
 	\begin{subfigure}[b]{0.48\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{images/Experimente/It_shortcut_no_pix/iter_shortcut_no_pix_hpsv2.png}
 		\caption{HPSv2 Benchmark}
 		\label{fig:image1}
 	\end{subfigure}% <--- WICHTIG: Dieses % verhindert ein ungewolltes Leerzeichen!
 	\hfill
 	% --- Second Image ---
 	\begin{subfigure}[b]{0.48\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{images/Experimente/It_shortcut_no_pix/iter_shortcut_no_pix_geneval.png}
 		\caption{GenEval Benchmark}
 		\label{fig:image2}
 	\end{subfigure}
 	
 	\caption[Finetuning Protocols Benchmark]{Investigation finetuning protocols.}
 	\label{fig:FinetuningProtocols}
 \end{figure}
 
 
 \subsection{Structural Compression Strategy}
 As an alternative to the pruning strategy of removing entire transformer blocks from PixArt-$\Sigma$, a compression strategy utilizing \gls{SVD} compression is investigated. This decomposition is applied to the \gls{MLP}, self-attention and cross-attention matrices within a transformer block. To evaluate the efficiency of the \gls{SVD} compression method against the structural block removal method, a representative operating point is chosen. The rank for the \gls{MLP} is set to $r_\text{MLP} = 512$ corresponding to a compression of approximately $44\%$ of its parameters and the ranks for the self- and cross-attention matrices are fixed to $r_\text{self-attn} = r_\text{cross-attn} = 128$ representing a parameter reduction of approximately $78\%$. This serves as a starting point. Later, the choice of compressing attention matrices more aggressively than the \gls{MLP} matrices is further investigated (see sec. \ref{sec:MLPAttnDiffCompRatios}). \\
  Tab. \ref{tab:SVDCompressionFirstExp} compares the configurations of both methods, specifically the number of blocks removed or compressed and corresponding training duration for each model. Note that the number of training steps differs significantly, as the default criterion - a \gls{CMMD} value exceeding 0.1 - was employed to the pruning rate per iteration, consistent with the default settings. The observation that the \gls{SVD} allows for more parameters to be removed with a less severe impact on the \gls{CMMD} value on the small reference dataset is an initial indication that compression is a more nuanced method than block removal. Fig. \ref{fig:SVD_vs_Removal} presents the performance for both methods on the HPSv2 and GenEval benchmarks. For both benchmarks the \gls{SVD} compression method outperforms the complete block removal method in low and high compression regimes. This suggests that distributing the parameter reduction across multiple blocks via compression is less adverse to model performance compared to removing individual blocks completely.
 
 \begin{table}[htbp]
 	\centering
 	\caption{Comparing training steps and modified blocks for block removal versus \gls{SVD} compression method.}
 	\label{tab:SVDCompressionFirstExp}
 	\begin{tabular}{@{} l c c c c @{}}
 		\toprule
 		\textbf{Model Variant} & \textbf{Number of}  & \textbf{Number of SVD} & \textbf{Training Steps} \\
 		\textbf{(Remaining \%)} & \textbf{Removed Blocks} & \textbf{Compressed Blocks} & \textbf{(in Thousands)} \\
 		\midrule
 		Model 90\% & 3 & - & 88.5 \\
 		SVD Model 90\% & - & 6 & 88.5 \\
 		Model 80\% & 6   & - &  265.5 \\
 		SVD Model 80\% & - & 11 & 88.5 \\
 		Model 70\% & 8   & - &  531.0 \\
 		SVD Model 70\% & - & 17 & 265,5k \\
 		Model 60\% & 11  & - &  708.0 \\
 		SVD Model 60\% & - & 22 & 442,5k \\

 		\bottomrule
 	\end{tabular}
 	
 \end{table}
 
 
   \begin{figure}[H]
 	\centering
 	% --- First Image ---
 	\begin{subfigure}[b]{0.48\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{images/Experimente/SVD_VS_Removal/SVD_vs_removal_hpsv2.png}
 		\caption{HPSv2 Benchmark}
 		\label{fig:image1}
 	\end{subfigure}% <--- WICHTIG: Dieses % verhindert ein ungewolltes Leerzeichen!
 	\hfill
 	% --- Second Image ---
 	\begin{subfigure}[b]{0.48\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{images/Experimente/SVD_VS_Removal/SVD_vs_removal_geneval.png}
 		\caption{GenEval Benchmark}
 		\label{fig:image2}
 	\end{subfigure}
 	
 	\caption[Benchmarks Structural Compression Strategy]{HPSv2 (left) and GenEval (right) benchmark for complete block removal and \gls{SVD} compression strategy.}
 	\label{fig:SVD_vs_Removal}
 \end{figure}
 
 \subsection{Sensitivity Analysis of MLP and Attention Layers under \gls{SVD} Compression}
 \label{sec:MLPAttnDiffCompRatios}
 In this section the \gls{SVD} compression strategy is applied but otherwise the default settings remains. The goal is to investigate how to distribute the parameter removal of the individual components of one transformer blocks best. Therefore, individual compression ratios are applied to the \gls{MLP} matrix and the attention matrices whereby both self- and cross-attention matrices are prunded by the same amount. In total three different scenarios are explored:
 
 \begin{itemize}
 	\item \textbf{Equal Compression}: \\
 	Both \gls{MLP} and attention matrices are compressed by the same percentage $p_\text{MLP} = p_\text{Attn} = 60\%$. 
 	\item \textbf{Attention-Dominant Compression}: \\
 	The attention matrices are more strongly compressed $p_\text{Attn} = 80\%$ compared to the \gls{MLP} matrix $p_\text{MLP} = 40\%$.
 	 	\item \textbf{\gls{MLP}-Dominant Compression}: \\
 	The \gls{MLP} matrix are more strongly compressed $p_\text{Attn} = 80\%$ compared to the attention matrices $p_\text{MLP} = 40\%$.
 \end{itemize}
 Note that the combined parameter count of self- and cross-attention is approximately equal to the parameter count of the \gls{MLP}. Therefore, all configurations remove a similar amount of total parameters of one transformer block. \\
The scores in fig. \ref{fig:DistributionCompressionRatios} reveal two distinct trends. The HPSv2 score demonstrates that prioritizing the compression of the attention matrices consistently yields higher image quality than the method focused on compressing the MLP matrices. The GenEval benchmark, however, shows that although the attention-dominant approach is advantageous in the low-compression regime, it declines sharply under heavier compression, eventually falling below the MLP-dominant approach for models with approximately 400 million parameters. \\
 One possible explanation for the divergence between the two benchmarks is the distribution of tasks among the different components of a transformer block. Recent studies suggest \cite{dong2025attention} that MLPs are crucial for knowledge storage and the processing of visual features. Consequently, strong compression of these layers leads to a significant decrease in image quality (see HPSv2 benchmark yellow line). In contrast, the GenEval benchmark measures text-image alignment. Since the cross-attention mechanism is responsible for integrating the text prompts into the visual latents its compression in many transformer blocks severially compromise severily compromising this linking. While the image quality suffers less than with the strong MLP compression, the model's text comprehension deteriorates disproportionately. This explains why the attention-dominant approach loses its advantage for heavy compression, and is in some cases even worse than the MLP-dominant approach in the GenEval benchmark. \\
 The equal compression strategy exhibits results similar to the attention-dominant compression approach in the high-compression regimes but is slightly worse in the low-compression regime. 
 
    \begin{figure}[H]
 	\centering
 	% --- First Image ---
 	\begin{subfigure}[b]{0.48\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{images/Experimente/Different_MLP_Attn_Pruning/differnt_comp_ratios_hpsv2.png}
 		\caption{HPSv2 Benchmark}
 		\label{fig:image1}
 	\end{subfigure}% <--- WICHTIG: Dieses % verhindert ein ungewolltes Leerzeichen!
 	\hfill
 	% --- Second Image ---
 	\begin{subfigure}[b]{0.48\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{images/Experimente/Different_MLP_Attn_Pruning/differnt_comp_ratios_geneval.png}
 		\caption{GenEval Benchmark}
 		\label{fig:image2}
 	\end{subfigure}
 	
 	\caption[Benchmarks Structural Compression Strategy]{HPSv2 (left) and GenEval (right) benchmark for complete block removal and \gls{SVD} compression strategy.}
 	\label{fig:DistributionCompressionRatios}
 \end{figure}
 

 	

 
 \subsection{SVD Compression Scheduling}
In previous experiments, SVD compression was applied to each block at most once. However, restricting compression to untouched blocks may be suboptimal and incrementally reducing the size of an already compressed block could potentially preserve model performance better than pruning a dense block. To investigate this, we apply the linear progressive compression scheme defined in sec. \ref{sec:LinearProgressiveCompression} is investigated in the following. Hereby, three different scenarios are investigated where different number of blocks (7, 14 or 21, 28) are compressed per iteration. This examines whether it is advantageous to compress more blocks but to a lesser extent, or whether the parameters to be removed should be better distributed across fewer blocks. Here, in each iteration 10\% of the original number of parameters are removed.  \\



 \textbf{Untersuche ob immer neue Blöcke gepruned werden oder eher dieselbenr mehrfach}
 
 \subsection{Summary of Best Settings for Structural Model Pruning}
 Considering the results of all experiments, they lead to the following best configuration for structural model pruning:
 
 \begin{itemize}
 	\item \textbf{Block Importance Analysis} \\
 	The experiments (sec. \ref{sec:BlockImportanceAnalyses}) showed that using the \gls{CMMD} value as primary block selection criteria is working well. Particularly in linear \gls{SVD} compression scheduling, where the number of compressed parameters varies per block, the \gls{CMMD} can be weighted by the number of removed parameters to ensure a fair comparison. \\
 	As block selection method, we found that the greedy algorithm already performs well enough such that more advanced methods like Optuna do not provide substantial benefits that would justify the additional compute.
 	
 	\item \textbf{Pruning Method} \\
 	The progressive model pruning has profen advantageous over the one-shot model pruning in the high-compression regime where models are compressed more than 70\% of the original size (sec. \ref{sec:OneShot_vs_Progrosseve_Pruning})). Therefore, this is the first choice.
 	
 	\item \textbf{Knowledge Distillation Loss} \\
 	The results indicates that the combination of the original diffusion loss together with the normalized feature and output loss leads to the strongest performance in all pruning regimes (sec. \ref{sec:ExpKDLoss}). 
 	
 	\item \textbf{Finetuning Protocol} \\
 	The finding suggests that incorporating the LAION-PixArt dataset into the training procedure is critical as it contains images of higher quality demonstrated in the higher HPSv2 and GenEval scores for the three and two stage finetuning protocols. However, the results regarding the superiority of three stage protocal over the two stage protocal are inconclusive. It is not clear if local finetuning is beneficial or not for high compression regimes (sec. \ref{sec:FinetuningProtocol}). Therefore, the two stage approach is chosen for the final setting.
 	
 	\item \textbf{Structural Compressin Strategy} \\
 	The experiments demonstrated that leveraging \gls{SVD} compression over complete block removal leads to superior performance even for less training steps (sec. \ref{sec:StructuralCompressionStrategy}).
 	
 	\item \textbf{Sensitivity Analysis of MLP and Attention Layers under \gls{SVD} Compression} \\
 	The sensitivity analysis for \gls{MLP} and attention layers revealed that strong compression of \gls{MLP}s led to more severe deterioration than heavy compression of attention layers. However, compressing both \gls{MLP} and attention layers equally yielded performance comparable to the strong attention compression experiment. Consequently, the equal compression scheme was adopted for the final configuration.
 	
 	\item \textbf{\gls{SVD} Compression Scheduling} \\
 	
 \end{itemize}