\pagenumbering{arabic}
\chapter{Introduction}
Over the past few years, generative artificial intelligence (\gls{AI})  has gained immense importance and relevance in society, as well as in business and politics. In addition to large language models (\gls{LLM}s), which have brought generative AI into the mainstream, generative image models are increasingly coming into focus. On the one hand, they open up a range of new possibilities in areas such as design and marketing or gaming and entertainment. On the other hand, they enable the generation of highly realistic synthetic images and videos, often referred to as deepfakes, raising severe ethical and privacy concerns, which is why policymakers are striving to establish rules-based guidelines for their use (European AI Act). \\
The advent of deep learning based image generators is often marked by the introduction of variational autoencoders (\gls{VAE}s) in 2013, as they demonstrated that neural networks could learn complex data distribution and generate novel samples. Subsequently, generative adversarial networks (\gls{GAN}s) became the dominant model type in the image generation field as they produce images of significantly higher quality than \gls{VAE}s. However, \gls{GAN}s are notoriously difficult to train, as they often exhibit mode collapse and suffer from exploding or vanishing gradients.\\
 In recent years, \gls{GAN}s were largely surpassed by diffusion models deploying an iterative image generation denoising process. The breakthrough of diffusion models as the new state-of-the-art in image generation models occured mainly in 2022 when the latent diffusion models, most notably Stable Diffusion, were released. At this time, they excelled in image quality and image generation diversity, but came with the cost of high inference time due to the iterative nature of their image generation process.\\
  The first stable diffusion models leveraged a U-Net architecture and already required a huge amount of training data and computational resources. The image generation process is typically controlled by textual prompts provided in the stable diffusion models via cross-attention mechanisms. To expand these capabilities, several new control and finetuning methods like ControlNet and DreamBooth were developed to enable precise spatial conditioning and personalization. Since the U-Net-based SD models already included transformer layers for text-conditioning, the next logical evolution was to transition to purely transformer-based models known as diffusion transformers (\gls{DiT}s), which also increased significantly in parameter size and required training data. In addition, a new formulation, so-called flow-matching, became predominant due to improved training stability and sampling efficiency which is leveraged by the current SOTA image generations like Flux.1-dev being a 12 billion parameter model.\\
  Possible areas of application for these models include edge devices such as mobile phones, e.g. image inpainting. In this case, it is advantageous to run these models directly on the edge devices if possible in order to avoid server costs, to make the associated functionality available offline, and to protect the privacy of users so that, for example, images do not have to be uploaded to external servers for processing.
  However, the current flow models like Flux.1-dev face two distinct challenges for real-time applications on edge devices. On the one hand, as mentioned before, they suffer from a long inference time due to the sequential application of the model. On the other hand, these models also require a substantial amount of VRAM during inference, e.g. Flux.1-dev requires about 32GB, which makes them unsuitable on edge devices like smartphones.\\
  There are several approaches to speed up inference time. For this, the main research stream is to enable the model to generate high-quality images with fewer inference steps. While at the beginning, diffusion models required up to 1000 inference steps to generate one single image, using more advanced \gls{ODE}-solvers for the denoising trajectory has led to significant reduction in inference steps. Another line of research is step- and guidance distillation. Distillation methods generally attempt to transfer the capabilities of a teacher model to a student model. In step distillation, the same diffusion model is often used as both the teacher and student model, and the goal is to teach the student model to generate images of the same or similar quality with as few as 4 or 2, sometimes even only 1 inference step leveraging its original capabilities \cite{ADD,LADD,ProgressiveDistillation,ConsistencyModel,LatentConsistencyModel,DistillationMatching}. This technique is applied to Flux-schnell, reducing the requirement to only 4 inference steps. In guidance distillation, the goal is to incorporate the concept of classifier free guidance (\gls{CFG}), which normally requires the model to be applied twice per inference step, into the model itself, thereby saving half of the model evaluations. This procedure was utilized to distill Flux.1-pro into Flux.1-dev.\\
  However, while step distillation and advanced solver effectively reduce the latency (inference time), they do not necessiarily adress the second bottleneck, namely high VRAM consumption. Even a faster model cannot be employed to an edge device if its parameters exceed the available VRAM. Proposed methods for VRAM reduction range from quantisation and more efficient implementation of model components, e.g. Flash Attention, to structural or unstructural model pruning.\\
   The first part of this thesis systematically examines how the number of parameters in diffusion transformers can be effectively reduced while minimizing the degradation of image generation quality, using PixArt-$\Sigma$ as a representative baseline. 
   Specifically, this research investigates the identification of architectural components with redundant parameters alongside the best pruning strategies, complete layer removal versus low-rank compression, and pruning schedules, e.g., one-shot versus progressive pruning. 
   Furthermore, various post-compression retraining frameworks are evaluated to optimize performance recovery. The empirical results demonstrate that progressively compressing model blocks via low-rank approximation using a linear distribution of compression ratios meaning compressing the least important structures most aggressively and more critical structures less, leads to the best results. \\
   In the second part of this thesis, the previously identified best pruning strategy is applied to the Flux.1-dev model, demonstrating that it outperforms existing compression methods.\\
    Furthermore, a training-free compression strategy originally proposed for \gls{LLM}s that is also based on \gls{SVD} is transferred to Flux.1-dev achieving superior performance compared to current alternatives. \\
  
  In the following, chapter two provides a general overview of image generator models, with a particular focus on diffusion and flow models. It also includes relevant metrics for measuring the performance of image generators. Chapter three provides an overview of alternative model compression methods, with a focus on methods applied to diffusion and flow models. Chapter 4 introduces all the methods used that were investigated, and Chapter 5 shows the results of the experiments to determine the effectiveness of these methods.
  
  
  

%   Famous distillation frameworks are latent adversarial diffusion distillation \cite{ADD,LADD} which utilizes besides a teacher model a discriminator as additional learning signal which differentiate if an image was generated by the student model or is from the real dataset or latent consistency models \cite{ConsistencyModel,LatentConsistencyModel} which are designed to map images with different noise levels to their corresponding clean image in one step by enforcing the self-consistency property meaning that two noisy images lying on the same denoising path are always mapped to the same clean image.
