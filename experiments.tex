\chapter{Experiments}
This master thesis investigate structural model distillation techniques, specifically focusing on the removal and compression of transformer blocks within the PixArt-$\Sigma$ and Flux-dev architectures. PixArt-$\Sigma$ is employed as an experimental baseline to evaluate various design choices in the distillation frameworks due to its relatively low parameter count enabling rapid iteration and extensive ablation studies. Subsequently, the most effective strategies are applied to Flux-dev to assess their efficacy in high-parameter regimes. 
\section{Pixart-$\Sigma$ Distillation}
\subsection{Block Importance Analysis}
In structural pruning, the selection of components to be removed or compressed is essential to ensure that the compressed model retains the highest possible image generation quality. Qualitatively, fig. \ref{fig:PixArtBlockImportanceQualitatively} shows the effect of removing every transformer block from PixArt-$\Sigma$ individually on its image generation capabilities without subsequent retraining. It is clearly visible that removing the first and last transformer blocks (blocks 1-4 and blocks 27-28) exerts the most significant influence on the image quality. This confirms the observation from previous work \cite{men2024shortgpt} that redundancies are primarily find in the middle part of transformer based model. To identify which blocks are best to remove meaning which blocks have a minimal contribution on the generation process of the final image, several different metrics can be employed. In the following section, different approaches are investigated to determine the importance of the individual transformer blocks. First, the traditional magnitude-based \cite{han2015learning} pruning and represential dissimilarity by computing the central kernal alignment score (\gls{CKA}) \cite{kornblith2019similarity} between the input and output of each transformer block are evaluated. Moreover, they are compared to standard performance metrics such as the \gls{CLIP}-score and \gls{CMMD}. \\
 Normally, several transformer blocks are removed or compressed simultaneously. Due to block interdependencies, we investigate three different options to identify the best grouping for blocks to remove. First, a greedy-algorithm is applied, iteratively searching for the best next block to compress based on the already compressed model from previous iterations. Second, the block selection process is formulated as hyperparameter search leveraging the Optuna framework. Last, a procedure of learning a mask indicating the blocks with least impact on the model performance is applied.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{images/Experimente/PixArt_Block_Analysis/BlockImportance.pdf}  % adjust filename and width
	\caption[Qualitative analyse of the block sensitivitäy of PixArt-$\Sigma$]{The figure shows the influence of selectively removing individual transformer blocks (1–28) on the final generation result. While the removal of middle blocks causes only minor visual deviations, interventions in early, structure-forming layers (blocks 1–4) and in the final layers (blocks 27–28) lead to a significant loss of image coherence and massive compression artifacts, respectively.}
	
	\label{fig:PixArtBlockImportanceQualitatively}
\end{figure}

\subsubsection{Compression Criteria}
In magnitude based pruning strategies the model parts with the lowest averaged weight magnitude are considered to be less important for the overall model performance. Interestingly, in Pixart-$\Sigma$ the weight magnitudes of the initial layers, which have a substantial influence on the final image synthesis (see fig. \ref{fig:PixArtBlockImportanceQualitatively}), have the lowest weight magnitudes across the architecture (see fig. \ref{fig:PixArtBlockMagnitudeCLIP} and \ref{fig:PixArtBlockMagnitudeCMMD} which both show the weight magnitudes for every block compared to the \gls{CLIP}-score or \gls{CMMD}). This observation contradicts with the assumption of low impact by low weights magnitudes. Furthermore, the weight magnitudes of the final layers are only marginally higher than those of the middle layers, while the middle layers' weight magnitudes exhibit a high degree of similarity. This would make a precise selection of blocks for removal based solely on this heuristic difficult. Consequently, using the weight magnitude to identify redundant blocks within PixArt-$\Sigma$ is not a suitable criterion for strucural distillation in this context.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/Pixart_Block_Analysis/magnitude_vs_clip.png}
	\caption[Magnitude-base pruning vs \gls{CLIP}-score]{The histogram shows the magnitudes of the weights of the individual transformer blocks (1-28). The blocks which have shown qualitatively (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) to be very important for the image generation capabilities of PixArt-$\Sigma$ are marked with red. In addition, the \gls{CLIP}-scores for each block is displayed. The \gls{CLIP}-scores follow the qualitative observations in contrast to the magnitude-based analysis. }
	\label{fig:magnitude_clip}
	
	\label{fig:PixArtBlockMagnitudeCLIP}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/Pixart_Block_Analysis/magnitude_vs_cmmd.png}
	\caption[Magnitude-base pruning vs \gls{CMMD}]{The histogram shows the magnitudes of the weights of the individual transformer blocks (1-28). The blocks which have shown qualitatively (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) to be very important for the image generation capabilities of PixArt-$\Sigma$ are marked with red. In addition, the \gls{CMMD} for each block is displayed. The \gls{CMMD} follow the qualitative observations in contrast to the magnitude-based analysis. }
	
	\label{fig:PixArtBlockMagnitudeCMMD}
\end{figure}

The \gls{CKA} score is computed for every block and every sampling step across 100 example prompts taken from the LAION dataset. Subsequently, these scores are averaged across all samples and time steps resulting in a mean similarity score $\mu_{CKA,i}$ for each individual Pixart-$\Sigma$ block. Averaging over the time steps should provide an estimate of the general importance of the block. However, the importance of a block at different time steps can vary drastically (see Appendix A). A low value indicates that the input and output of a specific block differs significantly, suggesting higher importance of the block. \\
Both fig. \ref{fig:PixArtBlockImportanceCLIP} and \ref{fig:PixArtBlockImportanceCMMD} show the transformation intensity $1-\mu_{CKA,i}$ for every block compared with \gls{CMMD} or \gls{CLIP} score, a greate value signifies a more substantial feature transformaton. While blocks 17 and 22 stand out with their high values, the implied importance of these blocks cannot be confirmed by qualitative analysis (see fig. \ref{fig:PixArtBlockImportanceQualitatively}). Furthermore, blocks 1, 24, 27, and 28 are not uniquely identified as critical blocks for the performance of the model, as the values for blocks 19, 20, and 21 are higher, which would mark them as more relevant. As this ranking also fails to align with the qualitative review of block influence on the final image, the use of representational similarity for block selection in PixArt-$\Sigma$ must be viewed critically too.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/Pixart_Block_Analysis/cka_vs_clip.png}  % adjust filename and width
	\caption[\gls{CKA} transformation intensity vs \gls{CLIP}-score]{The histogram shows the transformation intensity based on \gls{CKA} comparing the input and output features of the individual transformer blocks (1-28). The blocks which have shown qualitatively (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) to be very important for the image generation capabilities of PixArt-$\Sigma$ are marked with red. In addition, the \gls{CLIP}-scores for each block is displayed. The \gls{CLIP}-scores follow the qualitative observations in contrast to the transformation intensity analysis. }

	\label{fig:PixArtBlockImportanceCLIP}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{images/Experimente/Pixart_Block_Analysis/cka_vs_cmmd.png}  adjust filename and width	\caption[\gls{CKA} transformation intensity vs \gls{CMMD}]{The histogram shows the transformation intensity based on \gls{CKA} comparing the input and output features of the individual transformer blocks (1-28). The blocks which have shown qualitatively (see fig. \ref{fig:PixArtBlockImportanceQualitatively}) to be very important for the image generation capabilities of PixArt-$\Sigma$ are marked with red. In addition, the \gls{CMMD} for each block is displayed. The \gls{CMMD} follow the qualitative observations in contrast to the transformation intensity analysis. }

	\label{fig:PixArtBlockImportanceCMMD}
\end{figure}

Due to the contradictory results of magnitude-based pruning and \gls{CKA}-scores when compared with the qualitative observations, a third methodology was investigated, namely quantifying the importance of individual blocks based on standard performance metrics \cite{FastFlux}. Specifically, the \gls{CLIP}-score and \gls{CMMD} are computed on a small reference dataset consisting of 100 images from the LAION dataset. It should be noted that the standard protocols for computing statistically robust \gls{CMMD} and \gls{CLIP} scores typically require larger datasets, e.g. upwards of 10,000. However, given the iterative nature of the greedy search (see sec. \ref{label}) and the high computational overhead associated with repeated evaluations, using such a large reference set was prohibitively expensive. Nevertheless, a sample size of 100 images serves as a sufficient proxy to capture the relative importance ranking of the transformer blocks, providing the necessary directional guidance for the distillation process. As illustrated in the fig. \ref{fig:PixArtBlockImportanceCLIP} and \ref{fig:PixArtBlockImportanceCMMD} (and likewise in fig. \ref{fig:PixArtBlockMagnitudeCLIP} and \ref{fig:PixArtBlockMagnitudeCMMD}) both metrics closely align with the qualitative evaluation, confirming that the first four and the last two blocks exert the greatest impact on the final image quality. Consequently, these metrics are selected as primary criteria for identifying redundant blocks in PixArt-$\Sigma$.
This ranking serves as foundation for the subsequent, more more sophisticated block selection algorithms.\\

\subsubsection{Blocks Selection Algorithms}
A further key challenge after establishing the pruning criteria as the combination of \gls{CLIP}-score and \gls{CMMD} in structural model distillation is identifying the optimal combinations of blocks to remove simultaneously. When removing only a single block, there are merely 28 possibilities for Pixart-$\Sigma$. However, if the goal is to reduce the model size by $50\%$ necessitating the removal of 14 blocks, the number of possible block combinations escalates to $\binom{28}{14}$. Therefore, it is computationally infeasible to evaluate the \gls{CLIP}-score and \gls{CMMD} for every possible combination.\\
A natural alternative is to perform a greedy search algorithm. In this approach, the importance ranking for all blocks is initially computed, after which the least important block is identified and compressed. Next, the \gls{CLIP}-scores and \gls{CMMD} are re-evaluated for the remaining blocks within the newly reduced architecture. By iteratively evaluating and compressing the least important block the model architecture is progressively reduced while accounting for the interdependencies between blocks. \\
Another approach is to reformulate the block selection problem as hyperparameter search using the hyperparameter framework Optuna. The number of blocks to compress $N_c$, the blocks that are compressed $\{B_i\}_{i=1,\dots,N_c}$ and the compression ratios of each block $\{ \pi_i \}_{i=1,\dots,N_c}$ can be potential hyperparameters to optimize. We focused on identifying the blocks $\{B_i\}_{i=1,\dots,N_c}$ given $N_c$ and  $\{ \pi_i \}_{i=1,\dots,N_c}$ to reduce the search space. As objective function the \gls{CMMD} metric is used computed similarly like for the greedy algorithm on 100 reference images from LAION dataset. To further speed up the search process, the results of the greedy-algorithm are given as prior to Optuna such that the first trails are not picked randomly which would increase the search time drastically until a good setting is found. \\
The Optuna hyperparameter search was tested for the iterative SVD distillation framework with fixed compression ratios 


\begin{table}[ht]
	\centering
	\caption{Description of your table data.}
	\label{tab:my_table}
	\begin{tabularx}{\textwidth}{XXXXXXX} % l=left, c=center, r=right
		\toprule
		\textbf{Iteration     (Parameters)} & \textbf{Number Removed Blocks $N_c$} & \textbf{Greedy Algo $\{B_i\}_{i=1,\dots,N_c}$} & \textbf{Optuna $\{B_i\}_{i=1,\dots,N_c}$} & \textbf{Greedy Algo \gls{CMMD}} &  \textbf{Optuna \gls{CMMD}}\\
		\midrule
		1       & 11   & [0,1,2,3,4,5,6,16, 21,23,27] & [0,1,2,3,4,6,7,16, 17,25,27] & 0.122 & 0.086\\
		2  & 4 &[5,8,18,20]              &  [5,8,19,20]   & 0.128& 0.097\\
		3    &3 & [9,11,18]              & [9,11,21]                  & 0.188 & 0.171 \\
		4      &3     & [10,18,23]              & [14,18,22]                       & 0.551 & 0.292 \\
		\bottomrule
	\end{tabularx}
\end{table}