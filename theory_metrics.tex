\section{Evaluation Metrics}
For \gls{DM}s, several different metrics were introduced to measure the overall image quality, the distance of the generated distribution with the true distribution or the image prompt alignment. In the follwing, the metrics used in this work are presented: Fr\'echet Inception Distance (\gls{FID}) \cite{FID}, CLIP-Maximum Mean Discrepancy (\gls{CMMD}) \cite{cmmd}, Human Preference Score v2 (\gls{HPSv2}) \cite{HPSv2}



\subsection{Fr\'echet Inception Distance}
The \gls{FID} \cite{FID} measures the similarity between the model distribution $p_\text{model}$ with the true data distribution $p_\text{data}$. To compare the distributions a set of images is generated by the diffusion model and another set of images from the original data is needed. An inception model, e.g., Inception-v3 \cite{szegedy2016rethinking}, extracts features for every image of the sets. Under the assumption that the features from both sets follow a multivariate normal distribution the mean and covariance of the generated image features ($\boldsymbol{\mu}_\text{model}$, $\Sigma_\text{model}$) and the features from the original data ($\boldsymbol{\mu}_\text{data}$, $\Sigma_\text{data}$) are computed and compared by the Fr\'echet Distance \cite{dowson1982frechet}

\begin{equation}
	\text{FID} = \| \boldsymbol{\mu}_\text{data} - \boldsymbol{\mu}_\text{model}\|_2^2 + \text{Tr}\left(\Sigma_\text{model} + \Sigma_\text{data} - 2 \left(\Sigma_\text{model}\Sigma_\text{data}\right)^{\frac{1}{2}}\right) 
\end{equation}

meaning lower \gls{FID} values indicate a higher degree of similarity between the generated images and the original images. 

Although \gls{FID} is widely used \cite{cmmd} identified several shortcomings. First, the underlying Inception-v3 model is trained only on images from 1,000 classes which leads to embeddings that often fail to capture the rich and varied content of the images generated by current diffusion models. Secondly, the normality assumption for the feature distributions is frequentyl violated in reality which can lead to \gls{FID} scores which do not align to human ratings. A third drawback is that the \gls{FID} score does not capture complex distortion and depends highly sensitive on the sample size and exhibit bias.  For these reasons \cite{cmmd} proposed an alternative score which is discussed next.

\subsection{CLIP-Maximum Mean Discrepancy}
Similar to the \gls{FID}, the \gls{CMMD} \cite{cmmd} score measures the discrepancy between the data and the model distribution. It replaces the Inception-v3 embeddings with Contrastive Language- Image Pre-training (\gls{CLIP}) \cite{CLIP} embeddings. Since CLIP is trained on more images and leverages natural langugage supervision, its embeddings have better representation able to capture more semantics details of the images. The features are compared via the maximum mean discrepancy (\gls{MMD}) \cite{MMD1, MMD2}

\begin{equation}
	\begin{split}
		\text{MMD}^2 &= \frac{1}{m(m-1)} \sum_{i=1}^{m} \sum_{j \neq i}^{m} k(\mathbf{x}_{\text{data}, i}, \mathbf{x}_{\text{data}, j}) \\
		&\quad + \frac{1}{n(n-1)} \sum_{i=1}^{n} \sum_{j \neq i}^{n} k(\mathbf{x}_{\text{model}, i}, \mathbf{x}_{\text{model}, j}) \\
		&\quad - \frac{2}{mn} \sum_{i=1}^{m} \sum_{j=1}^{n} k(\mathbf{x}_{\text{data}, i}, \mathbf{x}_{\text{model}, j}) \quad.
	\end{split}
\end{equation}
with $\mathbf{x}_{\text{data}, i} \sim p_{\text{data}}$, $\mathbf{x}_{\text{model}, i} \sim p_{\text{model}}$ and a positive semi-definite Gaussian RBF kernel $k=\exp(- \frac{\| \mathbf{x}_i - \mathbf{x}_j \|^2}{ 2 \sigma^2})$. Through the choice of this kernel no distributional assumption over the features is needed and in contrast to \gls{FID} score it is an unbiased estimator. \cite{cmmd} showed in several ablation studies that the \gls{CMMD} score aligns more closely with human preferences and can detect distortion that \gls{FID} struggles to identify.   

\subsection{Human Preference Score v2 Benchmark}
The \gls{HPSv2} benchmark \cite{HPSv2} is designed to rank generated images based on human preferences. To model the human preference score, a \gls{CLIP} model\cite{CLIP} is finetuned on the HPDv2 dataset \cite{HPSv2} which contains 434,000 images, generated by nine different text-to-image models or taken from the COCO dataset with over 798,000 human preference choices. A human preference choice always includes two images $\mathbf{x}_1$ and $\mathbf{x}_2$ and a binary ranking $\mathbf{y}$ indicating whether image $\mathbf{x}_1$ is preferred over image $\mathbf{x}_2$, e.g. $\mathbf{y}=[1,0]$ means that $\mathbf{x}_1$ is preferred by the annotator. The human preference score is determined by the finetuned \gls{CLIP} model via 
\begin{equation}
	s_\theta (p,\mathbf{x}) = \mathcal{E}_\text{txt}(p) \cdot \mathcal{E}_\text{img}(\mathbf{x}) \cdot 100
	\label{HPSv2Score}
\end{equation}

where $p$ is the text prompt, x is the image, and $\mathcal{E}_\text{img}$ and  $\mathcal{E}_\text{txt}$ are the image and text encoder of the \gls{CLIP} model, respectively. The \gls{HPSv2} benchmark contains four categories, namely ''Animation``, ''Concept-Art``, ''Painting``, and ''Photo`` with 800 prompts each. The prompts are derived on COCO captions \cite{COCO} and DiffusionDB \cite{DiffusionDB} but cleaned and modified by ChatGPT. A higher score computed with eq. \ref{HPSv2Score} correspond to better image quality.

\subsection{GenEval Benchmark}
Unlike global metrics such as \gls{CLIP} and \gls{FID}, which struggle to capture the compositional correctness of images, the GenEval Benchmark \cite{ghosh2023geneval} is designed  to evaluate text-to-image alignment based on an object-focused framework. It leverages existing  models, such as object detection model, semantic model, \gls{CLIP}, to analyze the content of the image and determine whether it follows the prompt closely. The benchmark is divided into six categories of varying difficulty. 
\begin{enumerate}
	\item Single object: A single object is described by the prompt.
	\item Two objects: Two different objects are described by the prompt.
	\item Counting: One object should appear several times.
	\item Colors: An object should have a specific color.
	\item Position: Two or more objects should be placed in a certain order relative to each other.
	\item Attribute binding: Correctly associating attributes (e.g., colors) to specific objects (e.g., a red cube and a blue sphere).
\end{enumerate}
For each category, the prompt follows a specific structure. For the benchmark, 553 different prompts are used and for each prompt four images are generated. The scoring is based on a binary classification determining whether the object, number, and color of objects, etc. are correctly present in the image (see fig. \ref{Geneval}). This returns a ratio representing the proportion of images that contain all described aspects of the prompt.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{images/Background/GenEval.png}  % adjust filename and width
	\caption{Example of how image is investigated for GenEval benchmark  \cite{ghosh2023geneval}.}
	\label{Geneval}
\end{figure}


\subsection{Dense Prompt Graph Benchmark}
The goal of the Dense Prompt Graph (\gls{DPG}) benchmark \cite{DPG} is to evaluate the performance of image generation models on long and detailed prompts. It leverages existing datasets, such as COCO \cite{COCO}, PartiPrompts \cite{yu2022scaling} and DSG-1k \cite{DSG}  and enriches their prompts with GPT-4 \cite{GPT4} by adding details, attributes, and spatial relations between objects. A fourth dataset, Object365 \cite{Objects365} is leveraged which is a dataset originally developed for object detection models. The various objects in this dataset are organized into super-cantegories. Typically, one to four objects from the same super-category are randomly sampled, after which GPT-4 generates a dense prompt based on these objects. In total, 1,065 different prompts were collected. \\
For evaluation, GPT-4 generates binary questions  following the Dynamic Scene Graph (\gls{DSG}) framework \cite{DSG} which organizes the questions in a hierarchical tree where questions in child nodes can only be marked as correct if the questions in their parent nodes are also correct. This strucure ensures that the answers follow the laws of logic. For example, the parent node normally contains an existence question, e.g. "Is there a motorcycle?" while the child nodes contain questions about its attributes, e.g. "Is the motorcycle blue". Therefore, if there is no motorcycle also the question regarding the color is rendered obsolet.
 The vision language model mPLUG-large \cite{mplug} analyzes the generated images with respect to the questions. The evaluation follows a hierarchical aggregation process where the prompt score is defined as the arithmetic mean of all individual question results associated with a specific prompt, while the final DPG-Bench score is calculated by averaging these individual prompt scores across the entire dataset.