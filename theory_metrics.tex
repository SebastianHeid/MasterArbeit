\section{Evaluation Metrics}
For \gls{DM}s, several different metrics were introduced to measure the overall image quality, the distance of the generated distribution with the true distribution or the image prompt alignment. In the follwing, the metrics used in this work are presented: Fr\'echet Inception Distance (\gls{FID}) \cite{FID}, CLIP-Maximum Mean Discrepancy \gls{CMMD} \cite{cmmd}



\subsection{Fr\'echet Inception Distance}
The \gls{FID} \cite{FID} measures the similarity between the model distribution $p_\text{model}$ with the true data distribution $p_\text{data}$. To compare the distributions a set of images is generated by the diffusion model and another set of images from the original data is needed. An inception model, e.g., Inception-v3 \cite{szegedy2016rethinking}, extracts features for every image of the sets. Under the assumption that the features from both sets follow a multivariate normal distribution the mean and covariance of the generated image features ($\boldsymbol{\mu}_\text{model}$, $\Sigma_\text{model}$) and the features from the original data ($\boldsymbol{\mu}_\text{data}$, $\Sigma_\text{data}$) are computed and compared by the Fr\'echet Distance \cite{dowson1982frechet}

\begin{equation}
	\text{FID} = \| \boldsymbol{\mu}_\text{data} - \boldsymbol{\mu}_\text{model}\|_2^2 + \text{Tr}\left(\Sigma_\text{model} + \Sigma_\text{data} - 2 \left(\Sigma_\text{model}\Sigma_\text{data}\right)^{\frac{1}{2}}\right) 
\end{equation}

meaning lower \gls{FID} values indicate a higher degree of similarity between the generated images and the original images. 

Although \gls{FID} is widely used \cite{cmmd} identified several shortcomings. First, the underlying Inception-v3 model is trained only on images from 1,000 classes which leads to embeddings that often fail to capture the rich and varied content of the images generated by current diffusion models. Secondly, the normality assumption for the feature distributions is frequentyl violated in reality which can lead to \gls{FID} scores which do not align to human ratings. A third drawback is that the \gls{FID} score does not capture complex distortion and depends highly sensitive on the sample size and exhibit bias.  For these reasons \cite{cmmd} proposed an alternative score which is discussed next.

\subsection{CLIP-Maximum Mean Discrepancy}
Similar to the \gls{FID}, the \gls{CMMD} score measures the discrepancy between the data and the model distribution. It replaces the Inception-v3 embeddings with Contrastive Language- Image Pre-training (\gls{CLIP}) \cite{CLIP} embeddings. Since CLIP is trained on more images and leverages natural langugage supervision, its embeddings have better representation able to capture more semantics details of the images. The features are compared via the maximum mean discrepancy (\gls{MMD}) \cite{MMD1, MMD2}

\begin{equation}
	\begin{split}
		\text{MMD}^2 &= \frac{1}{m(m-1)} \sum_{i=1}^{m} \sum_{j \neq i}^{m} k(\mathbf{x}_{\text{data}, i}, \mathbf{x}_{\text{data}, j}) \\
		&\quad + \frac{1}{n(n-1)} \sum_{i=1}^{n} \sum_{j \neq i}^{n} k(\mathbf{x}_{\text{model}, i}, \mathbf{x}_{\text{model}, j}) \\
		&\quad - \frac{2}{mn} \sum_{i=1}^{m} \sum_{j=1}^{n} k(\mathbf{x}_{\text{data}, i}, \mathbf{x}_{\text{model}, j}) \quad.
	\end{split}
\end{equation}
with $\mathbf{x}_{\text{data}, i} \sim p_{\text{data}}$, $\mathbf{x}_{\text{model}, i} \sim p_{\text{model}}$ and a positive semi-definite Gaussian RBF kernel $k=\exp(- \frac{\| \mathbf{x}_i - \mathbf{x}_j \|^2}{ 2 \sigma^2})$. Through the choice of this kernel no distributional assumption over the features is needed and in contrast to \gls{FID} score it is an unbiased estimator. \cite{cmmd} showed in several ablation studies that the \gls{CMMD} score aligns more closely with human preferences and can detect distortion that \gls{FID} struggles to identify.   
