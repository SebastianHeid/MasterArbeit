% Large Language Models
@inproceedings{tan2025ominicontrol,
	title={Ominicontrol: Minimal and universal control for diffusion transformer},
	author={Tan, Zhenxiong and Liu, Songhua and Yang, Xingyi and Xue, Qiaochu and Wang, Xinchao},
	booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages={14940--14950},
	year={2025}
}
@article{CFG,
	title={Classifier-free diffusion guidance},
	author={Ho, Jonathan and Salimans, Tim},
	journal={arXiv preprint arXiv:2207.12598},
	year={2022}
}
@article{CG,
	title={Diffusion models beat gans on image synthesis},
	author={Dhariwal, Prafulla and Nichol, Alexander},
	journal={Advances in neural information processing systems},
	volume={34},
	pages={8780--8794},
	year={2021}
}
@article{GPT4,
   author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal},
   title = {Gpt-4 technical report},
   journal = {arXiv preprint arXiv:2303.08774},
   year = {2023},
   type = {Journal Article}
}

@article{Claude,
   author = {Anthropic},
   title = {Claude 3 Haiku: our fastest model yet},
   url = {https://www.anthropic.com/news/claude-3-haiku},
   year = {2024},
   type = {Journal Article}
}

@article{VQVAE,
	author = {Van Den Oord, Aaron and Vinyals, Oriol},
	title = {Neural discrete representation learning},
	journal = {Advances in neural information processing systems},
	volume = {30},
	year = {2017},
	type = {Journal Article}
}

@inproceedings{ControlNet,
	title={Adding conditional control to text-to-image diffusion models},
	author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
	booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
	pages={3836--3847},
	year={2023}
} 
@article{Gemini,
   author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie},
   title = {Gemini: a family of highly capable multimodal models},
   journal = {arXiv preprint arXiv:2312.11805},
   year = {2023},
   type = {Journal Article}
}

@article{Lama,
   author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti},
   title = {Llama 2: Open foundation and fine-tuned chat models},
   journal = {arXiv preprint arXiv:2307.09288},
   year = {2023},
   type = {Journal Article}
}


% Diffusion Models

@inproceedings{SD21,
   author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
   title = {High-resolution image synthesis with latent diffusion models},
   booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
   pages = {10684-10695},
   type = {Conference Proceedings}
}

@article{SDXL,
   author = {Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and Müller, Jonas and Penna, Joe and Rombach, Robin},
   title = {Sdxl: Improving latent diffusion models for high-resolution image synthesis},
   journal = {arXiv preprint arXiv:2307.01952},
   year = {2023},
   type = {Journal Article}
}

@inproceedings{Pixart_s,
   author = {Chen, Junsong and Ge, Chongjian and Xie, Enze and Wu, Yue and Yao, Lewei and Ren, Xiaozhe and Wang, Zhongdao and Luo, Ping and Lu, Huchuan and Li, Zhenguo},
   title = {Pixart-$\sigma$: Weak-to-strong training of diffusion transformer for 4k text-to-image generation},
   booktitle = {European Conference on Computer Vision},
   publisher = {Springer},
   pages = {74-91},
   type = {Conference Proceedings}
}

@article{Flux,
   author = {Labs, Black Forest},
   title = {Flux Model},
   url = {https://bfl.ai},
   year = {2024},
   type = {Journal Article}
}

@article{HiDream,
   author = {AI, Vivago},
   title = {HiDream-l1},
   url = {https://vivago.ai/home},
   year = {2025},
   type = {Journal Article}
}

% Diffusion Model Fundamentals
@inproceedings{DMBeginning,
   author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
   title = {Deep unsupervised learning using nonequilibrium thermodynamics},
   booktitle = {International conference on machine learning},
   publisher = {pmlr},
   pages = {2256-2265},
   type = {Conference Proceedings}
}

% General Generative Models
@misc{VAE,
   author = {Kingma, Diederik P and Welling, Max},
   title = {Auto-encoding variational bayes},
   publisher = {Banff, Canada},
   year = {2013},
   type = {Generic}
}

@article{AE,
   author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
   title = {Reducing the dimensionality of data with neural networks},
   journal = {science},
   volume = {313},
   number = {5786},
   pages = {504-507},
   ISSN = {0036-8075},
   year = {2006},
   type = {Journal Article}
}

@article{KL-Div,
   author = {Kullback, Solomon and Leibler, Richard A},
   title = {On information and sufficiency},
   journal = {The annals of mathematical statistics},
   volume = {22},
   number = {1},
   pages = {79-86},
   ISSN = {0003-4851},
   year = {1951},
   type = {Journal Article}
}

@article{ELBO,
   author = {Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
   title = {Variational inference: A review for statisticians},
   journal = {Journal of the American statistical Association},
   volume = {112},
   number = {518},
   pages = {859-877},
   ISSN = {0162-1459},
   year = {2017},
   type = {Journal Article}
}




@article{GAN,
   author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
   title = {Generative adversarial nets},
   journal = {Advances in neural information processing systems},
   volume = {27},
   year = {2014},
   type = {Journal Article}
}

@inproceedings{WGAN,
   author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
   title = {Wasserstein generative adversarial networks},
   booktitle = {International conference on machine learning},
   publisher = {PMLR},
   pages = {214-223},
   ISBN = {2640-3498},
   type = {Conference Proceedings}
}


@article{ConvGAN,
   author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
   title = {Unsupervised representation learning with deep convolutional generative adversarial networks},
   journal = {arXiv preprint arXiv:1511.06434},
   year = {2015},
   type = {Journal Article}
}

@inproceedings{FastGAN,
   author = {Liu, Bingchen and Zhu, Yizhe and Song, Kunpeng and Elgammal, Ahmed},
   title = {Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis},
   booktitle = {iclr},
   type = {Conference Proceedings}
}

@article{StyleGAN_3,
   author = {Karras, Tero and Aittala, Miika and Laine, Samuli and Härkönen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
   title = {Alias-free generative adversarial networks},
   journal = {Advances in neural information processing systems},
   volume = {34},
   pages = {852-863},
   year = {2021},
   type = {Journal Article}
}

@inproceedings{StyleGAN_T,
   author = {Sauer, Axel and Karras, Tero and Laine, Samuli and Geiger, Andreas and Aila, Timo},
   title = {Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis},
   booktitle = {International conference on machine learning},
   publisher = {PMLR},
   pages = {30105-30118},
   ISBN = {2640-3498},
   type = {Conference Proceedings}
}

@article{ImprovedGANTraining,
   author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
   title = {Improved techniques for training gans},
   journal = {Advances in neural information processing systems},
   volume = {29},
   year = {2016},
   type = {Journal Article}
}

@article{PacGAN,
   author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
   title = {Pacgan: The power of two samples in generative adversarial networks},
   journal = {Advances in neural information processing systems},
   volume = {31},
   year = {2018},
   type = {Journal Article}
}

@inproceedings{ModeCollapse1,
   author = {Thanh-Tung, Hoang and Tran, Truyen},
   title = {Catastrophic forgetting and mode collapse in GANs},
   booktitle = {2020 international joint conference on neural networks (ijcnn)},
   publisher = {IEEE},
   pages = {1-10},
   ISBN = {1728169267},
   type = {Conference Proceedings}
}

@inproceedings{BERT,
   author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
   title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
   booktitle = {Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
   pages = {4171-4186},
   type = {Conference Proceedings}
}

@inproceedings{ResidualConnection,
   author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
   title = {Deep residual learning for image recognition},
   booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
   pages = {770-778},
   type = {Conference Proceedings}
}
@article{OrgAttention,
   author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
   title = {Neural machine translation by jointly learning to align and translate},
   journal = {arXiv preprint arXiv:1409.0473},
   year = {2014},
   type = {Journal Article}
}
@article{SelfAttentionFigure,
   author = {Raschka, Sebastian},
   title = {Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch},
   url = {https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html},
   year = {2023},
   type = {Journal Article}
}
@article{Reformer,
   author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
   title = {Reformer: The efficient transformer},
   journal = {arXiv preprint arXiv:2001.04451},
   year = {2020},
   type = {Journal Article}
}
@article{SparseTransformer,
   author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
   title = {Generating long sequences with sparse transformers},
   journal = {arXiv preprint arXiv:1904.10509},
   year = {2019},
   type = {Journal Article}
}
@article{FlashAttention,
   author = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
   title = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
   journal = {Advances in neural information processing systems},
   volume = {35},
   pages = {16344-16359},
   year = {2022},
   type = {Journal Article}
}




@article{Performer,
   author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz},
   title = {Rethinking attention with performers},
   journal = {arXiv preprint arXiv:2009.14794},
   year = {2020},
   type = {Journal Article}
}




@article{LayerNorm,
   author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
   title = {Layer normalization},
   journal = {arXiv preprint arXiv:1607.06450},
   year = {2016},
   type = {Journal Article}
}





@inproceedings{NormalizingFlow,
   author = {Rezende, Danilo and Mohamed, Shakir},
   title = {Variational inference with normalizing flows},
   booktitle = {International conference on machine learning},
   publisher = {PMLR},
   pages = {1530-1538},
   type = {Conference Proceedings}
}

@article{AffineCouplingLayer,
   author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
   title = {Nice: Non-linear independent components estimation},
   journal = {arXiv preprint arXiv:1410.8516},
   year = {2014},
   type = {Journal Article}
}


@article{AttentionIsAllYouNeed,
   author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
   title = {Attention is all you need},
   journal = {Advances in neural information processing systems},
   volume = {30},
   year = {2017},
   type = {Journal Article}
}

@article{RNN,
   author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
   title = {Learning representations by back-propagating errors},
   journal = {nature},
   volume = {323},
   number = {6088},
   pages = {533-536},
   ISSN = {0028-0836},
   year = {1986},
   type = {Journal Article}
}


@inproceedings{ALiBi,
   author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
   title = {Generative agents: Interactive simulacra of human behavior},
   booktitle = {Proceedings of the 36th annual acm symposium on user interface software and technology},
   pages = {1-22},
   type = {Conference Proceedings}
}

@article{Cape,
   author = {Likhomanenko, Tatiana and Xu, Qiantong and Synnaeve, Gabriel and Collobert, Ronan and Rogozhnikov, Alex},
   title = {Cape: Encoding relative positions with continuous augmented positional embeddings},
   journal = {Advances in Neural Information Processing Systems},
   volume = {34},
   pages = {16079-16092},
   year = {2021},
   type = {Journal Article}
}

@article{FIRE,
   author = {Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
   title = {Functional interpolation for relative positions improves long context transformers},
   journal = {arXiv preprint arXiv:2310.04418},
   year = {2023},
   type = {Journal Article}
}

@inproceedings{Floater,
   author = {Liu, Xuanqing and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui},
   title = {Learning to encode position for transformer with continuous dynamical model},
   booktitle = {International conference on machine learning},
   publisher = {PMLR},
   pages = {6327-6335},
   ISBN = {2640-3498},
   type = {Conference Proceedings}
}

@article{RoPE,
   author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
   title = {Roformer: Enhanced transformer with rotary position embedding},
   journal = {Neurocomputing},
   volume = {568},
   pages = {127063},
   ISSN = {0925-2312},
   year = {2024},
   type = {Journal Article}
}

@article{Shape,
   author = {Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun and Inui, Kentaro},
   title = {Shape: Shifted absolute position embedding for transformers},
   journal = {arXiv preprint arXiv:2109.05644},
   year = {2021},
   type = {Journal Article}
}

@article{PositionalEmbeddingSurvey,
   author = {Zhao, Liang and Feng, Xiachong and Feng, Xiaocheng and Zhong, Weihong and Xu, Dongliang and Yang, Qing and Liu, Hongtao and Qin, Bing and Liu, Ting},
   title = {Length extrapolation of transformers: A survey from the perspective of positional encoding},
   journal = {arXiv preprint arXiv:2312.17044},
   year = {2023},
   type = {Journal Article}
}

@article{VisionTransformer,
   author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
   title = {An image is worth 16x16 words: Transformers for image recognition at scale},
   journal = {arXiv preprint arXiv:2010.11929},
   year = {2020},
   type = {Journal Article}
}

@inproceedings{Ccnet,
   author = {Huang, Zilong and Wang, Xinggang and Huang, Lichao and Huang, Chang and Wei, Yunchao and Liu, Wenyu},
   title = {Ccnet: Criss-cross attention for semantic segmentation},
   booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
   pages = {603-612},
   type = {Conference Proceedings}
}

@inproceedings{ImageTransformer,
   author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
   title = {Image transformer},
   booktitle = {International conference on machine learning},
   publisher = {PMLR},
   pages = {4055-4064},
   ISBN = {2640-3498},
   type = {Conference Proceedings}
}

@article{StandAloneAttentionVisonModels,
   author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
   title = {Stand-alone self-attention in vision models},
   journal = {Advances in neural information processing systems},
   volume = {32},
   year = {2019},
   type = {Journal Article}
}

@article{CNN,
   author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
   title = {Stand-alone self-attention in vision models},
   journal = {Advances in neural information processing systems},
   volume = {32},
   year = {2019},
   type = {Journal Article}
}

@inproceedings{ADD,
   author = {Sauer, Axel and Lorenz, Dominik and Blattmann, Andreas and Rombach, Robin},
   title = {Adversarial diffusion distillation},
   booktitle = {European Conference on Computer Vision},
   publisher = {Springer},
   pages = {87-103},
   type = {Conference Proceedings}
}

@article{ConsistencyModel,
   author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
   title = {Consistency models},
   year = {2023},
   type = {Journal Article}
}

@inproceedings{DistillationMatching,
   author = {Yin, Tianwei and Gharbi, Michaël and Zhang, Richard and Shechtman, Eli and Durand, Fredo and Freeman, William T and Park, Taesung},
   title = {One-step diffusion with distribution matching distillation},
   booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
   pages = {6613-6623},
   type = {Conference Proceedings}
}

@inproceedings{DistillDifModelCGAN,
   author = {Kang, Minguk and Zhang, Richard and Barnes, Connelly and Paris, Sylvain and Kwak, Suha and Park, Jaesik and Shechtman, Eli and Zhu, Jun-Yan and Park, Taesung},
   title = {Distilling diffusion models into conditional gans},
   booktitle = {European Conference on Computer Vision},
   publisher = {Springer},
   pages = {428-447},
   type = {Conference Proceedings}
}

@inproceedings{GuidedDistillation,
   author = {Meng, Chenlin and Rombach, Robin and Gao, Ruiqi and Kingma, Diederik and Ermon, Stefano and Ho, Jonathan and Salimans, Tim},
   title = {On distillation of guided diffusion models},
   booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
   pages = {14297-14306},
   type = {Conference Proceedings}
}

@article{ImprovedDistillationMatching,
   author = {Yin, Tianwei and Gharbi, Michaël and Park, Taesung and Zhang, Richard and Shechtman, Eli and Durand, Fredo and Freeman, Bill},
   title = {Improved distribution matching distillation for fast image synthesis},
   journal = {Advances in neural information processing systems},
   volume = {37},
   pages = {47455-47487},
   year = {2024},
   type = {Journal Article}
}

@inproceedings{LADD,
   author = {Sauer, Axel and Boesel, Frederic and Dockhorn, Tim and Blattmann, Andreas and Esser, Patrick and Rombach, Robin},
   title = {Fast high-resolution image synthesis with latent adversarial diffusion distillation},
   booktitle = {SIGGRAPH Asia 2024 Conference Papers},
   pages = {1-11},
   type = {Conference Proceedings}
}

@article{LatentConsistencyModel,
   author = {Luo, Simian and Tan, Yiqin and Huang, Longbo and Li, Jian and Zhao, Hang},
   title = {Latent consistency models: Synthesizing high-resolution images with few-step inference},
   journal = {arXiv preprint arXiv:2310.04378},
   year = {2023},
   type = {Journal Article}
}

@article{SDXL_Lightning,
   author = {Lin, Shanchuan and Wang, Anran and Yang, Xiao},
   title = {Sdxl-lightning: Progressive adversarial diffusion distillation},
   journal = {arXiv preprint arXiv:2402.13929},
   year = {2024},
   type = {Journal Article}
}

@article{ProgressiveDistillation,
   author = {Salimans, Tim and Ho, Jonathan},
   title = {Progressive distillation for fast sampling of diffusion models},
   journal = {arXiv preprint arXiv:2202.00512},
   year = {2022},
   type = {Journal Article}
}


@inproceedings{Bksdm,
   author = {Kim, Bo-Kyeong and Song, Hyoung-Kyu and Castells, Thibault and Choi, Shinkook},
   title = {Bk-sdm: A lightweight, fast, and cheap version of stable diffusion},
   booktitle = {European Conference on Computer Vision},
   publisher = {Springer},
   pages = {381-399},
   type = {Conference Proceedings}
}

@article{Koala,
   author = {Lee, Youngwan and Park, Kwanyong and Cho, Yoorhim and Lee, Yong-Ju and Hwang, Sung Ju},
   title = {Koala: Empirical lessons toward memory-efficient and fast diffusion models for text-to-image synthesis},
   journal = {Advances in Neural Information Processing Systems},
   volume = {37},
   pages = {51597-51633},
   year = {2024},
   type = {Journal Article}
}
@article{Laptop,
   author = {Zhang, Dingkun and Li, Sijia and Chen, Chen and Xie, Qingsong and Lu, Haonan},
   title = {Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models},
   journal = {arXiv preprint arXiv:2404.11098},
   year = {2024},
   type = {Journal Article}
}


@article{DinoV2,
   author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin},
   title = {Dinov2: Learning robust visual features without supervision},
   journal = {arXiv preprint arXiv:2304.07193},
   year = {2023},
   type = {Journal Article}
}


@article{ScoreBasedDiffModel,
   author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
   title = {Score-based generative modeling through stochastic differential equations},
   journal = {arXiv preprint arXiv:2011.13456},
   year = {2020},
   type = {Journal Article}
}

@inproceedings{LPIPS,
   author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
   title = {The unreasonable effectiveness of deep features as a perceptual metric},
   booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
   pages = {586-595},
   type = {Conference Proceedings}
}

@article{KnowledgeDistillation,
   author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
   title = {Distilling the knowledge in a neural network},
   journal = {arXiv preprint arXiv:1503.02531},
   year = {2015},
   type = {Journal Article}
}

@inproceedings{Unet,
   author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
   title = {U-net: Convolutional networks for biomedical image segmentation},
   booktitle = {Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
   publisher = {Springer},
   pages = {234-241},
   ISBN = {3319245732},
   type = {Conference Proceedings}
}



@inproceedings{DiffusionModelIntroPaper,
   author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
   title = {Deep unsupervised learning using nonequilibrium thermodynamics},
   booktitle = {International conference on machine learning},
   publisher = {pmlr},
   pages = {2256-2265},
   type = {Conference Proceedings}
}


@misc{DDPM,
   author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
   title = {Denoising Diffusion Probabilistic Models},
   pages = {arXiv:2006.11239},
   month = {June 01, 2020},
   abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
   keywords = {Computer Science - Machine Learning
Statistics - Machine Learning},
   DOI = {10.48550/arXiv.2006.11239},
   url = {https://ui.adsabs.harvard.edu/abs/2020arXiv200611239H},
   year = {2020},
   type = {Electronic Article}
}


@misc{DDIM,
   author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
   title = {Denoising Diffusion Implicit Models},
   pages = {arXiv:2010.02502},
   month = {October 01, 2020},
   note = {ICLR 2021; updated connections with ODEs at page 6, fixed some typos in the proof},
   abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \times$ to $50 \times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
   keywords = {Computer Science - Machine Learning
Computer Science - Computer
Vision and Pattern Recognition},
   DOI = {10.48550/arXiv.2010.02502},
   url = {https://ui.adsabs.harvard.edu/abs/2020arXiv201002502S},
   year = {2020},
   type = {Electronic Article}
}

@misc{LDM,
   author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
   title = {High-Resolution Image Synthesis with Latent Diffusion Models},
   pages = {arXiv:2112.10752},
   month = {December 01, 2021},
   note = {CVPR 2022},
   abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
   keywords = {Computer Science - Computer Vision and Pattern Recognition},
   DOI = {10.48550/arXiv.2112.10752},
   url = {https://ui.adsabs.harvard.edu/abs/2021arXiv211210752R},
   year = {2021},
   type = {Electronic Article}
}

@article{SDE,
   author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
   title = {Score-based generative modeling through stochastic differential equations},
   journal = {arXiv preprint arXiv:2011.13456},
   year = {2020},
   type = {Journal Article}
}

@article{SGM,
   author = {Song, Yang and Ermon, Stefano},
   title = {Generative modeling by estimating gradients of the data distribution},
   journal = {Advances in neural information processing systems},
   volume = {32},
   year = {2019},
   type = {Journal Article}
}


@article{SurveyDM,
   author = {Croitoru, Florinel-Alin and Hondru, Vlad and Ionescu, Radu Tudor and Shah, Mubarak},
   title = {Diffusion models in vision: A survey},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   volume = {45},
   number = {9},
   pages = {10850-10869},
   ISSN = {0162-8828},
   year = {2023},
   type = {Journal Article}
}

@article{RectifiedFlowMatching,
   author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
   title = {Flow straight and fast: Learning to generate and transfer data with rectified flow},
   journal = {arXiv preprint arXiv:2209.03003},
   year = {2022},
   type = {Journal Article}
}


@article{FlowMatching,
   author = {Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
   title = {Flow matching for generative modeling},
   journal = {arXiv preprint arXiv:2210.02747},
   year = {2022},
   type = {Journal Article}
}

@inproceedings{ImprovedDDPM,
   author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
   title = {Improved denoising diffusion probabilistic models},
   booktitle = {International conference on machine learning},
   publisher = {PMLR},
   pages = {8162-8171},
   ISBN = {2640-3498},
   type = {Conference Proceedings}
}

@article{VDM,
   author = {Kingma, Diederik and Salimans, Tim and Poole, Ben and Ho, Jonathan},
   title = {Variational diffusion models},
   journal = {Advances in neural information processing systems},
   volume = {34},
   pages = {21696-21707},
   year = {2021},
   type = {Journal Article}
}


@inproceedings{feller1949stochastic,
  author    = {Feller, William},
  title     = {On the Theory of Stochastic Processes, with Particular Reference to Applications},
  booktitle = {Proceedings of the First Berkeley Symposium on Mathematical Statistics and Probability},
  year      = {1949},
  publisher = {The Regents of the University of California},
  address   = {Berkeley, CA},
  pages     = {403--432}
}

@misc{pawlowski2024physics,
  author       = {Jan Pawlowski and Tilman Plehn},
  title        = {Physics and Machine Learning},
  howpublished = {Lecture script, Institut für Theoretische Physik, Universität Heidelberg},
  year         = {2024},
  month        = {January},
  note         = {Version from January 30, 2024}
}


@article{DDPMSamplerImage,
   author = {Butter, Anja and Huetsch, Nathan and Palacios Schweitzer, Sofia and Plehn, Tilman and Sorrenson, Peter and Spinner, Jonas},
   title = {Jet diffusion versus JetGPT–modern networks for the LHC},
   journal = {SciPost Physics Core},
   volume = {8},
   number = {1},
   pages = {026},
   ISSN = {2666-9366},
   year = {2025},
   type = {Journal Article}
}

@article{LectureNotesDDPM,
   author = {Strümke, Inga and Langseth, Helge},
   title = {Lecture Notes in Probabilistic Diffusion Models},
   journal = {arXiv preprint arXiv:2312.10393},
   year = {2023},
   type = {Journal Article}
}

@article{IntroSGM,
   author = {Song, Yang and Ermon, Stefano},
   title = {Generative modeling by estimating gradients of the data distribution},
   journal = {Advances in neural information processing systems},
   volume = {32},
   year = {2019},
   type = {Journal Article}
}

@article{ImprovedSGMs,
   author = {Song, Yang and Ermon, Stefano},
   title = {Improved techniques for training score-based generative models},
   journal = {Advances in neural information processing systems},
   volume = {33},
   pages = {12438-12448},
   year = {2020},
   type = {Journal Article}
}


@article{Survey2,
   author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
   title = {Diffusion models: A comprehensive survey of methods and applications},
   journal = {ACM Computing Surveys},
   volume = {56},
   number = {4},
   pages = {1-39},
   ISSN = {0360-0300},
   year = {2023},
   type = {Journal Article}
}


@article{ScoreBasedODE,
   author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
   title = {Score-based generative modeling through stochastic differential equations},
   journal = {arXiv preprint arXiv:2011.13456},
   year = {2020},
   type = {Journal Article}
}

@inproceedings{SlicedScoreMatching,
   author = {Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
   title = {Sliced score matching: A scalable approach to density and score estimation},
   booktitle = {Uncertainty in artificial intelligence},
   publisher = {PMLR},
   pages = {574-584},
   ISBN = {2640-3498},
   type = {Conference Proceedings}
}

@inproceedings{SteinDiscrepancy,
   author = {Gorham, Jackson and Mackey, Lester},
   title = {Measuring sample quality with kernels},
   booktitle = {International Conference on Machine Learning},
   publisher = {PMLR},
   pages = {1292-1301},
   ISBN = {2640-3498},
   type = {Conference Proceedings}
}

@article{Sana,
	author = {Xie, Enze and Chen, Junsong and Chen, Junyu and Cai, Han and Tang, Haotian and Lin, Yujun and Zhang, Zhekai and Li, Muyang and Zhu, Ligeng and Lu, Yao},
	title = {Sana: Efficient high-resolution image synthesis with linear diffusion transformers},
	journal = {arXiv preprint arXiv:2410.10629},
	year = {2024},
	type = {Journal Article}
}



@article{Anderson,
   author = {Anderson, Brian DO},
   title = {Reverse-time diffusion equation models},
   journal = {Stochastic Processes and their Applications},
   volume = {12},
   number = {3},
   pages = {313-326},
   ISSN = {0304-4149},
   year = {1982},
   type = {Journal Article}
}

@article{ODESolver1,
   author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
   title = {Elucidating the design space of diffusion-based generative models},
   journal = {Advances in neural information processing systems},
   volume = {35},
   pages = {26565-26577},
   year = {2022},
   type = {Journal Article}
}

@article{ODESolver2,
   author = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
   title = {Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps},
   journal = {Advances in Neural Information Processing Systems},
   volume = {35},
   pages = {5775-5787},
   year = {2022},
   type = {Journal Article}
}
@article{ODESolver3,
   author = {Zhang, Qinsheng and Chen, Yongxin},
   title = {Fast sampling of diffusion models with exponential integrator},
   journal = {arXiv preprint arXiv:2204.13902},
   year = {2022},
   type = {Journal Article}
}

@article{FastSDE,
   author = {Jolicoeur-Martineau, Alexia and Li, Ke and Piché-Taillefer, Rémi and Kachman, Tal and Mitliagkas, Ioannis},
   title = {Gotta go fast when generating data with score-based models},
   journal = {arXiv preprint arXiv:2105.14080},
   year = {2021},
   type = {Journal Article}
}


@article{FlowMatching,
   author = {Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
   title = {Flow matching for generative modeling},
   journal = {arXiv preprint arXiv:2210.02747},
   year = {2022},
   type = {Journal Article}
}


@article{MCMC,
   author = {Tierney, Luke},
   title = {Markov chains for exploring posterior distributions},
   journal = {the Annals of Statistics},
   pages = {1701-1728},
   ISSN = {0090-5364},
   year = {1994},
   type = {Journal Article}
}

@article{VariationalInference,
	author = {Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
	title = {Variational inference: A review for statisticians},
	journal = {Journal of the American statistical Association},
	volume = {112},
	number = {518},
	pages = {859-877},
	ISSN = {0162-1459},
	year = {2017},
	type = {Journal Article}
}


