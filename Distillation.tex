\section{Model Distillation}
Diffusion models excel in generating high-quality and diverse images, showcasing remarkable capabilities in image synthesis. However, their main disadvantage is their prolonged inference time which stems primarly from two factors. First, diffusion models rely on an iterative generation process, removing noise step-by-step. To obtain high-quality images often 50-100 steps are required. Second, recent diffusion models like Flux-dev \cite{Flux} are remarkably large, with billions of parameters, making each step computationally and memory-intensive. These issues limit their use in real-time applications or in memory-constraint environments, such as edge devices.

Diffusion model distillation addresses these challenges. The principal idea of most distillation methods is to transfer the knowledge and image generation capacity from a fully trained diffusion model, often referred to as teacher model, to a target model, often referred to as student model. The research landscape reveals two complementary strategies. First, accelerating inference by reducing the number of denoising steps, and second, creating more compact models through model size reduction. The first strategy, reducing the number of inference steps, is predominant in the literature and several different approaches such as adversarial distillation \cite{ADD, LADD}, progressive distillation \cite{ProgressiveDistillation, SDXL_Lightning}, distribution matching distillation \cite{DistillationMatching, ImprovedDistillationMatching}, consistency models \cite{ConsistencyModel, LatentConsistencyModel} and guided distillation \cite{GuidedDistillation} were proposed. The second strategy, focusing on structural-based knowledge distillation \cite{KnowledgeDistillation} is very suitable for reducing the model complexity and parameter count. \\
All these distillation methods have in common that there is an efficiency-quality trade-off. The smaller the student model gets and the less inference steps are used the lower is the image quality. 

This thesis focuses on reducing the model size of the models of Pixart-$\Sigma$ and Flux-dev using the concept of knowledge distillation which is presented in the following section in detail.
\subsection{Knowledge Distillation}
\label{sec:BackgroundKnowledgeDistillation}
The concept of knowledge distillation \cite{KnowledgeDistillation} was originally introduced for training a small model, referred to as the student, on classification tasks such as MNIST \cite{MNIST}. In this framework, a larger model with strong generalization capabilities, referred to as the teacher, serves as guidance. It was shown, that computing the loss between the probability outputs of the teacher and the student enables much more efficient training than relying solely on the original class labels. In such a setting, the small model can be trained on less data or with a higher learning rate. As a possible explanation, the authors hypothesized that even the very small probabilities the teacher assigns to incorrect classes, and particularly their relative magnitudes, carry valuable information for the student. For example, if a "2" from the MNIST dataset is to be classified, it is valuable information to know whether it resembles a "3" or a "5". When only hard targets, e.g., one-hot labels,  are used, this relational information is lost. To make this information accessible to the student, one must deviate from the standard temperature $T=1$ of the softmax function to soften the probabilities. Specifically, both teacher and student use temperature-scaling softmax($\frac{z_i}{T}$) where $T > 1$ making the probability distribution flatter and revealing more information about class similarities. In general, it has been shown that a combination of the distillation loss $\mathcal{L_\text{KD}}$, calculated using the outputs from the teacher, and the task loss $\mathcal{L_\text{Task}}$, calculated with the hard labels, is advantageous
\begin{equation}
	\mathcal{L} = \mathcal{L}_\text{Task} + \lambda_\text{KD}\mathcal{L}_\text{KD} \quad.
\end{equation}
This idea has been adapted to train smaller versions of diffusion \cite{Bksdm,Laptop, Koala}  and flow \cite{FastFlux,TinyFusion, ma2025pluggable,Dense2MoE,flux1-lite,flux_mini_2024} models. Here, the original model with its pretrained weights acts as the teacher, while the student model is obtained by removing individual parts of the teacher's architecture. Often the student is initialized with the weights of the teacher. The general idea in these approaches is to use the intermediate features as well as the final output as learning signal from the teacher to the student via \gls{MSE} loss. The mapping between teacher and student layers is established by selecting a subset of the teacher's blocks that corresponds to the reduced depth of the student architecture. If the architectures and therefore the dimensions of the intermediate features differ between teacher and student model, linear projection matrices $\phi_i$ are used to project the student's features to the same dimension as the teacher's features. Furthermore, due to different magnitudes in different layers, a weighting factor $w_i$ for each layer $i$ is applied such that the contribution of the different layers are in the same range.
\begin{equation}
	\mathcal{L}_\text{FeatKD} = \sum_{i \in \text{Layers}} w_i || F_i^T - \phi_i(F_i^S) ||^2 \quad.
\end{equation}

  Additionally, the original diffusion loss $\mathcal{L_\text{Task}}$ is taken into account too such that the final loss is given by 
\begin{equation}
	\label{eq:KnowledgeDistillationLoss}
	\mathcal{L} = \mathcal{L_\text{Task}} + \lambda_\text{OutKD} \mathcal{L_\text{OutKD}} + \lambda_\text{FeatKD} \mathcal{L_\text{FeatKD}}
\end{equation}
where $\mathcal{L_\text{Task}}$ is the standard diffusion loss, $\mathcal{L_\text{FeatKD}}$ is the loss computed between the intermediate features and $\mathcal{L_\text{OutKD}}$ is the loss computed between the final predictions of the student and the teacher model.