\section{Model Distillation}

\subsection{Knowledge Distillation}
The concept of knowledge distillation was introduced for training a small model referred to as the student - on classification tasks, using a larger model - referred to as the teacher with strong generalization capabilities as guidance. It was shown, that computing the loss between the probability outputs of the teacher and the student enables much more efficient training than using the original class labels. In such a setting the small model could be trained on less data and with a higher learning rate. The authors hypothesized that even the very small probabilities the teacher assigns to incorrect classes, and particularly their relative magnitudes, carry valuable information for the student For example if a 2 of the MNIST dataset \cite{} should be classified it is valuable information for the model whether it resembles a 3 or a 5. When only hard targets, e.g., one-hot labels,  are used, this relational information is lost. Here, it is important to make this information accessible to the student meaning that e.g. in a classification problem one might have to deviate from the standard temperature $T$ of the softmax function to soften the probabilities even more. Specifically, both teacher and student use temperature scaling softmax($\frac{z_i}{T}$) where $T > 1$ makes the probability distribution softer, revealing more information about class similarities. In general, it has been shown that a combination of the loss $\mathcal{L_\text{KD}}$, calculated with the outputs from the teacher, and the original loss $\mathcal{L_\text{Task}}$, calculated with the hard labels, is advantageous
\begin{equation}
	\mathcal{L} = \mathcal{L}_\text{Task} + \lambda_\text{KD}\mathcal{L}_\text{KD}
\end{equation}
This idea was used to train a smaller version of Stable Diffusion \cite{BERT} or Stable Diffusion XL \cite{Laptop, Koala}. Hereby, the original model with its pretrained weights is the teacher and the student model was obtained by removing individual parts of the Unet \cite{Unet} architecture. The general idea in all approaches is to use the intermediate features from the Unet architecture as well as the final output as learning signal from the teacher to the student. Additionally, the original diffusion loss $\mathcal{L_\text{Task}}$ taken into account too such that the final loss is given by 
\begin{equation}
	\mathcal{L} = \mathcal{L_\text{Task}} + \lambda_\text{OutKD} \mathcal{L_\text{OutKD}} + \lambda_\text{FeatKD} \mathcal{L_\text{FeatKD}}
\end{equation}
where $\mathcal{L_\text{Task}}$ is the loss used for regular training of diffusion models, $\mathcal{L_\text{FeatKD}}$ is the loss computed between the intermediate features and $\mathcal{L_\text{OutKD}}$ is the loss computed between the final output of the student and the teacher model.