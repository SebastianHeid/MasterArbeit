\begin{thebibliography}{100}

\bibitem{GPT4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
  Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, and
  Shyamal Anadkat.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{HiDream}
Vivago AI.
\newblock Hidream-l1.
\newblock 2025.
\newblock URL: \url{https://vivago.ai/home}.

\bibitem{akiba2019optuna}
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama.
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock In {\em Proceedings of the 25th ACM SIGKDD international conference
  on knowledge discovery \& data mining}, pages 2623--2631, 2019.

\bibitem{Anderson}
Brian~DO Anderson.
\newblock Reverse-time diffusion equation models.
\newblock {\em Stochastic Processes and their Applications}, 12(3):313--326,
  1982.

\bibitem{Claude}
Anthropic.
\newblock Claude 3 haiku: our fastest model yet.
\newblock 2024.
\newblock URL: \url{https://www.anthropic.com/news/claude-3-haiku}.

\bibitem{WGAN}
Martin Arjovsky, Soumith Chintala, and Léon Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International conference on machine learning}, pages
  214--223. PMLR.

\bibitem{LayerNorm}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{OrgAttention}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{bergstra2011algorithms}
James Bergstra, R{\'e}mi Bardenet, Yoshua Bengio, and Bal{\'a}zs K{\'e}gl.
\newblock Algorithms for hyper-parameter optimization.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{ELBO}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American statistical Association},
  112(518):859--877, 2017.

\bibitem{VariationalInference}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American statistical Association},
  112(518):859--877, 2017.

\bibitem{DDPMSamplerImage}
Anja Butter, Nathan Huetsch, Sofia Palacios~Schweitzer, Tilman Plehn, Peter
  Sorrenson, and Jonas Spinner.
\newblock Jet diffusion versus jetgpt–modern networks for the lhc.
\newblock {\em SciPost Physics Core}, 8(1):026, 2025.

\bibitem{FastFlux}
Fuhan Cai, Yong Guo, Jie Li, Wenbo Li, Xiangzhong Fang, and Jian Chen.
\newblock Fastflux: Pruning flux with block-wise replacement and sandwich
  training.
\newblock {\em arXiv preprint arXiv:2506.10035}, 2025.

\bibitem{Pixart_s}
Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao
  Wang, Ping Luo, Huchuan Lu, and Zhenguo Li.
\newblock Pixart-$\sigma$: Weak-to-strong training of diffusion transformer for
  4k text-to-image generation.
\newblock In {\em European Conference on Computer Vision}, pages 74--91.
  Springer.

\bibitem{Pixart_d}
Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao,
  and Zhenguo Li.
\newblock Pixart-$\delta$: Fast and controllable image generation with latent
  consistency models.
\newblock {\em arXiv preprint arXiv:2401.05252}, 2024.

\bibitem{Pixart_a}
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao
  Wang, James Kwok, Ping Luo, Huchuan Lu, et~al.
\newblock Pixart-$\alpha$: Fast training of diffusion transformer for
  photorealistic text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2310.00426}, 2023.

\bibitem{chen2024sharegpt4v}
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng
  Zhao, and Dahua Lin.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock In {\em European Conference on Computer Vision}, pages 370--387.
  Springer, 2024.

\bibitem{CNF}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{COCO}
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
  Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco captions: Data collection and evaluation server.
\newblock {\em arXiv preprint arXiv:1504.00325}, 2015.

\bibitem{SparseTransformer}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{DSG}
Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason
  Baldridge, Mohit Bansal, Jordi Pont-Tuset, and Su~Wang.
\newblock Davidsonian scene graph: Improving reliability in fine-grained
  evaluation for text-to-image generation.
\newblock {\em arXiv preprint arXiv:2310.18235}, 2023.

\bibitem{Performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, and
  Lukasz Kaiser.
\newblock Rethinking attention with performers.
\newblock {\em arXiv preprint arXiv:2009.14794}, 2020.

\bibitem{cordts2016cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
  Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3213--3223, 2016.

\bibitem{SurveyDM}
Florinel-Alin Croitoru, Vlad Hondru, Radu~Tudor Ionescu, and Mubarak Shah.
\newblock Diffusion models in vision: A survey.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  45(9):10850--10869, 2023.

\bibitem{flux1-lite}
Javier~Martín Daniel~Verdú.
\newblock Flux.1 lite: Distilling flux1.dev for efficient text-to-image
  generation.
\newblock 2024.

\bibitem{FlashAttention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock {\em Advances in neural information processing systems},
  35:16344--16359, 2022.

\bibitem{davari2022reliability}
MohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, and
  Eugene Belilovsky.
\newblock Reliability of cka as a similarity measure in deep learning.
\newblock {\em arXiv preprint arXiv:2210.16156}, 2022.

\bibitem{ImageNet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{BERT}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 conference of the North American
  chapter of the association for computational linguistics: human language
  technologies, volume 1 (long and short papers)}, pages 4171--4186.

\bibitem{CG}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in neural information processing systems},
  34:8780--8794, 2021.

\bibitem{AffineCouplingLayer}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock {\em arXiv preprint arXiv:1410.8516}, 2014.

\bibitem{VisionTransformer}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, and Sylvain Gelly.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{dowson1982frechet}
DC~Dowson and BV666017 Landau.
\newblock The fr{\'e}chet distance between multivariate normal distributions.
\newblock {\em Journal of multivariate analysis}, 12(3):450--455, 1982.

\bibitem{SD3}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas
  M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic
  Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image
  synthesis.
\newblock In {\em Forty-first international conference on machine learning},
  2024.

\bibitem{TinyFusion}
Gongfan Fang, Kunjun Li, Xinyin Ma, and Xinchao Wang.
\newblock Tinyfusion: Diffusion transformers learned shallow.
\newblock In {\em Proceedings of the Computer Vision and Pattern Recognition
  Conference}, pages 18144--18154, 2025.

\bibitem{fang2025tinyfusion}
Gongfan Fang, Kunjun Li, Xinyin Ma, and Xinchao Wang.
\newblock Tinyfusion: Diffusion transformers learned shallow.
\newblock In {\em Proceedings of the Computer Vision and Pattern Recognition
  Conference}, pages 18144--18154, 2025.

\bibitem{feller1949stochastic}
William Feller.
\newblock On the theory of stochastic processes, with particular reference to
  applications.
\newblock In {\em Proceedings of the First Berkeley Symposium on Mathematical
  Statistics and Probability}, pages 403--432, Berkeley, CA, 1949. The Regents
  of the University of California.

\bibitem{filters2016pruning}
Determine Filters’Importance.
\newblock Pruning filters for efficient convnets.
\newblock {\em arXiv preprint arXiv:1608.08710}, 2016.

\bibitem{fpgaminer2024joycaption}
fpgaminer.
\newblock {JoyCaption}: An open, uncensored image captioning {VLM}.
\newblock \url{https://github.com/fpgaminer/joycaption}, 2024.
\newblock Accessed: 2026-01-22.

\bibitem{ghosh2023geneval}
Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.
\newblock Geneval: An object-focused framework for evaluating text-to-image
  alignment.
\newblock {\em Advances in Neural Information Processing Systems},
  36:52132--52152, 2023.

\bibitem{GAN}
Ian~J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{SteinDiscrepancy}
Jackson Gorham and Lester Mackey.
\newblock Measuring sample quality with kernels.
\newblock In {\em International Conference on Machine Learning}, pages
  1292--1301. PMLR.

\bibitem{MMD1}
Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Sch{\"o}lkopf, and
  Alex Smola.
\newblock A kernel method for the two-sample-problem.
\newblock {\em Advances in neural information processing systems}, 19, 2006.

\bibitem{MMD2}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and
  Alexander Smola.
\newblock A kernel two-sample test.
\newblock {\em The journal of machine learning research}, 13(1):723--773, 2012.

\bibitem{gretton2005measuring}
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch{\"o}lkopf.
\newblock Measuring statistical dependence with hilbert-schmidt norms.
\newblock In {\em International conference on algorithmic learning theory},
  pages 63--77. Springer, 2005.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{ResidualConnection}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem{FID}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{KnowledgeDistillation}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{AE}
Geoffrey~E Hinton and Ruslan~R Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock {\em science}, 313(5786):504--507, 2006.

\bibitem{DDPM}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models, June 01, 2020 2020.
\newblock URL: \url{https://ui.adsabs.harvard.edu/abs/2020arXiv200611239H},
  \href {https://doi.org/10.48550/arXiv.2006.11239}
  {\path{doi:10.48550/arXiv.2006.11239}}.

\bibitem{CFG}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock {\em arXiv preprint arXiv:2207.12598}, 2022.

\bibitem{hu2022lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, Weizhu Chen, et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em ICLR}, 1(2):3, 2022.

\bibitem{DPG}
Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu.
\newblock Ella: Equip diffusion models with llm for enhanced semantic
  alignment.
\newblock {\em arXiv preprint arXiv:2403.05135}, 2024.

\bibitem{Ccnet}
Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu
  Liu.
\newblock Ccnet: Criss-cross attention for semantic segmentation.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 603--612.

\bibitem{BatchNorm}
Sergey Ioffe.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock {\em arXiv preprint arXiv:1611.01144}, 2016.

\bibitem{cmmd}
Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan
  Chakrabarti, and Sanjiv Kumar.
\newblock Rethinking fid: Towards a better evaluation metric for image
  generation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9307--9315, 2024.

\bibitem{FastSDE}
Alexia Jolicoeur-Martineau, Ke~Li, Rémi Piché-Taillefer, Tal Kachman, and
  Ioannis Mitliagkas.
\newblock Gotta go fast when generating data with score-based models.
\newblock {\em arXiv preprint arXiv:2105.14080}, 2021.

\bibitem{ODESolver1}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in neural information processing systems},
  35:26565--26577, 2022.

\bibitem{StyleGAN_3}
Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten,
  Jaakko Lehtinen, and Timo Aila.
\newblock Alias-free generative adversarial networks.
\newblock {\em Advances in neural information processing systems}, 34:852--863,
  2021.

\bibitem{Bksdm}
Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi.
\newblock Bk-sdm: A lightweight, fast, and cheap version of stable diffusion.
\newblock In {\em European Conference on Computer Vision}, pages 381--399.
  Springer.

\bibitem{VDM}
Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock Variational diffusion models.
\newblock {\em Advances in neural information processing systems},
  34:21696--21707, 2021.

\bibitem{VAE}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes, 2013.

\bibitem{SAM}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 4015--4026, 2023.

\bibitem{Reformer}
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{Shape}
Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui.
\newblock Shape: Shifted absolute position embedding for transformers.
\newblock {\em arXiv preprint arXiv:2109.05644}, 2021.

\bibitem{kornblith2019similarity}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In {\em International conference on machine learning}, pages
  3519--3529. PMlR, 2019.

\bibitem{KL-Div}
Solomon Kullback and Richard~A Leibler.
\newblock On information and sufficiency.
\newblock {\em The annals of mathematical statistics}, 22(1):79--86, 1951.

\bibitem{Flux}
Black~Forest Labs.
\newblock Flux model.
\newblock 2024.
\newblock URL: \url{https://bfl.ai}.

\bibitem{MNIST}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 2002.

\bibitem{lee2020layer}
Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin.
\newblock Layer-adaptive sparsity for the magnitude-based pruning.
\newblock {\em arXiv preprint arXiv:2010.07611}, 2020.

\bibitem{Koala}
Youngwan Lee, Kwanyong Park, Yoorhim Cho, Yong-Ju Lee, and Sung~Ju Hwang.
\newblock Koala: Empirical lessons toward memory-efficient and fast diffusion
  models for text-to-image synthesis.
\newblock {\em Advances in Neural Information Processing Systems},
  37:51597--51633, 2024.

\bibitem{mplug}
C~Li, H~Xu, J~Tian, W~Wang, M~Yan, B~Bi, J~Ye, H~Chen, G~Xu, Z~Cao, et~al.
\newblock mplug: Effective and efficient vision-language learning by
  cross-modal skip-connections. arxiv 2022.
\newblock {\em arXiv preprint arXiv:2205.12005}, 1, 2022.

\bibitem{li2024playground}
Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail
  Doshi.
\newblock Playground v2.5: Three insights towards enhancing aesthetic quality
  in text-to-image generation, 2024.
\newblock \href {https://arxiv.org/abs/2402.17245} {\path{arXiv:2402.17245}}.

\bibitem{FIRE}
Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil
  Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.
\newblock Functional interpolation for relative positions improves long context
  transformers.
\newblock {\em arXiv preprint arXiv:2310.04418}, 2023.

\bibitem{Cape}
Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex
  Rogozhnikov.
\newblock Cape: Encoding relative positions with continuous augmented
  positional embeddings.
\newblock {\em Advances in Neural Information Processing Systems},
  34:16079--16092, 2021.

\bibitem{SDXL_Lightning}
Shanchuan Lin, Anran Wang, and Xiao Yang.
\newblock Sdxl-lightning: Progressive adversarial diffusion distillation.
\newblock {\em arXiv preprint arXiv:2402.13929}, 2024.

\bibitem{PacGAN}
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh.
\newblock Pacgan: The power of two samples in generative adversarial networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{FlowMatching}
Yaron Lipman, Ricky~TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
\newblock Flow matching for generative modeling.
\newblock {\em arXiv preprint arXiv:2210.02747}, 2022.

\bibitem{FastGAN}
Bingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal.
\newblock Towards faster and stabilized gan training for high-fidelity few-shot
  image synthesis.
\newblock In {\em iclr}.

\bibitem{LLaVA}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em Advances in neural information processing systems},
  36:34892--34916, 2023.

\bibitem{RectifiedFlowMatching}
Xingchao Liu, Chengyue Gong, and Qiang Liu.
\newblock Flow straight and fast: Learning to generate and transfer data with
  rectified flow.
\newblock {\em arXiv preprint arXiv:2209.03003}, 2022.

\bibitem{Floater}
Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh.
\newblock Learning to encode position for transformer with continuous dynamical
  model.
\newblock In {\em International conference on machine learning}, pages
  6327--6335. PMLR.

\bibitem{ODESolver2}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model
  sampling in around 10 steps.
\newblock {\em Advances in Neural Information Processing Systems},
  35:5775--5787, 2022.

\bibitem{LatentConsistencyModel}
Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao.
\newblock Latent consistency models: Synthesizing high-resolution images with
  few-step inference.
\newblock {\em arXiv preprint arXiv:2310.04378}, 2023.

\bibitem{ma2025pluggable}
Jian Ma, Qirong Peng, Xujie Zhu, Peixing Xie, Chen Chen, and Haonan Lu.
\newblock Pluggable pruning with contiguous layer distillation for diffusion
  transformers.
\newblock {\em arXiv preprint arXiv:2511.16156}, 2025.

\bibitem{men2024shortgpt}
Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei
  Han, and Weipeng Chen.
\newblock Shortgpt: Layers in large language models are more redundant than you
  expect, 2024.
\newblock {\em URL https://arxiv. org/abs/2403.03853}, 2(3):4, 2024.

\bibitem{GuidedDistillation}
Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon,
  Jonathan Ho, and Tim Salimans.
\newblock On distillation of guided diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14297--14306.

\bibitem{neuhold2017mapillary}
Gerhard Neuhold, Tobias Ollmann, Samuel Rota~Bulo, and Peter Kontschieder.
\newblock The mapillary vistas dataset for semantic understanding of street
  scenes.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 4990--4999, 2017.

\bibitem{ImprovedDDPM}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In {\em International conference on machine learning}, pages
  8162--8171. PMLR.

\bibitem{ALiBi}
Joon~Sung Park, Joseph O'Brien, Carrie~Jun Cai, Meredith~Ringel Morris, Percy
  Liang, and Michael~S Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock In {\em Proceedings of the 36th annual acm symposium on user
  interface software and technology}, pages 1--22.

\bibitem{ImageTransformer}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock In {\em International conference on machine learning}, pages
  4055--4064. PMLR.

\bibitem{pawlowski2024physics}
Jan Pawlowski and Tilman Plehn.
\newblock Physics and machine learning.
\newblock Lecture script, Institut für Theoretische Physik, Universität
  Heidelberg, January 2024.
\newblock Version from January 30, 2024.

\bibitem{DiT}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 4195--4205, 2023.

\bibitem{adaLN}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron
  Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem{SDXL}
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas
  Müller, Joe Penna, and Robin Rombach.
\newblock Sdxl: Improving latent diffusion models for high-resolution image
  synthesis.
\newblock {\em arXiv preprint arXiv:2307.01952}, 2023.

\bibitem{pons2024effective}
Ian Pons, Bruno Yamamoto, Anna~H Reali~Costa, and Artur Jordao.
\newblock Effective layer pruning through similarity metric perspective.
\newblock In {\em International Conference on Pattern Recognition}, pages
  423--438. Springer, 2024.

\bibitem{CLIP}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PmLR, 2021.

\bibitem{ConvGAN}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1511.06434}, 2015.

\bibitem{T5XXL}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{StandAloneAttentionVisonModels}
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,
  and Jon Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{CNN}
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,
  and Jon Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{SelfAttentionFigure}
Sebastian Raschka.
\newblock Understanding and coding the self-attention mechanism of large
  language models from scratch.
\newblock 2023.
\newblock URL:
  \url{https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html}.

\bibitem{NormalizingFlow}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International conference on machine learning}, pages
  1530--1538. PMLR.

\bibitem{SD21}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 10684--10695.

\bibitem{LDM}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models,
  December 01, 2021 2021.
\newblock CVPR 2022.
\newblock URL: \url{https://ui.adsabs.harvard.edu/abs/2021arXiv211210752R},
  \href {https://doi.org/10.48550/arXiv.2112.10752}
  {\path{doi:10.48550/arXiv.2112.10752}}.

\bibitem{RNN}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533--536, 1986.

\bibitem{ImprovedGANTraining}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{ProgressiveDistillation}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em arXiv preprint arXiv:2202.00512}, 2022.

\bibitem{LADD}
Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser,
  and Robin Rombach.
\newblock Fast high-resolution image synthesis with latent adversarial
  diffusion distillation.
\newblock In {\em SIGGRAPH Asia 2024 Conference Papers}, pages 1--11.

\bibitem{StyleGAN_T}
Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila.
\newblock Stylegan-t: Unlocking the power of gans for fast large-scale
  text-to-image synthesis.
\newblock In {\em International conference on machine learning}, pages
  30105--30118. PMLR.

\bibitem{ADD}
Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach.
\newblock Adversarial diffusion distillation.
\newblock In {\em European Conference on Computer Vision}, pages 87--103.
  Springer.

\bibitem{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock {\em Advances in neural information processing systems},
  35:25278--25294, 2022.

\bibitem{schuhmann2022laionaesthetic}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al.
\newblock {LAION-Aesthetic} v1.
\newblock
  \url{https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md},
  2022.
\newblock Accessed: 2026-01-22.

\bibitem{Objects365}
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing
  Li, and Jian Sun.
\newblock Objects365: A large-scale, high-quality dataset for object detection.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 8430--8439, 2019.

\bibitem{DMBeginning}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International conference on machine learning}, pages
  2256--2265. pmlr.

\bibitem{DiffusionModelIntroPaper}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International conference on machine learning}, pages
  2256--2265. pmlr.

\bibitem{DDIM}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models, October 01, 2020 2020.
\newblock ICLR 2021; updated connections with ODEs at page 6, fixed some typos
  in the proof.
\newblock URL: \url{https://ui.adsabs.harvard.edu/abs/2020arXiv201002502S},
  \href {https://doi.org/10.48550/arXiv.2010.02502}
  {\path{doi:10.48550/arXiv.2010.02502}}.

\bibitem{ConsistencyModel}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models.
\newblock 2023.

\bibitem{SGM}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{IntroSGM}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{ImprovedSGMs}
Yang Song and Stefano Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock {\em Advances in neural information processing systems},
  33:12438--12448, 2020.

\bibitem{SlicedScoreMatching}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In {\em Uncertainty in artificial intelligence}, pages 574--584.
  PMLR.

\bibitem{SDE}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{ScoreBasedODE}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{LectureNotesDDPM}
Inga Strümke and Helge Langseth.
\newblock Lecture notes in probabilistic diffusion models.
\newblock {\em arXiv preprint arXiv:2312.10393}, 2023.

\bibitem{RoPE}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock {\em Neurocomputing}, 568:127063, 2024.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{tan2025ominicontrol}
Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang.
\newblock Ominicontrol: Minimal and universal control for diffusion
  transformer.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 14940--14950, 2025.

\bibitem{Gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu,
  Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, and Katie Millican.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{flux_mini_2024}
{TencentARC}.
\newblock flux-mini.
\newblock \url{https://huggingface.co/TencentARC/flux-mini}, 2024.
\newblock Hugging Face Model Hub.

\bibitem{ModeCollapse1}
Hoang Thanh-Tung and Truyen Tran.
\newblock Catastrophic forgetting and mode collapse in gans.
\newblock In {\em 2020 international joint conference on neural networks
  (ijcnn)}, pages 1--10. IEEE.

\bibitem{MCMC}
Luke Tierney.
\newblock Markov chains for exploring posterior distributions.
\newblock {\em the Annals of Statistics}, pages 1701--1728, 1994.

\bibitem{Lama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti
  Bhosale.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{VQVAE}
Aaron Van Den~Oord and Oriol Vinyals.
\newblock Neural discrete representation learning.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{AttentionIsAllYouNeed}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Łukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{DiffusionDB}
Zijie~J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and
  Duen~Horng Chau.
\newblock Diffusiondb: A large-scale prompt gallery dataset for text-to-image
  generative models.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 893--911, 2023.

\bibitem{HPSv2}
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and
  Hongsheng Li.
\newblock Human preference score v2: A solid benchmark for evaluating human
  preferences of text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2306.09341}, 2023.

\bibitem{Sana}
Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai
  Zhang, Muyang Li, Ligeng Zhu, and Yao Lu.
\newblock Sana: Efficient high-resolution image synthesis with linear diffusion
  transformers.
\newblock {\em arXiv preprint arXiv:2410.10629}, 2024.

\bibitem{Survey2}
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao
  Zhang, Bin Cui, and Ming-Hsuan Yang.
\newblock Diffusion models: A comprehensive survey of methods and applications.
\newblock {\em ACM Computing Surveys}, 56(4):1--39, 2023.

\bibitem{ImprovedDistillationMatching}
Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo
  Durand, and Bill Freeman.
\newblock Improved distribution matching distillation for fast image synthesis.
\newblock {\em Advances in neural information processing systems},
  37:47455--47487, 2024.

\bibitem{DistillationMatching}
Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand,
  William~T Freeman, and Taesung Park.
\newblock One-step diffusion with distribution matching distillation.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 6613--6623.

\bibitem{yu2022scaling}
Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang,
  Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, et~al.
\newblock Scaling autoregressive models for content-rich text-to-image
  generation.
\newblock {\em arXiv preprint arXiv:2206.10789}, 2(3):5, 2022.

\bibitem{RMSNorm}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{Laptop}
Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu.
\newblock Laptop-diff: Layer pruning and normalized distillation for
  compressing diffusion models.
\newblock {\em arXiv preprint arXiv:2404.11098}, 2024.

\bibitem{ControlNet}
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 3836--3847, 2023.

\bibitem{ODESolver3}
Qinsheng Zhang and Yongxin Chen.
\newblock Fast sampling of diffusion models with exponential integrator.
\newblock {\em arXiv preprint arXiv:2204.13902}, 2022.

\bibitem{Dense2MoE}
Youwei Zheng, Yuxi Ren, Xin Xia, Xuefeng Xiao, and Xiaohua Xie.
\newblock Dense2moe: Restructuring diffusion transformer to moe for efficient
  text-to-image generation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 18661--18670, 2025.

\end{thebibliography}
