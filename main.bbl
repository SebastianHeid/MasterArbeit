\begin{thebibliography}{10}

\bibitem{GPT4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
  Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, and
  Shyamal Anadkat.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{HiDream}
Vivago AI.
\newblock Hidream-l1.
\newblock 2025.
\newblock URL: \url{https://vivago.ai/home}.

\bibitem{Anderson}
Brian~DO Anderson.
\newblock Reverse-time diffusion equation models.
\newblock {\em Stochastic Processes and their Applications}, 12(3):313--326,
  1982.

\bibitem{Claude}
Anthropic.
\newblock Claude 3 haiku: our fastest model yet.
\newblock 2024.
\newblock URL: \url{https://www.anthropic.com/news/claude-3-haiku}.

\bibitem{WGAN}
Martin Arjovsky, Soumith Chintala, and Léon Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International conference on machine learning}, pages
  214--223. PMLR.

\bibitem{LayerNorm}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{OrgAttention}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{ELBO}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American statistical Association},
  112(518):859--877, 2017.

\bibitem{VariationalInference}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American statistical Association},
  112(518):859--877, 2017.

\bibitem{DDPMSamplerImage}
Anja Butter, Nathan Huetsch, Sofia Palacios~Schweitzer, Tilman Plehn, Peter
  Sorrenson, and Jonas Spinner.
\newblock Jet diffusion versus jetgpt–modern networks for the lhc.
\newblock {\em SciPost Physics Core}, 8(1):026, 2025.

\bibitem{Pixart_s}
Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao
  Wang, Ping Luo, Huchuan Lu, and Zhenguo Li.
\newblock Pixart-$\sigma$: Weak-to-strong training of diffusion transformer for
  4k text-to-image generation.
\newblock In {\em European Conference on Computer Vision}, pages 74--91.
  Springer.

\bibitem{SparseTransformer}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{Performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, and
  Lukasz Kaiser.
\newblock Rethinking attention with performers.
\newblock {\em arXiv preprint arXiv:2009.14794}, 2020.

\bibitem{SurveyDM}
Florinel-Alin Croitoru, Vlad Hondru, Radu~Tudor Ionescu, and Mubarak Shah.
\newblock Diffusion models in vision: A survey.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  45(9):10850--10869, 2023.

\bibitem{FlashAttention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock {\em Advances in neural information processing systems},
  35:16344--16359, 2022.

\bibitem{BERT}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 conference of the North American
  chapter of the association for computational linguistics: human language
  technologies, volume 1 (long and short papers)}, pages 4171--4186.

\bibitem{AffineCouplingLayer}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock {\em arXiv preprint arXiv:1410.8516}, 2014.

\bibitem{VisionTransformer}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, and Sylvain Gelly.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{feller1949stochastic}
William Feller.
\newblock On the theory of stochastic processes, with particular reference to
  applications.
\newblock In {\em Proceedings of the First Berkeley Symposium on Mathematical
  Statistics and Probability}, pages 403--432, Berkeley, CA, 1949. The Regents
  of the University of California.

\bibitem{GAN}
Ian~J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{SteinDiscrepancy}
Jackson Gorham and Lester Mackey.
\newblock Measuring sample quality with kernels.
\newblock In {\em International Conference on Machine Learning}, pages
  1292--1301. PMLR.

\bibitem{ResidualConnection}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem{AE}
Geoffrey~E Hinton and Ruslan~R Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock {\em science}, 313(5786):504--507, 2006.

\bibitem{DDPM}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models, June 01, 2020 2020.
\newblock URL: \url{https://ui.adsabs.harvard.edu/abs/2020arXiv200611239H},
  \href {https://doi.org/10.48550/arXiv.2006.11239}
  {\path{doi:10.48550/arXiv.2006.11239}}.

\bibitem{Ccnet}
Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu
  Liu.
\newblock Ccnet: Criss-cross attention for semantic segmentation.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 603--612.

\bibitem{FastSDE}
Alexia Jolicoeur-Martineau, Ke~Li, Rémi Piché-Taillefer, Tal Kachman, and
  Ioannis Mitliagkas.
\newblock Gotta go fast when generating data with score-based models.
\newblock {\em arXiv preprint arXiv:2105.14080}, 2021.

\bibitem{ODESolver1}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in neural information processing systems},
  35:26565--26577, 2022.

\bibitem{StyleGAN_3}
Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten,
  Jaakko Lehtinen, and Timo Aila.
\newblock Alias-free generative adversarial networks.
\newblock {\em Advances in neural information processing systems}, 34:852--863,
  2021.

\bibitem{VDM}
Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock Variational diffusion models.
\newblock {\em Advances in neural information processing systems},
  34:21696--21707, 2021.

\bibitem{VAE}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes, 2013.

\bibitem{Reformer}
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{Shape}
Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui.
\newblock Shape: Shifted absolute position embedding for transformers.
\newblock {\em arXiv preprint arXiv:2109.05644}, 2021.

\bibitem{KL-Div}
Solomon Kullback and Richard~A Leibler.
\newblock On information and sufficiency.
\newblock {\em The annals of mathematical statistics}, 22(1):79--86, 1951.

\bibitem{Flux}
Black~Forest Labs.
\newblock Flux model.
\newblock 2024.
\newblock URL: \url{https://bfl.ai}.

\bibitem{FIRE}
Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil
  Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.
\newblock Functional interpolation for relative positions improves long context
  transformers.
\newblock {\em arXiv preprint arXiv:2310.04418}, 2023.

\bibitem{Cape}
Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex
  Rogozhnikov.
\newblock Cape: Encoding relative positions with continuous augmented
  positional embeddings.
\newblock {\em Advances in Neural Information Processing Systems},
  34:16079--16092, 2021.

\bibitem{PacGAN}
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh.
\newblock Pacgan: The power of two samples in generative adversarial networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{FlowMatching}
Yaron Lipman, Ricky~TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
\newblock Flow matching for generative modeling.
\newblock {\em arXiv preprint arXiv:2210.02747}, 2022.

\bibitem{FastGAN}
Bingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal.
\newblock Towards faster and stabilized gan training for high-fidelity few-shot
  image synthesis.
\newblock In {\em iclr}.

\bibitem{RectifiedFlowMatching}
Xingchao Liu, Chengyue Gong, and Qiang Liu.
\newblock Flow straight and fast: Learning to generate and transfer data with
  rectified flow.
\newblock {\em arXiv preprint arXiv:2209.03003}, 2022.

\bibitem{Floater}
Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh.
\newblock Learning to encode position for transformer with continuous dynamical
  model.
\newblock In {\em International conference on machine learning}, pages
  6327--6335. PMLR.

\bibitem{ODESolver2}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model
  sampling in around 10 steps.
\newblock {\em Advances in Neural Information Processing Systems},
  35:5775--5787, 2022.

\bibitem{ImprovedDDPM}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In {\em International conference on machine learning}, pages
  8162--8171. PMLR.

\bibitem{ALiBi}
Joon~Sung Park, Joseph O'Brien, Carrie~Jun Cai, Meredith~Ringel Morris, Percy
  Liang, and Michael~S Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock In {\em Proceedings of the 36th annual acm symposium on user
  interface software and technology}, pages 1--22.

\bibitem{ImageTransformer}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock In {\em International conference on machine learning}, pages
  4055--4064. PMLR.

\bibitem{pawlowski2024physics}
Jan Pawlowski and Tilman Plehn.
\newblock Physics and machine learning.
\newblock Lecture script, Institut für Theoretische Physik, Universität
  Heidelberg, January 2024.
\newblock Version from January 30, 2024.

\bibitem{SDXL}
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas
  Müller, Joe Penna, and Robin Rombach.
\newblock Sdxl: Improving latent diffusion models for high-resolution image
  synthesis.
\newblock {\em arXiv preprint arXiv:2307.01952}, 2023.

\bibitem{ConvGAN}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1511.06434}, 2015.

\bibitem{StandAloneAttentionVisonModels}
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,
  and Jon Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{CNN}
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,
  and Jon Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{SelfAttentionFigure}
Sebastian Raschka.
\newblock Understanding and coding the self-attention mechanism of large
  language models from scratch.
\newblock 2023.
\newblock URL:
  \url{https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html}.

\bibitem{NormalizingFlow}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International conference on machine learning}, pages
  1530--1538. PMLR.

\bibitem{SD21}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 10684--10695.

\bibitem{LDM}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models,
  December 01, 2021 2021.
\newblock CVPR 2022.
\newblock URL: \url{https://ui.adsabs.harvard.edu/abs/2021arXiv211210752R},
  \href {https://doi.org/10.48550/arXiv.2112.10752}
  {\path{doi:10.48550/arXiv.2112.10752}}.

\bibitem{RNN}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533--536, 1986.

\bibitem{ImprovedGANTraining}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{StyleGAN_T}
Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila.
\newblock Stylegan-t: Unlocking the power of gans for fast large-scale
  text-to-image synthesis.
\newblock In {\em International conference on machine learning}, pages
  30105--30118. PMLR.

\bibitem{DMBeginning}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International conference on machine learning}, pages
  2256--2265. pmlr.

\bibitem{DiffusionModelIntroPaper}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International conference on machine learning}, pages
  2256--2265. pmlr.

\bibitem{DDIM}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models, October 01, 2020 2020.
\newblock ICLR 2021; updated connections with ODEs at page 6, fixed some typos
  in the proof.
\newblock URL: \url{https://ui.adsabs.harvard.edu/abs/2020arXiv201002502S},
  \href {https://doi.org/10.48550/arXiv.2010.02502}
  {\path{doi:10.48550/arXiv.2010.02502}}.

\bibitem{SGM}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{IntroSGM}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{ImprovedSGMs}
Yang Song and Stefano Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock {\em Advances in neural information processing systems},
  33:12438--12448, 2020.

\bibitem{SlicedScoreMatching}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In {\em Uncertainty in artificial intelligence}, pages 574--584.
  PMLR.

\bibitem{SDE}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{ScoreBasedODE}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{LectureNotesDDPM}
Inga Strümke and Helge Langseth.
\newblock Lecture notes in probabilistic diffusion models.
\newblock {\em arXiv preprint arXiv:2312.10393}, 2023.

\bibitem{RoPE}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock {\em Neurocomputing}, 568:127063, 2024.

\bibitem{Gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu,
  Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, and Katie Millican.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{ModeCollapse1}
Hoang Thanh-Tung and Truyen Tran.
\newblock Catastrophic forgetting and mode collapse in gans.
\newblock In {\em 2020 international joint conference on neural networks
  (ijcnn)}, pages 1--10. IEEE.

\bibitem{MCMC}
Luke Tierney.
\newblock Markov chains for exploring posterior distributions.
\newblock {\em the Annals of Statistics}, pages 1701--1728, 1994.

\bibitem{Lama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti
  Bhosale.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{VQVAE}
Aaron Van Den~Oord and Oriol Vinyals.
\newblock Neural discrete representation learning.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{AttentionIsAllYouNeed}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Łukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{Sana}
Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai
  Zhang, Muyang Li, Ligeng Zhu, and Yao Lu.
\newblock Sana: Efficient high-resolution image synthesis with linear diffusion
  transformers.
\newblock {\em arXiv preprint arXiv:2410.10629}, 2024.

\bibitem{Survey2}
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao
  Zhang, Bin Cui, and Ming-Hsuan Yang.
\newblock Diffusion models: A comprehensive survey of methods and applications.
\newblock {\em ACM Computing Surveys}, 56(4):1--39, 2023.

\bibitem{ODESolver3}
Qinsheng Zhang and Yongxin Chen.
\newblock Fast sampling of diffusion models with exponential integrator.
\newblock {\em arXiv preprint arXiv:2204.13902}, 2022.

\end{thebibliography}
