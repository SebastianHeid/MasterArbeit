\section{Diffusion Model Distillation}
Diffusion models excel in generating high-quality and diverse images, showcasing remarkable capabilities in image synthesis. However, their main disadvantage is their long inference time. This stems from two factors: First, diffusion models have an iterative generation process removing noise step-by-step. To obtain high-quality images often 50-100 steps are required. Second, recent diffusion models like Flux \cite{Flux} or HiDream \cite{HiDream} are very large, with billions of parameters, making each step computationally and memory intensive. These issues limit their use for real-time applications or in memory constraint environments such as edge devices.

Diffusion model distillation addresses these challenges. The principal idea of all distillation methods is to transfer the knowledge and image generation capacity from a fully trained diffusion model often referred to as teacher model to a target model often referred to as student model. The research landscape reveals two complementary strategies: accelerating inference by reducing the number of denoising steps, and creating more compact models through model size reduction. The first strategy, reducing number of inference step, is more dominant in the literature and several different approaches such as adversarial distillation \cite{ADD, LADD}, progressive distillation \cite{ProgressiveDistillation, SDXL_Lightning}, distribution matching distillation \cite{DistillationMatching, ImprovedDistillationMatching}, consistency models \cite{ConsistencyModel, LatentConsistencyModel} and guided distillation \cite{GuidedDistillation}. The second strategy was explored via knowledge Distillation being very suitable for reducing the model size which was applied to  stable diffusion \cite{Bksdm} and stable diffusion XL \cite{Laptop, Koala}. \\

\subsection{Distillation Methods for Few-Step Diffusion Models}
In the following various approaches are presented for training few step diffusion models. Each method employs a pre-trained teacher model to guide the distillation of a student model capable of generating images in significantly fewer steps.
\subsubsection{Adversarial Diffusion Model Distillation}
Adversarial Diffusion Distillation (ADD) \cite{ADD} adopts a training setup similar to GANs. A discriminator is trained to distinguish whether an image comes from the real dataset or was generated by the student model. This component uses the standard MinMax loss, as commonly used in GAN training.

In addition to the adversarial loss, a second learning signal is introduced by comparing images generated by the student model that is designed to perform generation in a few (or even a single) denoising steps, with those produced by a fully sampled teacher diffusion model. The full training procedure is illustrated in Fig. \ref{ADD}, where Stable Diffusion is used as the teacher model.

As shown in the upper diagram of fig. \ref{ADD}, the input image is first encoded into the latent space, and noise is added. For example, if the student is trained to operate in 4 steps, one of four noise levels (e.g., steps 999, 749, 499, 249) is randomly selected. The student model then performs a single denoising step.

The student's output serves two roles. First, it is passed to the discriminator to compute the adversarial loss $\mathcal{L_{\text{adv}}}$. The discriminator consists of a frozen backbone, DinoV2 \cite{DinoV2}, and learnable heads. Second, noise is added again before feeding it into the frozen teacher model. After the teacher performs full denoising, its output is compared to the studentâ€™s original prediction using an MSE loss $\mathcal{L_{\text{dis}}}$. The overall training objective combines both the adversarial and distillation losses with a weighting constant $\lambda$

\begin{equation}
    \mathcal{L} = \mathcal{L_{\text{adv}}} + \lambda \mathcal{L_{\text{dis}}}
\end{equation}
 A famous model that has been obtained by employing adversarial diffusion distillation is SDXL-Turbo \cite{ADD}.
Despite it success, the proposed distillation method comes with two decisive shortcomings. First, the use of DinoV2 as a discriminator backbone limits the size of the generated images to $512\times512$, since one must not deviate too much from the image size on which DinoV2 was trained in order to retain the expressiveness of DinoV2's features. Second, the encoding and decoding of the images into the latent space and back is computationally expensive. Ideally, distillation would be directly performed in the latent space. This insight let to latent adversarial diffusion distillation (LADD) \cite{LADD}. The first key difference is the replacement of real images from a dataset with synthetic images generated by the teacher model prior to training. To be more precise, only the latents not the images of the teacher model needs to be stored because the whole distillation process takes place in the latent space. Second, the role of the discriminator was taken over by the teacher model itself. Third, only the adversarial loss was employed, as adding the distillation loss does not show empirical benefits. 
The full training scheme is illustrated in the lower diagram of fig. \ref{ADD}. Initially, noise is added to the pre-generated latents of the teacher. Afterwards the student model is applied to predict denoised latents. To compute the MinMax objective noise is added to the student generated latents before it is given as input to the discriminator consisting of the frozen teacher model and trainable heads. As "real" latents the noised pre-generated latents of the teacher model are used. \\
In summary, LADD is a simplified and more compute efficient version of ADD maintaining the main idea of using adversarial distillation. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/Background/ADD.png}  % adjust filename and width
    \caption{(Latent) adversarial diffusion distillation from \cite{LADD}.}
    \label{ADD}
\end{figure}



\subsubsection{Progressive Distillation}
In contrast to ADD, where a single training run is used to create a model that requires only very few inference steps to generate a high-quality image, the idea of progressive distillation \cite{ProgressiveDistillation} is to iteratively train models that require fewer and fewer inference steps, thereby reducing the complexity of the task within one iteration. Assuming a diffusion model requires $N$ steps during inference, then in the first iteration, a student model, initialized with the weights of the teacher model, is trained to generate images with only $\frac{N}{2}$ steps. In the second iteration, the previously trained student model is used as the teacher and another student model is trained, needing only $\frac{N}{4}$ inference steps. This procedure can be repeated until a one-step model is reached. The objective function is similar to that used in training of a diffusion model. Let $\hat{f}$ be the teacher model and $f$ the student model. During training, a noisy input $\mathbf{z}_t$ is sampled from the training dataset. The student model is applied exactly once, $\mathbf{z}_{\text{pred}} = f(\mathbf{z}_t)$ while the teacher model is applied twice, $\hat{\mathbf{z}} = \hat{f}\circ \hat{f} \left(\mathbf{z}_t \right)  $. In the objective function, the prediction of the teacher $\hat{\mathbf{z}}$ is compared with the prediction of the student $\mathbf{z}_{\text{pred}}$, e.g, via MSE. This core idea is shown in fig. \ref{ProgressiveDistillation}. A possible downside of this method is that progressive distillation carries the risk of error accumulation, as in each iteration the previous student is used as a teacher, therefore especially two or one step models show a downgrade in performance. Furthermore, the iterative approach is resource intensive, as many trainings have to be performed.\\
Stable Diffusion XL lightning \cite{SDXL_Lightning} is a distilled version of Stable Diffusion XL (SDXL) where, a combination of progressive and adversarial distillation is applied. First, the authors trained a version of SDXL with 32 inference steps using only progressive distillation. Afterwards, they additionally applied adversarial distillation to go further down to a few step-model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Background/ProgressiveDistillation.png}  % adjust filename and width
    \caption{Progressive distillation from \cite{ProgressiveDistillation}.}
    \label{ProgressiveDistillation}
\end{figure}

\subsubsection{Consistency Model}
Consistency models \cite{ConsistencyModel} have been designed to map images with different noise levels to their corresponding clean image in one step. To achieve this, during training the self-consistency property is enforced for the model meaning that two noisy images lying on the same denoising path are mapped to the same clean image. This self-consistency property is the foundation of the training procedures which enables one-step image generation.


Let $\left\{ x_t \right\}_{t=\epsilon}^T$ be a denoising trajectory where $x_\epsilon$ is the clean image and $x_T$ is pure noise. A model $f_\theta$ has the self-consistency property if for two images $x_t$ and $x_{t{'}}$ with $t,t^{'} \in \left[  \epsilon,T \right]$ which are on the same denoising trajectory $f_\theta(x_t,t) = f_\theta(x_{t{'}}, t')$ holds (see fig. \ref{SelfConsistency}). In addition, a consistency model has to fulfill the boundary condition $f_\theta(x_\epsilon, \epsilon) = x_\epsilon$. This can be enforced by parameterize the model as  

\begin{equation}
    f_\theta(x,t) = c_\text{skip}(t)x + c_\text{out}(t)F_\theta(x)
\end{equation}
where $F_\theta$ is a neural network and $c_\text{skip}(t)$ and $c_\text{out}(t)$ are differentiable function such as  $c_\text{skip}(\epsilon) = 1$ and $c_\text{out}(\epsilon) = 0$. During the training process, noise is first added to the image from the training dataset in a similar way to training a standard diffusion model obtaining a noisy image $x_{t+1}$. Next, the teacher model is applied once to obtain a slightly denoised image $x_t^\phi$ where $\phi$ denotes the fixed parameters of the teacher model. It is important to note that both $x_{t+1}$ and $x_t^\phi$ lie on the same denoising path and accodingly to the self-consistency property  $f_\theta(x_{t+1},t+1) = f_\theta(x_t^\phi, t)$ should hold. Therefore, the training objective is given by 
\begin{equation}
    \mathcal{L_\text{CD}}\left(\theta, \theta^-; \phi\right) = \mathbb{E}\left[ \lambda(t) d\left( f_\theta(x_{t+1}, t+1), f_{\theta^-}(x_{t}^\phi, t)  \right)  \right]
\end{equation}
where $\theta$ denotes the parameters of the student consistency model, $\theta^-$ denotes a running exponential moving average (EMA) of the past $\theta$ values and $d$ stands for the MSE. The EMA is used for training stability reasons. With this training framework one step models can be obtained. But also multi-step inference can be accomplished with this model if higher image quality is needed. It is worth noting that distillation is only one way to train consistency models. Another way is to train them from scratch. How this could be achieved is described in \cite{ConsistencyModel} but out of scope of this short summary. Furthermore, successive works show that this concept can also be extended to latent space.  \cite{LatentConsistencyModel}. However, some literature \cite{LADD} claims that consistency models requires extensive hyperparameter tuning and engeneering in order to work.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/Background/Self_Consistency.png}  % adjust filename and width
    \caption{Self-consistency property taken from \cite{ConsistencyModel}.}
    \label{SelfConsistency}
\end{figure}


\subsubsection{Distribution Matching Distillation}
In contrast to other distillation approaches, distribution matching distillation \cite{DistillationMatching} does not regress individual samples, but instead focuses on matching the whole image distribution. Concretely, it optimizes the student model to produce samples that collectively match the statistical porperties of real data and not the output of the teacher on specific noise-image pairs.
To do so, two diffusion models and an additional noise-image dataset used for regularization is required (see fig. \ref{DistributionMatchingDistillation}). The distilled student model $f_\theta$ is initialized with the weights of a pretrained teacher model $\hat{f_\phi}$. The learning objective is to minimize the KL-divergence between the \textit{real} data distribution $p_\text{real}$ and the \textit{fake} data distribution $p_\text{fake}$ produced from the distilled model $f_\theta$
\begin{equation}
    \text{KL} [p_\text{real} \,\|\,p_\text{fake}] = \mathbb{E}_{\substack{z \sim \mathcal{N}(0; \mathbf{I}) \\ x = f_\theta(z)}} \left[ 
  -\left( \log p_{\text{real}}(x) - \log p_{\text{fake}}(x) \right) 
\right]
\end{equation}
Although this KL-divergence is not tractable, it can be used for optimizing the student model because the goal is not to evaluate the KL divergence itself, but to compute its gradient, which can be expressed as
\begin{equation}
    \nabla_\theta \text{KL} [p_\text{real} \,\|\,p_\text{fake}] = \mathbb{E}_{\substack{z \sim \mathcal{N}(0; \mathbf{I}) \\ x = f_\theta(z)}} \left[
  -\left( s_{\text{real}}(x) - s_{\text{fake}}(x) \right) \frac{df}{d\theta}
\right]
\end{equation}
with $s_{\text{real}}(x) = \nabla_x \log p_\text{real}(x)$ and $s_{\text{fake}}(x) = \nabla_x \log p_\text{fake}(x)$ being the scores of the real and fake distributions. To prevent diverging scores, the distributions are perturbed with Gaussian noise, which ensures that the gradients are well-defined. The score functions $s_{\text{real}}(x)$ and $s_{\text{fake}}(x)$ are modeled by two diffusion models $\hat{
f}_\text{fake}$ and $\hat{f}_\text{real}$, both initialized with pretrained weights of the teacher model. The weight of $\hat{f}_\text{real}$ are kept fixed throughout the whole training process, whereas $\hat{
f}_\text{fake}$ is trained jointly with the distilled student model which is necessary because the fake distribution evolves over the course of training. The score and the diffusion models are connected via \cite{ScoreBasedDiffModel}
\begin{equation}
    s_\text{real}(x_t,t) = - \frac{x_t - \alpha_t\hat{f}_\text{real}(x_t,t)}{\sigma^2_t}
\end{equation}
with $\alpha_t$ being the scaling factor of the diffusion process, and similarly for $s_\text{fake}$. In addition, to stabilize training and prevent mode collapse, an additional regression loss $\mathcal{L}_\text{reg}$ is applied, in which the generated image of the student model based on the noise from a noise-image pair is compared via Learned Perceptual Image Patch Similarity (LPIPS) to the corresponding reference image. \cite{LPIPS} such that the total loss is given by 
\begin{equation}
    \mathcal{L} = \text{KL} + \lambda_\text{reg} \mathcal{L}_\text{reg}
\end{equation}
In total, this method provide a good training framework for one-step diffusion models. However, one disadvantage is that the method is very computationally intensive  due to three networks being involved during the training process and the pregeneration of the noise-image pairs. \cite{ImprovedDistillationMatching} removes the necessity of the noise-image pair dataset and introduces additionally a GAN loss for stabilizing the training. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Background/DistributionMatching.png}  % adjust filename and width
    \caption{First, the regression loss for training the distilled model is computed using noise-image image pairs being computed in advance. Second, the two diffusion models are used to approximate the score functions for the \textit{real} and \textit{fake} distribution. The diffuison model used to approximate the fake score is updated too during the training process. Figure taken from  \cite{DistillationMatching}.}
    \label{DistributionMatchingDistillation}
\end{figure}

\subsection{Distillation-Based Model Compression}
The second goal of distillation, besides reducing the number of inference steps, is to distill the knowledge and capabilities of large models into smaller ones. Hereby, knowledge distillation \cite{KnowledgeDistillation} is widely used \cite{Laptop, Koala, Bksdm}.

\subsubsection{Knowledge Distillation}
The concept of knowledge distillation was introduced for training a small model referred to as the student - on classification tasks, using a larger model - referred to as the teacher with strong generalization capabilities as guidance. It was shown, that computing the loss between the probability outputs of the teacher and the student enables much more efficient training than using the original class labels. In such a setting the small model could be trained on less data and with a higher learning rate. The authors hypothesized that even the very small probabilities the teacher assigns to incorrect classes, and particularly their relative magnitudes, carry valuable information for the student For example if a 2 of the MNIST dataset \cite{} should be classified it is valuable information for the model whether it resembles a 3 or a 5. When only hard targets, e.g., one-hot labels,  are used, this relational information is lost. Here, it is important to make this information accessible to the student meaning that e.g. in a classification problem one might have to deviate from the standard temperature $T$ of the softmax function to soften the probabilities even more. Specifically, both teacher and student use temperature scaling softmax($\frac{z_i}{T}$) where $T > 1$ makes the probability distribution softer, revealing more information about class similarities. In general, it has been shown that a combination of the loss $\mathcal{L_\text{KD}}$, calculated with the outputs from the teacher, and the original loss $\mathcal{L_\text{Task}}$, calculated with the hard labels, is advantageous
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{Task} + \lambda_\text{KD}\mathcal{L}_\text{KD}
\end{equation}
This idea was used to train a smaller version of Stable Diffusion \cite{BERT} or Stable Diffusion XL \cite{Laptop, Koala}. Hereby, the original model with its pretrained weights is the teacher and the student model was obtained by removing individual parts of the Unet \cite{Unet} architecture. The general idea in all approaches is to use the intermediate features from the Unet architecture as well as the final output as learning signal from the teacher to the student. Additionally, the original diffusion loss $\mathcal{L_\text{Task}}$ taken into account too such that the final loss is given by 
\begin{equation}
    \mathcal{L} = \mathcal{L_\text{Task}} + \lambda_\text{OutKD} \mathcal{L_\text{OutKD}} + \lambda_\text{FeatKD} \mathcal{L_\text{FeatKD}}
\end{equation}
where $\mathcal{L_\text{Task}}$ is the loss used for regular training of diffusion models, $\mathcal{L_\text{FeatKD}}$ is the loss computed between the intermediate features and $\mathcal{L_\text{OutKD}}$ is the loss computed between the final output of the student and the teacher model.