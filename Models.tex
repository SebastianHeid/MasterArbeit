\chapter{State-of-the-Art Image Generation Models}
In the following, the models, namely Pixart-$\Sigma$ \cite{Pixart_s} and Flux-dev \cite{Flux}, which were used for the experiments are described in detail.
\section{Pixart-$\Sigma$}
PixArt-$\Sigma$ \cite{Pixart_s}, the latest addition to the PixArt family \cite{Pixart_a, Pixart_d}, was developed to demonstrate that high-fidelity image generation can be achieved with significantly reduced training costs and model parameters. Pixart-$\Sigma$ is a further development of  Pixart-$\alpha$. Therefore, the shared architecture of Pixart-$\alpha$ and Pixart-$\Sigma$ is introduced first. Next, the key points for efficient training of Pixart-$\alpha$ are described, followed by the specific enhancements implemented in PixArt-$\Sigma$.

\subsection{Architecture }
The architecture of Pixart-$\Sigma$ is based on the class-conditional diffusion transformer \gls{DiT}-XL/2 \cite{DiT} which contains 28 transformer blocks. The transformer blocks have primarily three inputs (see fig. \ref{Pixart_Übersicht}). \\
The \textbf{timestep} $t$ is projected into a time embedding vector using a 256-frequency embedding, from which a \gls{MLP} extracts the global scale and shift parameters used as input for the customized adaptive layer normalization called adaLN-single layer, which modulates the hidden states based on the time embedding. \\
 The second input is the \textbf{text prompt} that describes the content of the image. It is tokenized into 120 tokens in Pixart-$\alpha$, repsectively 300 tokens in Pixart-$\sigma$, and encoded by the T5-XXL text encoder \cite{T5XXL}. A linear projection layer is applied to reduce the dimension from the T5-XXL encoder after which it is incorporated via cross-attention layers into the model. \\
  The third input is the \textbf{latent image} which is encoded via a pre-trained autoencoder, e.g. SDXL-VAE \cite{SDXL}, transforming the shape from  $(3,H,W)$ to $(4, H/8, W/8)$. From which patches of size $2\times2$ are extracted and flattened into a sequence of visual tokens. Finally, positional encoding is applied before feeding them as input to the transformer blocks.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{images/Background/Pixart/Architektur_Uebersicht.pdf}  % adjust filename and width
	\caption{Overview architecture of Pixart-$\Sigma$. $H$ and $W$ are the height and width of the image. $h=\frac{H}{8}$ and $w=\frac{W}{8}$ represent the dimensions in latent space and $N = \frac{H}{8 \cdot p} \frac{W}{8 \cdot p}$ with $p$ being the patch size.}
	\label{Pixart_Übersicht}
\end{figure}


The main components of the diffusion model are the transformer blocks. They consist of four main parts, namely the self-attention layer, the cross-attention layer, the \gls{MLP} and the adaLN-single mechanism (see fig. \ref{TransformerBlock}). \\
The \textbf{multi-head self-attention} layer allows the tokens to attend to each other and capture spatial relationships. It is embedded between time-dependent modulation layers. \\
The \textbf{multi-head cross-attention} layer incorporates text conditioning into the model to align the generated image with the text prompt and is placed between the self-attention and the \gls{MLP} layer. \\
The \textbf{\gls{MLP}} processes each token individually to refine the features. Similar to the self-attention layer, it is positioned between modulation layers. \\
\textbf{adaLN-single} is a modification of the adaptive layer normalization (\gls{adaLN}) method \cite{adaLN} replacing the static parameters of the normalization layer with dynamic values that are predicted directly from conditioning signals such as the timestep. This enables the model to efficiently modulate the feature distribution in each block, thereby controlling the generation process globally. In \cite{DiT} both class and time conditioning were handled via adaLN. In contrast, Pixart-$\Sigma$ only uses it for incorporating the time embedding. To reduce the number of parameters global scale and shift parameters $\bar{S}$ are extracted from the shared \gls{MLP} and each block has additional learnable parameters $E_i$ which are added to the global parameters via a summation function $g$ $S_i = g(\bar{S}, E_i)$.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{images/Background/Pixart/Architekture_Detail.pdf}  % adjust filename and width
	\caption{Diffusion block architecture taken from  \cite{Pixart_a}.}
	\label{TransformerBlock}
\end{figure}

The output of the diffusion model is modulated and processed by a linear layer before the token sequence is reshaped back into spatial dimensions. The final output comprises eight channels as not only the noise but also the variance is predicted. The total number of parameters of individual parts of the transformer blocks and the total model can be found in tab. \ref{Pix_parameter_count}

\begin{table}[h]
	\centering
	\caption{Number of parameters for the individual components of the Transformer block and the total model.}
	\label{Pix_parameter_count}
	\renewcommand{\arraystretch}{1.2} % Macht die Tabelle etwas luftiger/lesbarer
	\begin{tabular}{@{}l c@{}} % @{} entfernt den seitlichen Rand für einen cleanen Look
		\toprule
		\textbf{Component} & \textbf{Parameters  (Mio)} \\ 
		\midrule
		Multi-Head Self-Attention & 5.3 \\
		Multi-Head Cross-Attention & 5.3 \\
		Feed-Forward Network (MLP) & 10.6 \\
		\midrule
		\textbf{Total (Single Block)} & 21.3 \\
		\midrule
		\textbf{Total Model (PixArt-$\Sigma$)} & 610 \\
		\bottomrule
	\end{tabular}
\end{table}
\subsection{Training Strategy}
Pixart training proceeds in three distinct stages, explicitly decoupling the learning of pixel dependencies from text-concept alignment to improve training efficiency. First, the focus is on learning pixel dependencies to generate semantically meaningful images. To achieve this, the model is initialized with weights pre-trained on ImageNet using class-conditional generation. This pre-training provides a strong initialization for the visual distribution at a low computational cost. In a second step, the focus is on text alignment so that the model generates images that accurately reflects the text prompt. For this purpose, the SAM \cite{SAM} dataset was utilized with corresponding high-density prompts generated by the vision-language model LLaVA \cite{LLaVA}. Finally, the model undergoes finetuning on high-resolution and high-aesthetic qualtity images to refine visual details which is done on a data set that is not publicly accessible.

\subsection{PixArt-$\Sigma$ Enhancements }
Pixart-$\Sigma$ is a further development of Pixart-$\alpha$ which is able to generate images at higher resolution, up to 4k, and of higher quality. In this process, the weights from Pixart-$\alpha$ are used as initialization for Pixart-$\Sigma$ introducing three main changes, namely a more powerful VAE, an improved dataset and a more efficient self-attention mechanism leveraging KV-compression.\\
First, Pixart-$\Sigma$ utilizes a more powerful VAE encoder, specifically the VAE from SDXL \cite{SDXL}, to obtain more expressive image features.  \\
Second, the image dataset was extended with high resolution images up to 4k. Moreover, a more powerful model, Share-Captioner \cite{chen2024sharegpt4v}, was leveraged to create captions of higher quality and length. To accomodate this the number of tokens for the text embedding is increased from 120 to 300. \\
Third, a compression technique for the KV tokens in the self-attention layers is introduced to reduce the computational cost for high resolution images which otherwise scales quadratically with the number of tokens $O(N^2)$. The KV tokens are compressed via a convolutional 2x2 kernel operating in spatial space exploiting the redundancy of feature semantics within a $R \times R$ window. However, the Q tokens are kept uncompressed to mitigate the information loss. Consequently, the computational cost reduces from $O(N^2)$ to $O(\frac{N^2}{R^2})$. The attention operation (compare to eq. \ref{AttentionEq}) changes to 
\begin{equation}
	\text{Attention}(Q,K,V) = \text{softmax}\left( \frac{Q f_c(K)^T}{\sqrt{d_k}}\right) f_c(V)
\end{equation} 
with $f_c$ representing the compression operation. \\
To conclude, Pixart-$\Sigma$ shows that leveraging a weaker model as a base for introduce improvements helps to accelerate the training process enormously. 
\section{Flux-dev}