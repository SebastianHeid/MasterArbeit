\section{Generative Models}
In recent years, generative models have become one of the hottest areas of research in the AI community and reached the public domain. Especially, text generators, also known as large language models (LLMs) \cite{Claude, Gemini, GPT4, Lama} and image generators \cite{SD21, SDXL, Pixart_s, Flux, HiDream} are being used more and more in the professional world and in daily life. Several different generator methods have been developed, such as variational autoencoders (VAEs) \cite{VAE}, generative adversarial networks (GANs) \cite{GAN, ConvGAN}, normalizing flows (NFs) \cite{NormalizingFlow}, and diffusion models (DMs) \cite{DMBeginning}. \\
The principal idea of a generative network $f_\theta$ is to learn a mapping from a simple distribution, e.g., a normal distribution $p_{\text{latent}}(\textbf{z}) = \mathcal{N}(\textbf{z}; \boldsymbol{\mu}, \boldsymbol{\Sigma)}$, often called the latent distribution, to a highly complex data distribution $p_{\text{data}}(\textbf{x})$. A new data element $\textbf{x}$ is obtained  by sampling from the latent distribution $\textbf{z} \sim p(\textbf{z})$ and applying the model
\begin{equation}
\textbf{x} = f_\theta(\textbf{z}) \sim p_{\text{model}}(\textbf{x}) \approx p_{data}(\textbf{x})
\end{equation}
where the model's output distribution $p_{\text{model}}(\textbf{x})$ should approximate the true data distribution as closely as possible. 

In the following, the main concepts of GANs, VAEs, and NFs are briefly presented followed by an extensive discussion of diffusion models in chapter \ref{sec:diffModel}.


\subsection{Variational Autoencoder}
\label{VariationaAutoEncoder}
\subsubsection{Autoencoder}
An autoencoder \cite{AE} is a neural network commonly used for dimensionality reduction tasks. It aims to compress high-dimensional data to lower-dimensional space by preserving important information. The architecture consists of two main parts, the encoder and the decoder. The encoder maps high-dimensional input data to a latent space, effectively compressing the data, and the decoder learns to reconstruct the original data from its latent space representation. Typically, as learning objective, a simple reconstruction loss, e.g. mean-squared error, is used comparing the original input data and the output of the autoencoder. 

\subsubsection{Evidence Lower Bound Loss (ELBO)}
The VAE belongs to the class of probabilistic models. A common learning objective for probabilistic models is maximum likelihood estimation (MLE). Given a set of observations $\lbrace \textbf{x}_i \rbrace_{i=1}^N$
 the likelihood is given under iid (identical and independent distributed samples) assumption as
\begin{equation}
     \mathcal{L}_{\text{MLP}} = \prod_{i=1}^n p_{\text{model}}(\textbf{x}_i)
\end{equation}
However, in practice, minimizing the negative log-likelihood is often preferred over maximizing the likelihood due to possible stability problems caused by the product of likelihoods
\begin{equation}
    \mathcal{L}_{\text{NLL}} = - \sum_{i=1}^n \log{p_{\text{model}}(\textbf{x}_i)}
\end{equation}
This approach works well for simple distributions. However, for models that include latent variables $\textbf{z}=z_{1:m}$, such as VAEs, the likelihood becomes an intractable marginalization over the latent space
\begin{equation}
    p(\textbf{x}) = \int p(\textbf{x} |\textbf{z}) p(\textbf{z})d\textbf{z}
\end{equation}
leading to the the so-called evidence lower bound (ELBO) loss \cite{ELBO} being used as training objective for VAEs. \\
 In general, untractable probability distributions are a core challenge in modern statistics. This is especially the case for posterior distributions in Bayesian statistics. Assuming the joint probability distribution 
 \begin{equation}
      p(\textbf{x}, \textbf{z}) = p(\textbf{z}) \cdot p(\textbf{x} |\textbf{z})
 \end{equation}
 is given with data $\lbrace \textbf{x}_i \rbrace_{i=1}^N$ and latent variables $\lbrace \textbf{z}_j \rbrace_{j=1}^M$. During inference, the in general untractable posterior $p(\textbf{z} | \textbf{x})$ is the quantity that needs to be computed. In the past, the dominant approach to tackle this challenge was using Monte Carlo Markov Chain simulations (MCMC) \cite{MCMC}, which approximate the true posterior distribution by sampling. The drawback of these methods is that the sampling procedures are very slow. Therefore, in recent years, a new method has gained popularity, called variational inference (VI) \cite{VariationalInference}. In contrast to MCMC VI reformulates the problem as an optimization task 
 \begin{equation}
     q^*(\textbf{z}) = \underset{q(\textbf{z}) \in \mathscr{Q}}{\arg\min} \text{ KL} \left[ q(\textbf{z}) \,\|\, p(\textbf{z}|\textbf{x})\right] 
     \label{VI-objective}
 \end{equation}
where $\mathscr{Q}$ is a family of distributions and the goal is to find the proposal density $q(\textbf{z}) \in \mathscr{Q}$ that best approximate the true posterior distribution. To compare the proposal density and the posterior the Kullback-Leibler divergence (KL) \cite{KL-Div} is used having the general form of 
\begin{equation}
    \text{KL} [p_0 \,\|\,p_1]= \int p_0(\textbf{x}) \log \frac{p_0(\textbf{x})}{p_1(\textbf{x})} d\textbf{x}
\end{equation}
comparing two distributions $p_0$ and $p_1$.
The key for a good choice of $\mathscr{Q}$ is to choose a family that is complex enough so that it contains a $q^*(\textbf{z}) $ close to the true posterior but simple enough to enable efficient optimization. From applying VI to the problem of finding the true posterior it follows 

\begin{align}
    \text{ KL} \left[ q(\textbf{z}) \,\|\, p(\textbf{z}|\textbf{x})\right] &= \int q(\textbf{z}) \log \frac{q(\textbf{z})}{p(\textbf{z}|\textbf{x})}d\textbf{z} \\
    &= \int q(\textbf{z}) \log q(\textbf{z}) d\textbf{z} - \int q(\textbf{z}) \log p(\textbf{z}|\textbf{x}) d\textbf{z} \\
    &= \int q(\textbf{z})  \log q(\textbf{z}) d\textbf{z} - \int q(\textbf{z})\frac{p(\textbf{z},\textbf{x})}{p(\textbf{x})} d\textbf{z} \\
    &= \mathbb{E}\left[ \log q(\textbf{z}) \right] - \mathbb{E}\left[ \log p(\textbf{z},\textbf{x}) \right] + \log p(\textbf{x})
\end{align}

From here, the ELBO loss is obtained by
\begin{align}
    \log p(\textbf{x}) &\geq \log p(\textbf{x}) - \text{ KL} \left[ q(\textbf{z}) \,\|\, p(\textbf{z}|\textbf{x})\right] \\ 
    &= \mathbb{E}\left[ \log p(\textbf{z},\textbf{x}) \right] -\mathbb{E}\left[ \log q(\textbf{z}) \right] \\
    &=\mathbb{E}\left[ \log p(\textbf{x}| \textbf{z}) \right] - \text{ KL} \left[ q(\textbf{z}) \,\|\, p(\textbf{z})\right] \\
    &= \text{ELBO}
\end{align}


Maximizing the ELBO loss is equivalent to minimizing the KL divergence from \ref{VI-objective} and most importantly, it is tractable. \\
For training VAEs the ELBO objective is used. The difference from the usual MSE loss used for training autoencoders is that it contains an additional term from the KL divergence that enforces a specific structure on the latent space. Let $\theta$ be the decoder parameters and $\phi$ the encoder parameters then the objective of the VAE is given by 

 
 \begin{equation}
     \mathcal{L}_{\text{VAE}} = \underbrace{
\mathbb{E}_{q_\phi(\textbf{z} \mid \textbf{x})}\left[-\log p_\theta(\textbf{x} \mid \textbf{z})\right]
}_{\text{Reconstruction Loss}}
+ 
\underbrace{
\text{KL}\left[q_\phi(\textbf{z} \mid \textbf{x}) \,\|\, p(\textbf{z})\right]
}_{\text{Regularization}}
 \end{equation}
In practice, the latent distribution is often set to a standard normal distribution $p_{\text{latent}}(\textbf{z}) = \mathcal{N}(\textbf{z};\textbf{0},\textbf{1})$ and the reparameterization trick \cite{VAE} enabling gradient computation is applied.
After training, a new data point can be generated by sampling from the latent distribution $\textbf{z} \sim p_{\text{latent}}(\textbf{z})$ and applying the decoder of the trained model.


\subsection{Normalizing Flow}
Normalizing flows (NFs) \cite{NormalizingFlow} are another class of propablistic models, similar to VAEs, which learn a mapping between a latent distribution to the data distribution. However, NFs have some key differences compared to VAEs. First, NFs are fully invertible meaning the network $F_\theta$ consists of a sequence of (simpler) bijective operations so that an inverse transformation $\overline{F_\theta}$ mapping back from data to latent distribution exists. As a consequence, the latent space and the data space need to have the same dimension. \\
A second decisive advantage is that the NFs allow the exact and tractable computation of the likeihood using the change of variables formula. 
\begin{equation}
    p_{\text{latent}(\textbf{z})} = p_\text{model}(\textbf{x}) \left| \frac{\partial F_\theta(\textbf{z})}{\partial \textbf{z}}\right| =  p_\text{model}(F_\theta(\textbf{z})) \left| \frac{\partial F_\theta(\textbf{z})}{\partial \textbf{z}} \right|
\end{equation}
for the latent distribution and 
\begin{equation}
    p_{\text{model}}(\textbf{x}) = p_{\text{latent}}(\textbf{z}) \left| \frac{\partial F_\theta(\textbf{z})}{\partial \textbf{z}} \right|^{-1} =
    p_{\text{latent}}(\overline{F}_\theta(\textbf{x})) \left| \frac{\partial \overline{F}_\theta(\textbf{x})}{\partial \textbf{x}} \right|
    \label{NF-likelihood}
\end{equation}
It is worth noting that computing the likelihood \eqref{NF-likelihood} includes computing the determinant of a jacobian matrix. Therefore, NFs rely on carefully designed invertible layers (e.g., affine coupling layers \cite{AffineCouplingLayer}) where the determinant of the jacobian can be easily computed. This enables training via exact maximum likelihood estimation and does not rely on the ELBO loss like VAEs.
\subsection{Generative Adversarial Network}
In contrast to VAEs, NFs, or DMs generative adversarial networks \cite{GAN} belong to the class of non-probablistic generators which do not require an explicit likelihood function. The generator $G$ is trained in an adversarial training setup that contains, on the one hand, the generator that should generate new data points being as realistic as possible and, on the other hand, a discriminator that should be able to distinguish if a data point is generated by the generator ("fake") or from the original dataset ("real"). The generator and the discriminator are alternately trained, where the generator tries to fool the discriminator by generating more realistic data points, and the discriminator $D$ becomes better at distinguishing fake from real. This is formalized in a MinMax condition.

\begin{equation}
\min_G \max_D \; \mathbb{E}_{\textbf{x} \sim p_{\text{data}}(\textbf{x})}[\log D(\textbf{x})] + \mathbb{E}_{\textbf{z} \sim p(\textbf{z})}[\log(1 - D(G(\textbf{z})))]
\end{equation}
where $G(\textbf{z})$ is a sample generated by the generator and $D(\textbf{x})$ is the output of the discriminator. Being close to one if the discriminator identifies $x$ as "real" and close to zero if the discriminator identifies $\textbf{x}$ as "fake". \\
GANs were the dominant class of network architectures for image generating \cite{StyleGAN_T, StyleGAN_3, FastGAN} but were overtaken by diffusion models in the last years. The GANs training process turned out to be often instable, and extensive hyperparameter tuning and engineering is required \cite{ImprovedGANTraining, PacGAN}. One challenge is to carefully balance the learning dynamics of the generator and the discriminator. If one overpowers the other, e.g., the discriminator always perfectly distinguishes "real" from "fake" samples, the generator stops learning. Another difficulty is known as mode collapse \cite{ModeCollapse1}, meaning that the generator just produces samples from a part of the original data distribution but fails to completely capture it. A third challenge is the problem of exploding and vanishing gradients \cite{WGAN} which prevents effective learning of the generator.
