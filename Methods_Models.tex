\chapter{Methods}
\section{Image Generation Models}
In the following section, the models, namely Pixart-$\Sigma$ \cite{Pixart_s} and Flux-dev \cite{Flux}, which were used for the experiments in this work are described in detail.
\subsection{Pixart-$\Sigma$}
PixArt-$\Sigma$ \cite{Pixart_s}, the latest addition to the PixArt family \cite{Pixart_a, Pixart_d}, was developed to demonstrate that high-fidelity image generation can be achieved with significantly reduced training costs and model parameters. Pixart-$\Sigma$ is a further development of  Pixart-$\alpha$. Therefore, the shared architecture of Pixart-$\alpha$ and Pixart-$\Sigma$ is introduced first. Next, the key points for efficient training of Pixart-$\alpha$ are described, followed by the specific enhancements implemented in PixArt-$\Sigma$.

\subsubsection{Architecture }
The architecture of Pixart-$\Sigma$ is based on the class-conditional diffusion transformer \gls{DiT}-XL/2 \cite{DiT} which contains 28 transformer blocks. The transformer blocks have primarily three inputs (see fig. \ref{Pixart_Übersicht}) which are described in the following. \\
The first input is the \textbf{timestep} $t$ which is projected into a time embedding vector using a 256-frequency embedding, from which a \gls{MLP} extracts the global scale and shift parameters. They are used as input for the customized adaptive layer normalization called adaLN-single layer, which modulates the hidden states based on the time embedding. \\
 The second input is the \textbf{text prompt} that describes the content of the image. It is tokenized into 120 tokens in Pixart-$\alpha$, repsectively 300 tokens in Pixart-$\Sigma$, and encoded by the T5-XXL text encoder \cite{T5XXL}. A linear projection layer is then applied to reduce the high dimensional output from the T5-XXL encoder after which it is incorporated via cross-attention layers into the model. \\
  The third input is the \textbf{latent image} which is encoded via a pre-trained autoencoder, e.g. SDXL-VAE \cite{SDXL} in Pixart-$\Sigma$, transforming the image $(3,H,W)$ into latent space $(4, H/8, W/8)$. Patches of size $2\times2$ are extracted from the latent representation of the image and flattened into a sequence of visual tokens. Finally, 2D sine-cosine positional encoding is applied before feeding them as input to the transformer blocks.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{images/Background/Pixart/Architektur_Uebersicht.pdf}  % adjust filename and width
	\caption{Overview architecture of Pixart-$\Sigma$. $H$ and $W$ are the height and width of the image. $h=\frac{H}{8}$ and $w=\frac{W}{8}$ represent the dimensions in latent space and $N = \frac{H}{8 \cdot p} \frac{W}{8 \cdot p}$ with $p$ being the patch size.}
	\label{Pixart_Übersicht}
\end{figure}


The main components of the diffusion model are the transformer blocks. They consist of four main parts, namely the self-attention layer, the cross-attention layer, the \gls{MLP} and the adaLN-single mechanism (see fig. \ref{TransformerBlock}). \\
The \textbf{multi-head self-attention} layer allows the tokens to attend to each other and capture spatial relationships. It is embedded between time-dependent modulation layers at the beginning of the transformer block. \\
The \textbf{multi-head cross-attention} layer incorporates text conditioning into the model to align the generated image with the text prompt and is placed between the self-attention and the \gls{MLP} layer. \\
The \textbf{\gls{MLP}} processes each token individually to refine the features. Similar to the self-attention layer, it is positioned between modulation layers at the end of the transformer block. \\
\textbf{adaLN-single} is a modification of \gls{adaLN} (see chapter \ref{AdaptiveLayerNormalization}) method \cite{adaLN} replacing the static parameters of the normalization layer with dynamic values that are predicted directly from conditioning signals such as the timestep. This enables the model to efficiently modulate the feature distribution in each block, thereby controlling the generation process globally. In \cite{DiT} both class and time conditioning were handled via adaLN. In contrast, Pixart-$\Sigma$ only uses it for incorporating the time embedding. To reduce the number of parameters global scale and shift parameters $\bar{S}$ are extracted from one shared \gls{MLP} and each block has additional learnable parameters $E_i$ which are added to the global parameters via a summation function $g$ $S_i = g(\bar{S}, E_i)$ to provide flexibility for every transformer block.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{images/Background/Pixart/Architekture_Detail.pdf}  % adjust filename and width
	\caption{Diffusion block architecture taken from  \cite{Pixart_a}.}
	\label{TransformerBlock}
\end{figure}

The output of the final transformer block is modulated and processed by a linear layer before the token sequence is reshaped back into spatial dimensions. The final output comprises eight channels as not only the noise but also the variance is predicted. The total number of parameters of individual parts of the transformer blocks and the total model can be found in tab. \ref{Pix_parameter_count}.

\begin{table}[h]
	\centering
	\caption{Number of parameters for the individual components of the Transformer block and the total Pixart-$\Sigma$ model.}
	\label{Pix_parameter_count}
	\renewcommand{\arraystretch}{1.2} % Macht die Tabelle etwas luftiger/lesbarer
	\begin{tabular}{@{}l c@{}} % @{} entfernt den seitlichen Rand für einen cleanen Look
		\toprule
		\textbf{Component} & \textbf{Parameters  (Mio)} \\ 
		\midrule
		Multi-Head Self-Attention & 5.3 \\
		Multi-Head Cross-Attention & 5.3 \\
		Feed-Forward Network (MLP) & 10.6 \\
		\midrule
		\textbf{Total (Single Block)} & 21.3 \\
		\midrule
		\textbf{Total Model (PixArt-$\Sigma$)} & 610.9 \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection{Training Strategy}
Pixart training proceeds in three distinct stages, explicitly decoupling the learning of pixel dependencies from text-concept alignment to improve training efficiency. First, the focus is on learning pixel dependencies to generate semantically meaningful images. To achieve this, the model is initialized with weights pre-trained on ImageNet \cite{ImageNet} using class-conditional generation. This pre-training provides a strong initialization for the visual distribution at a low computational cost. In a second step, the focus is on text alignment so that the model generates images that accurately reflects the text prompt. For this purpose, the SAM \cite{SAM} dataset was utilized with corresponding high-density prompts generated by the vision-language model LLaVA \cite{LLaVA}. Finally, the model undergoes finetuning on high-resolution and high-aesthetic qualtity images to refine visual details which was done on a data set that is not publicly accessible.

\subsubsection{PixArt-$\Sigma$ Enhancements }
Pixart-$\Sigma$ is a further development of Pixart-$\alpha$ which is able to generate images at higher resolution, up to 4k, and of higher quality. In this process, the weights from Pixart-$\alpha$ are used as initialization for Pixart-$\Sigma$ introducing three main changes, namely a more powerful VAE, an improved dataset and a more efficient self-attention mechanism leveraging KV-compression.\\
First, Pixart-$\Sigma$ utilizes a more powerful VAE encoder, specifically the VAE from SDXL \cite{SDXL}, to obtain more expressive image features.  \\
Second, the image dataset was extended with high resolution images up to 4k. Moreover, a more powerful model, Share-Captioner \cite{chen2024sharegpt4v}, was leveraged to create captions of higher quality and length. To accomodate this the number of tokens for the text embedding is increased from 120 to 300. \\
Third, a compression technique for the KV tokens in the self-attention layers is introduced to reduce the computational cost for high resolution images which otherwise scales quadratically with the number of tokens $O(N^2)$. The KV tokens are compressed via a convolutional 2x2 kernel operating in spatial space exploiting the redundancy of feature semantics within a $R \times R$ window. However, the Q tokens are kept uncompressed to mitigate the information loss. Consequently, the computational cost reduces from $O(N^2)$ to $O(\frac{N^2}{R^2})$. The attention operation (compare to eq. \ref{AttentionEq}) changes to 
\begin{equation}
	\text{Attention}(Q,K,V) = \text{softmax}\left( \frac{Q f_c(K)^T}{\sqrt{d_k}}\right) f_c(V)
\end{equation} 
with $f_c$ representing the compression operation. \\
To conclude, Pixart-$\Sigma$ shows that leveraging a weaker model as a base for introduce improvements helps to accelerate the training process enormously. 



\subsection{Flux-dev}
Flux-dev \cite{Flux} is a publicly available, guidance-distilled rectified flow model based on the commercial model Flux.1 \cite{Flux}. Due to the absence of a publication/paper describing the exact training process, the data set used and the specific design choices, the following section focuses on presenting the models's architecture.

\subsubsection{Architecture}
Flux-dev is based on a multi-modal diffusion transformer (\gls{MMDiT}) \cite{SD3} architecture containing 19 double-stream blocks followed by 38 single-stream blocks (see fig. \ref{FluxOverview}) adding up to a total of 11.9 billion parameters (see tab. \ref{Flux_parameter_count}). For prompt processing,  it leverages two distinct text encoders, namely CLIP ViT-L/14 \cite{CLIP} and T5-XXL \cite{T5XXL}. For image encoding a \gls{VAE} is used compressing the height $H$ and width $W$ of the image by a factor of eight. Each double-stream block has mainly four inputs: a guidance vector, the positional information of the image patches and the text tokens as well as the text and the image hidden states. Hereby, the text and the image are processed seperately in two distinct streams allwoing the model to apply modality-specific weights. This dual-path approach is the defining characteristic of a \gls{MMDiT}. In contrast, the single-stream blocks have only three inputs as they combine the text and image hidden states and process them together.  \\

The \textbf{guidance vector} is a combination of the timestep, a guidance parameter and the pooled vector of the clip embedding. In guidance distillation, the model is trained to simulate classifier free guidance internally. This speeds up the inference process because in contrast to classifier free guidance where the model is applied twice per inference step only one inference pass per step is needed. The guidance is a scalar which corresponds to the guidance scale in standard classifier free guidance. First, the guidance parameter and the timestep are embedded via \gls{MLP}s  into high dimensional tensors which are summed together. Then the pooled clip vector for the text prompt is embedded via another MLP and added. The resulting vector, called guidance vector, is injected via modulation in every double-stream and every single-stream block. \\
Another input to both block types is the concatenated \textbf{positional encoding} for the text prompt and the image. First, based on the latent representation of the image, 3D coordinates for the position of every patch in the image is created. The 3D vector for the image represents the 2D positional IDs $(x,y)$ of the patch in the image and a time dimension $t$ for the possibility to adapt Flux-dev for videos. However, for image generation the time dimension is always set to zero. Text position IDs are included for architectural consistency but are initialized to zero, as text tokens lack meaningful 2D spatial coordinates in a latent image grid. In contrast to Pixart models, Flux-dev uses rotary positional embedding (\gls{RoPE}) \cite{RoPE}. The embedded vector is then used in the attention mechanism to rotate the key and query vectors such as the spatial relationships within the image and the sequential structure of the text are preserved. \\
For the text embedding, the \textbf{text prompt} is tokenized into $512$ tokens such that the model is  capable of processing long and detailed prompts. The T5-XXL encoder embeds the tokens into high-dimensional vectors $(1,512,4096)$ and a linear projection is applied before it is fed into the first double-stream block. The following double-stream blocks always receive the hidden states of the text from the previous block. \\
The \textbf{image} $(3,H,W)$ is encoded via \gls{VAE} to a latent representation $(16,h=\frac{H}{8}, w=\frac{W}{8})$. To prepare the latents for the transformer, Flux-dev packs $2 \times 2$ neighboring latent pixels into a single patch and therefore increasing the dimension from $16$ to $64$ during the patchify operation while simultaneously reducing the sequence length $N$ to $\frac{h}{2} \cdot \frac{w}{2}$ $(1,N=\frac{h}{2} \cdot \frac{w}{2}, 64)$. These patches are then linearly projected into a higher dimensional embedding space before being fed into the first double block.  \\
The output, sepcifically the text and image hidden states, of the 19th double-stream block  are concatenated and used as single input to the first single-stream block.




\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{images/Background/Flux/Flux_Uebersicht.pdf}  % adjust filename and width
	\caption{Overview Flux-dev architecture.}
\label{FluxOverview}
\end{figure}

\subsubsection{Double-Stream Blocks}
The double-stream blocks (see fig. \ref{SingleDoubleStreamBlock} on the right) are the fundamental building blocks of the first stage of the Flux-dev architecture. In contrast to other diffusion-based models \cite{Pixart_s, SD21, SDXL} the block consists of two separate streams, which process image and text information separately and only exchange information during the attention process. The blocks consist of three main components: the modulation, which incorporates the guidance vector encoding information about the timestep, the guidance, and the pooled CLIP vector, the attention mechanism, which bidirectionally transfers information from text to image features and vice versa and the \gls{MLP}s,  which further process the features. \\
The \textbf{modulation} of the text and image features is implemented via \gls{adaLN}. As shown in fig. \ref{SingleDoubleStreamBlock}, the guidance vector passes through two separate projection layers, which generate each two sets of scale and shift paramerters for \gls{adaLN}  as well as two gating parameters. The scaling and shifting of the features to incorporate the time, guidance and pooled text conditioning is applied twice, once before the attention mechanism and once before the \gls{MLP}. Subsequently, the gating parameters scale the updated features before they are added to the residual stream. This technique is often applied in very deep neural networks because the linear projection layers producing the gating values are initialized with zeros. Consequently, the gating paramters start at zero, such that the block initially acts as an identity function, which stabilizes the training.  \\
The \textbf{attention} mechanism enables the exchange of information between the text and the image features. First, the text and image features are processed by seperate linear layers which project them into query, key and value representations and are then normalized via \gls{RMSNorm} \cite{RMSNorm}. Subsequently, they are concatenated into a joint sequence. At this stage the \gls{RoPE}s are integrated into the model by rotating the joint query and key vectors before the attention mechanism is applied such that self- and cross-attention take place simultaneously. Afterwards, the updated text and image features are separated and processed by individual projection layers. \\
The third main parts are the \textbf{\gls{MLP}s}‚ in each stream which process the updated feature vectors further to increase the representational capacity further. Like the attention mechanism, the MLP is also enclosed by a modulation and connected via a gated residual connection.
\subsubsection{Single-Stream Blocks}
The single-stream blocks (see fig. \ref{SingleDoubleStreamBlock} on the left) are the fundamental building blocks of the second stage of the Flux-dev architecture. Their primary components are the self-attention mechanism and the modulation layers. First, the concatenated image and text features $X$ are modulated using \gls{adaLN} where the shift and scale parameters are obtained by processing the guidance vector through a linear layer. Similar to the double-stream architecure, a gating mechanism is applied after the attention mechnism is executed. To optimize throughput, the model uses a fused linear projection where the hidden states for the attention mechanism and the parallel feed-forward path are computed simultaneously.  The outputs of the attention mechanism and the feed-forward path are concatenated together and processed by a subsequent linear projection before the gating mechanism is applied. Finally, the updated features are combined with the residual path to form the final output of the block.
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{images/Background/Flux/SingleDoubleBlocks.pdf}  % adjust filename and width
	\caption{Single-stream block (left) and double-stream block (right) of Flux-dev. Graphics are adapted from \cite{FastFlux}.}
	\label{SingleDoubleStreamBlock}
\end{figure}



\begin{table}[h]
	\centering
	\caption{Number of parameters for the individual components of double-stream block from Flux-dev.}
	\label{tab:Flux_parameter_count}
	\renewcommand{\arraystretch}{1.2} % Macht die Tabelle etwas luftiger/lesbarer
	\begin{tabular}{@{}l c@{}} % @{} entfernt den seitlichen Rand für einen cleanen Look
		\toprule
		\textbf{Component} & \textbf{Parameters  (Mio)} \\ 
		\midrule
		Double-Stream Image Linear Layer (Attention) & 28.3 \\
		Double-Stream Image \gls{MLP} & 75.5 \\
		Double-Stream Image Modulation & 56.6 \\
		Double-Stream Image Projection & 9.4 \\
		Double-Stream Text Linear Layer (Attention) & 28.3 \\
		Double-Stream Text \gls{MLP} & 75.5 \\
		Double-Stream Text Modulation & 56.6 \\
		Double-Stream Text Projection & 9.4 \\
		\midrule
		\textbf{Total Double-Stream Block} & 339.8 \\
		\midrule
		Single-Stream Fused Linear Layer (Linear1) & 66.1\\
		Single-Stream Projection & 47.2 \\
		Single-Stream Modulation & 28.3 \\
		\midrule
		\textbf{Total Single-Stream Block} & 141.6 \\
			\midrule
		\textbf{Total Model (Flux-dev)} & 11,901.4 \\
		\bottomrule
	\end{tabular}
\end{table}

